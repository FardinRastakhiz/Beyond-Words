{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using existing file ml-latest-small.zip\n",
      "Extracting .\\ml-latest-small.zip\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import download_url, extract_zip\n",
    "\n",
    "url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "movies_path = './ml-latest-small/movies.csv'\n",
    "ratings_path = './ml-latest-small/ratings.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Action  Adventure  Drama  Horror\n",
      "movieId                                  \n",
      "1             0          1      0       0\n",
      "2             0          1      0       0\n",
      "3             0          0      0       0\n",
      "4             0          0      1       0\n",
      "5             0          0      0       0\n"
     ]
    }
   ],
   "source": [
    "  # Load the entire movie data frame into memory:\n",
    "movies_df = pd.read_csv(movies_path, index_col='movieId')\n",
    "\n",
    "# Split genres and convert into indicator variables:\n",
    "genres = movies_df['genres'].str.get_dummies('|')\n",
    "print(genres[[\"Action\", \"Adventure\", \"Drama\", \"Horror\"]].head())\n",
    "# Use genres as movie input features:\n",
    "movie_feat = torch.from_numpy(genres.values).to(torch.float)\n",
    "assert movie_feat.size() == (9742, 20)  # 20 genres in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of user IDs to consecutive values:\n",
      "==========================================\n",
      "   userId  mappedID\n",
      "0       1         0\n",
      "1       2         1\n",
      "2       3         2\n",
      "3       4         3\n",
      "4       5         4\n",
      "\n",
      "Mapping of movie IDs to consecutive values:\n",
      "===========================================\n",
      "   movieId  mappedID\n",
      "0        1         0\n",
      "1        3         1\n",
      "2        6         2\n",
      "3       47         3\n",
      "4       50         4\n",
      "\n",
      "Final edge indices pointing from users to movies:\n",
      "=================================================\n",
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    1,    2,  ..., 3121, 1392, 2873]])\n"
     ]
    }
   ],
   "source": [
    "# Load the entire ratings data frame into memory:\n",
    "ratings_df = pd.read_csv(ratings_path)\n",
    "\n",
    "# Create a mapping from unique user indices to range [0, num_user_nodes):\n",
    "unique_user_id = ratings_df['userId'].unique()\n",
    "unique_user_id = pd.DataFrame(data={\n",
    "    'userId': unique_user_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_user_id)),\n",
    "})\n",
    "print(\"Mapping of user IDs to consecutive values:\")\n",
    "print(\"==========================================\")\n",
    "print(unique_user_id.head())\n",
    "print()\n",
    "# Create a mapping from unique movie indices to range [0, num_movie_nodes):\n",
    "unique_movie_id = ratings_df['movieId'].unique()\n",
    "unique_movie_id = pd.DataFrame(data={\n",
    "    'movieId': unique_movie_id,\n",
    "    'mappedID': pd.RangeIndex(len(unique_movie_id)),\n",
    "})\n",
    "print(\"Mapping of movie IDs to consecutive values:\")\n",
    "print(\"===========================================\")\n",
    "print(unique_movie_id.head())\n",
    "# Perform merge to obtain the edges from users and movies:\n",
    "ratings_user_id = pd.merge(ratings_df['userId'], unique_user_id,\n",
    "                            left_on='userId', right_on='userId', how='left')\n",
    "ratings_user_id = torch.from_numpy(ratings_user_id['mappedID'].values)\n",
    "ratings_movie_id = pd.merge(ratings_df['movieId'], unique_movie_id,\n",
    "                            left_on='movieId', right_on='movieId', how='left')\n",
    "ratings_movie_id = torch.from_numpy(ratings_movie_id['mappedID'].values)\n",
    "# With this, we are ready to construct our `edge_index` in COO format\n",
    "# following PyG semantics:\n",
    "edge_index_user_to_movie = torch.stack([ratings_user_id, ratings_movie_id], dim=0)\n",
    "assert edge_index_user_to_movie.size() == (2, 100836)\n",
    "print()\n",
    "print(\"Final edge indices pointing from users to movies:\")\n",
    "print(\"=================================================\")\n",
    "print(edge_index_user_to_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100836,), torch.Size([100836]), (610, 2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ratings_df['userId'].shape, ratings_user_id.shape, unique_user_id.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,  ..., 609, 609, 609])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "import torch_geometric.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "# Save node indices:\n",
    "data[\"user\"].node_id = torch.arange(len(unique_user_id))\n",
    "data[\"movie\"].node_id = torch.arange(len(movies_df))\n",
    "# Add the node features and edge indices:\n",
    "data[\"movie\"].x = movie_feat\n",
    "data[\"user\", \"rates\", \"movie\"].edge_index = edge_index_user_to_movie\n",
    "\n",
    "data[\"user\", \"rates\", \"movie\"].edge_attr = edge_index_user_to_movie[0] / torch.max(edge_index_user_to_movie[0])\n",
    "\n",
    "# We also need to make sure to add the reverse edges from movies to users\n",
    "# in order to let a GNN be able to pass messages in both directions.\n",
    "# We can leverage the `T.ToUndirected()` transform for this from PyG:\n",
    "data = T.ToUndirected()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'edge_attr' in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test = torch.tensor(edge_index_user_to_movie)\n",
    "test[:,0] = torch.ones((2,))\n",
    "print(edge_index_user_to_movie)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# For this, we first split the set of edges into\n",
    "# training (80%), validation (10%), and testing edges (10%).\n",
    "# Across the training edges, we use 70% of edges for message passing,\n",
    "# and 30% of edges for supervision.\n",
    "# We further want to generate fixed negative edges for evaluation with a ratio of 2:1.\n",
    "# Negative edges during training will be generated on-the-fly.\n",
    "# We can leverage the `RandomLinkSplit()` transform for this from PyG:\n",
    "transform = T.RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    disjoint_train_ratio=0.3,\n",
    "    neg_sampling_ratio=2.0,\n",
    "    add_negative_train_samples=False,\n",
    "    edge_types=(\"user\", \"rates\", \"movie\"),\n",
    "    rev_edge_types=(\"movie\", \"rev_rates\", \"user\"), \n",
    ")\n",
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# In the first hop, we sample at most 20 neighbors.\n",
    "# In the second hop, we sample at most 10 neighbors.\n",
    "# In addition, during training, we want to sample negative edges on-the-fly with\n",
    "# a ratio of 2:1.\n",
    "# We can make use of the `loader.LinkNeighborLoader` from PyG:\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Define seed edges:\n",
    "edge_label_index = train_data[\"user\", \"rates\", \"movie\"].edge_label_index\n",
    "edge_label = train_data[\"user\", \"rates\", \"movie\"].edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = LinkNeighborLoader(\n",
    "    data=train_data,\n",
    "    num_neighbors=[20, 10],\n",
    "    neg_sampling_ratio=2.0,\n",
    "    edge_label_index=((\"user\", \"rates\", \"movie\"), edge_label_index),\n",
    "    edge_label=edge_label,\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "het: HeteroData = next(iter(train_loader))\n",
    "print(het.edge_index_dict)\n",
    "print(het.edge_attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, GENConv, GeneralConv, GATv2Conv, TransformerConv, to_hetero, BatchNorm\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class HeteroGCNConv(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, dropout = 0.57) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        self.conv2 = GATv2Conv(out_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        \n",
    "        self.batch_norm = BatchNorm(out_feature)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_weights: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_weights))\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weights)\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "emb_model = to_hetero(HeteroGCNConv(256, 128), data.metadata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "het.edge_attr_dict[('user', 'rates', 'movie')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "user_emb = torch.nn.Embedding(data['user'].num_nodes, 256)\n",
    "movie_emb = torch.nn.Embedding(data['movie'].num_nodes, 256)\n",
    "movie_lin = torch.nn.Linear(20, 256)\n",
    "x_dict = {\n",
    "    \"user\": user_emb(het[\"user\"].node_id),\n",
    "    \"movie\": movie_lin(het[\"movie\"].x) + movie_emb(het[\"movie\"].node_id)\n",
    "}\n",
    "x_dict = emb_model(x_dict, het.edge_index_dict, het.edge_attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "user_emb = torch.nn.Embedding(data['user'].num_nodes, 256)\n",
    "movie_emb = torch.nn.Embedding(data['movie'].num_nodes, 256)\n",
    "movie_lin = torch.nn.Linear(20, 256)\n",
    "x_dict = {\n",
    "    \"user\": user_emb(het[\"user\"].node_id),\n",
    "    \"movie\": movie_lin(het[\"movie\"].x) + movie_emb(het[\"movie\"].node_id)\n",
    "}\n",
    "x_dict = emb_model(x_dict, het.edge_index_dict, het.edge_attr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm, global_mean_pool, global_add_pool, global_max_pool, MemPooling, SAGEConv, to_hetero, HeteroBatchNorm\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_feature: int, out_features: int,\n",
    "                 metadata,\n",
    "                 base_hidden_feature: int=256,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(GNN, self).__init__()\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.bsh: int = base_hidden_feature\n",
    "        bsh2: int = int(self.bsh/2)\n",
    "        bsh4: int = int(self.bsh/4)\n",
    "        bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "        self.encoder = GSequential('x_dict, edge_index, edge_weights', [\n",
    "            (to_hetero(HeteroG CNConv(input_feature, self.bsh), metadata), 'x_dict, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, bsh2), metadata), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh4), metadata), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh8), metadata), 'x3, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "            (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "            (BatchNorm(bsh4), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "            (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "            (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (BatchNorm(bsh2), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "            (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "            (BatchNorm(self.bsh), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.input_features), 'x1, edge_index, edge_weights -> x1'),\n",
    "        ])\n",
    "        \n",
    "                \n",
    "        self.user_emb = torch.nn.Embedding(data['user'].num_nodes, self.input_features)\n",
    "        self.movie_emb = torch.nn.Embedding(data['movie'].num_nodes, self.input_features)\n",
    "        self.movie_lin = torch.nn.Linear(20, self.input_features)\n",
    "        \n",
    "    def forward(self, x: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"word\": self.user_emb(x[\"word\"].node_id),\n",
    "          \"movie\": self.movie_lin(x[\"movie\"].x) + self.movie_emb(x[\"movie\"].node_id)\n",
    "        }\n",
    "\n",
    "        x1_dict, x2_dict, x3_dict, x4_dict = self.encoder(x_dict, x.edge_index_dict, x.edge_attr_dict)\n",
    "        print(f'x4_dict[\"movie\"]: {x4_dict[\"movie\"].shape}, x_dict[\"movie\"]: {x_dict[\"movie\"].shape}')\n",
    "        x_att, x4 = self.attention(x3_dict[\"movie\"], x4_dict[\"movie\"], \n",
    "                                   x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                                   x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        x_dec = self.decoder(x1_dict[\"movie\"], x2_dict[\"movie\"], x_att, \n",
    "                             x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                             x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        return x_dict #x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "words = torch.zeros((25 , 300), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "het.edge_index_dict\n",
    "het.edge_attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gnn_model = GNN(20, 1, metadata=data.metadata(), base_hidden_feature=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('user',\n",
       "  'rates',\n",
       "  'movie'): tensor([0.3924, 0.9819, 0.4401,  ..., 0.7783, 0.2414, 0.8112]),\n",
       " ('movie',\n",
       "  'rev_rates',\n",
       "  'user'): tensor([0.0016, 0.0016, 0.0016,  ..., 0.8177, 0.8177, 0.8177])}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "het.edge_index_dict\n",
    "het.edge_attr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gnn_model = GNN(20, 1, metadata=data.metadata(), base_hidden_feature=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x4_dict[\"movie\"]: torch.Size([2808, 32]), x_dict[\"movie\"]: torch.Size([2808, 20])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'user': tensor([[ 1.1510,  0.1046, -0.5639,  ...,  1.0205, -0.0078,  0.0808],\n",
       "         [ 0.3696, -0.9333, -2.0932,  ..., -0.6952, -0.3166,  0.9402],\n",
       "         [-0.6207,  1.2826,  1.5125,  ...,  0.0499, -0.0366,  1.0635],\n",
       "         ...,\n",
       "         [ 0.5995, -0.3419,  1.8178,  ...,  1.1518, -1.0551,  0.7446],\n",
       "         [-1.0250,  1.9003, -0.4490,  ..., -0.8544,  0.6340,  1.5318],\n",
       "         [ 2.3937, -0.9057,  1.7867,  ..., -2.3540,  1.7853,  0.0841]],\n",
       "        grad_fn=<EmbeddingBackward0>),\n",
       " 'movie': tensor([[ 4.9276e-01,  1.5015e+00,  4.4476e-01,  ...,  1.7528e+00,\n",
       "          -6.5898e-02, -1.0381e-02],\n",
       "         [ 8.6721e-02, -8.2240e-01, -4.4348e-01,  ...,  2.3993e-02,\n",
       "          -3.3888e+00, -1.2052e-02],\n",
       "         [ 7.0787e-01, -3.8669e-01,  2.7619e-01,  ...,  3.6967e-01,\n",
       "           7.7473e-01,  3.2005e-01],\n",
       "         ...,\n",
       "         [ 2.0895e-01,  1.3283e+00, -1.0325e+00,  ..., -8.2338e-01,\n",
       "          -2.4185e-02,  4.1061e-01],\n",
       "         [ 4.0624e-01, -5.2136e-02,  4.3045e-01,  ...,  3.4253e-01,\n",
       "          -1.9966e+00,  1.7588e+00],\n",
       "         [-1.1696e+00, -2.7681e-02,  6.8844e-01,  ..., -1.3059e+00,\n",
       "           2.1573e+00, -1.8810e-03]], grad_fn=<AddBackward0>)}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_model(het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = (torch.zeros((3,3)), torch.zeros((4, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Practices\\Tasks\\HeterogeneousGraphs\\HeterogeneousGraphs1.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/HeterogeneousGraphs1.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sample\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "sample.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinkNeighborLoader()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.hetero_data.HeteroData"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# het.edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9742, 20])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"movie\"].x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Our final classifier applies the dot-product between source and destination\n",
    "# node embeddings to derive edge-level predictions:\n",
    "class Classifier(torch.nn.Module):\n",
    "    def forward(self, x_user: Tensor, x_movie: Tensor, edge_label_index: Tensor) -> Tensor:\n",
    "        # Convert node embeddings to edge-level representations:\n",
    "        edge_feat_user = x_user[edge_label_index[0]]\n",
    "        edge_feat_movie = x_movie[edge_label_index[1]]\n",
    "        # Apply dot-product to get a prediction per supervision edge:\n",
    "        return (edge_feat_user * edge_feat_movie).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        # Since the dataset does not come with rich features, we also learn two\n",
    "        # embedding matrices for users and movies:\n",
    "        self.movie_lin = torch.nn.Linear(20, hidden_channels)\n",
    "        self.user_emb = torch.nn.Embedding(data[\"user\"].num_nodes, hidden_channels)\n",
    "        self.movie_emb = torch.nn.Embedding(data[\"movie\"].num_nodes, hidden_channels)\n",
    "        # Instantiate homogeneous GNN:\n",
    "        self.gnn = GNN(hidden_channels)\n",
    "        # Convert GNN model into a heterogeneous variant:\n",
    "        self.gnn = to_hetero(self.gnn, metadata=data.metadata())\n",
    "        self.classifier = Classifier()\n",
    "    def forward(self, data: HeteroData) -> Tensor:\n",
    "        print(f'data[\"user\"].node_id.shape: {data[\"user\"].node_id.shape}')\n",
    "        print(f'self.user_emb(data[\"user\"].node_id).shape: {self.user_emb(data[\"user\"].node_id).shape}')\n",
    "        \n",
    "        \n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(data[\"user\"].node_id),\n",
    "          \"movie\": self.movie_lin(data[\"movie\"].x) + self.movie_emb(data[\"movie\"].node_id),\n",
    "        } \n",
    "        # `x_dict` holds feature matrices of all node types\n",
    "        # `edge_index_dict` holds all edge indices of all edge types\n",
    "        print(f'x_dict 1: {x_dict}')\n",
    "        x_dict = self.gnn(x_dict, data.edge_index_dict)\n",
    "        print(f'x_dict 2: {x_dict}')\n",
    "        pred = self.classifier(\n",
    "            x_dict[\"user\"],\n",
    "            x_dict[\"movie\"],\n",
    "            data[\"user\", \"rates\", \"movie\"].edge_label_index,\n",
    "        )\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"user\"].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "het = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data[\"user\"].node_id.shape: torch.Size([610])\n",
      "self.user_emb(data[\"user\"].node_id).shape: torch.Size([610, 64])\n",
      "x_dict 1: {'user': tensor([[-0.1731, -0.0329, -1.0646,  ..., -0.1266, -1.2288, -1.2651],\n",
      "        [ 2.0936, -0.9719, -0.0572,  ..., -0.1425, -0.5372, -0.1820],\n",
      "        [ 1.2944, -0.1054,  0.4820,  ..., -0.6956, -0.4736,  0.8452],\n",
      "        ...,\n",
      "        [-0.0977, -0.4481,  0.1680,  ..., -0.6580,  0.1051, -2.0542],\n",
      "        [-0.0750,  2.1841,  1.6005,  ...,  0.7096,  0.5372, -0.0957],\n",
      "        [ 0.7012,  1.5016,  0.0513,  ..., -1.3722,  0.3077, -0.5693]],\n",
      "       grad_fn=<EmbeddingBackward0>), 'movie': tensor([[ 0.1400,  1.0546,  1.0634,  ..., -1.1045, -1.3633,  0.3525],\n",
      "        [ 1.0973,  1.1088,  0.3908,  ..., -0.7576,  1.0406,  0.3367],\n",
      "        [ 0.4720,  0.9814,  0.4514,  ...,  0.7214, -0.7344,  0.1905],\n",
      "        ...,\n",
      "        [-0.3643,  1.0382, -1.4448,  ..., -0.5761, -1.3072,  1.5188],\n",
      "        [-0.1337, -0.0369,  0.0053,  ..., -1.0227,  0.4914,  1.1152],\n",
      "        [-1.4048,  0.2001,  0.3864,  ..., -1.2904, -0.5426, -0.5389]],\n",
      "       grad_fn=<AddBackward0>)}\n",
      "src: tensor([[ 0.1400,  1.0546,  1.0634,  ..., -1.1045, -1.3633,  0.3525],\n",
      "        [ 1.0973,  1.1088,  0.3908,  ..., -0.7576,  1.0406,  0.3367],\n",
      "        [ 0.4720,  0.9814,  0.4514,  ...,  0.7214, -0.7344,  0.1905],\n",
      "        ...,\n",
      "        [-0.3643,  1.0382, -1.4448,  ..., -0.5761, -1.3072,  1.5188],\n",
      "        [-0.1337, -0.0369,  0.0053,  ..., -1.0227,  0.4914,  1.1152],\n",
      "        [-1.4048,  0.2001,  0.3864,  ..., -1.2904, -0.5426, -0.5389]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "src.shape: torch.Size([2758, 64])\n",
      "src: tensor([[-0.1731, -0.0329, -1.0646,  ..., -0.1266, -1.2288, -1.2651],\n",
      "        [ 2.0936, -0.9719, -0.0572,  ..., -0.1425, -0.5372, -0.1820],\n",
      "        [ 1.2944, -0.1054,  0.4820,  ..., -0.6956, -0.4736,  0.8452],\n",
      "        ...,\n",
      "        [-0.0977, -0.4481,  0.1680,  ..., -0.6580,  0.1051, -2.0542],\n",
      "        [-0.0750,  2.1841,  1.6005,  ...,  0.7096,  0.5372, -0.0957],\n",
      "        [ 0.7012,  1.5016,  0.0513,  ..., -1.3722,  0.3077, -0.5693]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "src.shape: torch.Size([610, 64])\n",
      "src: tensor([[-0.1731, -0.0329, -1.0646,  ..., -0.1266, -1.2288, -1.2651],\n",
      "        [ 2.0936, -0.9719, -0.0572,  ..., -0.1425, -0.5372, -0.1820],\n",
      "        [ 1.2944, -0.1054,  0.4820,  ..., -0.6956, -0.4736,  0.8452],\n",
      "        ...,\n",
      "        [-0.0977, -0.4481,  0.1680,  ..., -0.6580,  0.1051, -2.0542],\n",
      "        [-0.0750,  2.1841,  1.6005,  ...,  0.7096,  0.5372, -0.0957],\n",
      "        [ 0.7012,  1.5016,  0.0513,  ..., -1.3722,  0.3077, -0.5693]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "src.shape: torch.Size([610, 64])\n",
      "src: tensor([[ 0.1400,  1.0546,  1.0634,  ..., -1.1045, -1.3633,  0.3525],\n",
      "        [ 1.0973,  1.1088,  0.3908,  ..., -0.7576,  1.0406,  0.3367],\n",
      "        [ 0.4720,  0.9814,  0.4514,  ...,  0.7214, -0.7344,  0.1905],\n",
      "        ...,\n",
      "        [-0.3643,  1.0382, -1.4448,  ..., -0.5761, -1.3072,  1.5188],\n",
      "        [-0.1337, -0.0369,  0.0053,  ..., -1.0227,  0.4914,  1.1152],\n",
      "        [-1.4048,  0.2001,  0.3864,  ..., -1.2904, -0.5426, -0.5389]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "src.shape: torch.Size([2758, 64])\n",
      "src: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1375, 1.1725, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1063,  ..., 0.0000, 1.1509, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0715,  ..., 0.0000, 0.6105, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 1.4560,  ..., 0.0000, 1.5851, 1.2509],\n",
      "        [0.2946, 0.8291, 0.0000,  ..., 1.0853, 0.3640, 0.0000],\n",
      "        [0.7112, 0.6170, 0.0000,  ..., 0.3170, 0.6745, 0.3985]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "src.shape: torch.Size([2758, 64])\n",
      "src: tensor([[0.0000, 0.1859, 0.4503,  ..., 0.4142, 0.0000, 1.2837],\n",
      "        [1.1885, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.6878, 0.5987, 0.0000,  ..., 0.0431, 0.0000, 0.7084],\n",
      "        ...,\n",
      "        [0.3720, 0.1311, 0.0000,  ..., 0.0000, 0.7425, 0.0000],\n",
      "        [0.2348, 1.0089, 0.0145,  ..., 0.0695, 0.3443, 1.1113],\n",
      "        [0.0000, 0.7580, 0.0000,  ..., 0.0000, 0.0000, 0.6453]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "src.shape: torch.Size([610, 64])\n",
      "src: tensor([[0.0000, 0.1859, 0.4503,  ..., 0.4142, 0.0000, 1.2837],\n",
      "        [1.1885, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.6878, 0.5987, 0.0000,  ..., 0.0431, 0.0000, 0.7084],\n",
      "        ...,\n",
      "        [0.3720, 0.1311, 0.0000,  ..., 0.0000, 0.7425, 0.0000],\n",
      "        [0.2348, 1.0089, 0.0145,  ..., 0.0695, 0.3443, 1.1113],\n",
      "        [0.0000, 0.7580, 0.0000,  ..., 0.0000, 0.0000, 0.6453]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "src.shape: torch.Size([610, 64])\n",
      "src: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.1375, 1.1725, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1063,  ..., 0.0000, 1.1509, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0715,  ..., 0.0000, 0.6105, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 1.4560,  ..., 0.0000, 1.5851, 1.2509],\n",
      "        [0.2946, 0.8291, 0.0000,  ..., 1.0853, 0.3640, 0.0000],\n",
      "        [0.7112, 0.6170, 0.0000,  ..., 0.3170, 0.6745, 0.3985]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "src.shape: torch.Size([2758, 64])\n",
      "x_dict 2: {'user': tensor([[-0.7311,  0.3533, -0.3488,  ...,  0.4522, -0.7004,  0.0315],\n",
      "        [-0.1619,  0.0306,  0.1150,  ...,  0.6764, -0.1873, -0.0900],\n",
      "        [-0.0383,  0.8033, -0.5891,  ...,  0.3581, -0.4166, -0.1739],\n",
      "        ...,\n",
      "        [-0.2611,  0.0714,  0.3433,  ..., -0.0209, -0.4023,  0.3636],\n",
      "        [-0.1628,  0.1878, -0.0845,  ...,  0.0126, -0.4477,  0.0224],\n",
      "        [-0.3117,  0.3797,  0.2043,  ...,  0.0669, -0.5723,  0.0071]],\n",
      "       grad_fn=<AddBackward0>), 'movie': tensor([[-0.3365, -0.8075, -0.4239,  ...,  0.0016,  0.1889,  0.2284],\n",
      "        [ 0.2502, -0.4253, -0.2736,  ...,  0.0818,  0.0450,  0.0150],\n",
      "        [-0.0429, -0.2289, -0.3256,  ...,  0.0838,  0.1175,  0.2123],\n",
      "        ...,\n",
      "        [-0.3725, -0.0371, -0.4341,  ...,  0.0955, -0.2751, -0.2292],\n",
      "        [-0.2607, -0.3704,  0.2146,  ...,  0.1653, -0.5406,  0.1088],\n",
      "        [-0.0170, -0.0363, -0.1131,  ...,  0.0388, -0.0264, -0.1712]],\n",
      "       grad_fn=<AddBackward0>)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.8000e-02, -3.2009e-01, -8.7974e-01, -7.8132e-01,  1.9838e-01,\n",
       "         1.4694e+00, -6.8347e-01, -6.7994e-01, -3.5775e-02, -1.3441e+00,\n",
       "         2.6486e-01, -6.2920e-01,  6.8263e-02,  3.6945e-01, -6.5919e-02,\n",
       "        -6.7867e-01, -1.9387e-01, -4.9250e-01,  1.1928e-01, -1.3715e-01,\n",
       "        -1.1608e-01, -4.1062e-01, -1.8796e+00, -3.1967e-01, -3.3600e-01,\n",
       "        -5.0233e-01, -3.1731e-01, -1.2445e+00,  9.4310e-01,  7.4915e-01,\n",
       "        -1.5191e-01,  2.8177e-01, -1.0342e+00,  1.3886e+00, -1.6083e+00,\n",
       "         8.7437e-01, -3.4305e-02, -7.9155e-01,  3.6961e-01,  7.0661e-01,\n",
       "        -9.2525e-02, -3.9848e-01, -5.3912e-01, -6.9309e-01, -3.1650e-01,\n",
       "         9.3210e-01, -1.2362e+00, -3.2783e-01,  1.8021e-01,  1.1343e-01,\n",
       "        -1.2535e+00, -1.2975e+00, -1.0150e+00, -8.0170e-01, -5.3207e-01,\n",
       "         6.2693e-01, -7.4344e-01, -2.1599e+00,  8.6293e-01, -1.1305e+00,\n",
       "        -1.6756e+00,  7.1698e-01, -8.0521e-01, -3.6644e-01,  4.7580e-02,\n",
       "         3.9132e-01, -5.4051e-01, -1.8751e-01,  3.4749e-01, -1.3846e-01,\n",
       "        -1.5897e-01, -1.2010e-01, -7.7561e-01, -5.2577e-01, -2.2757e-01,\n",
       "         5.6467e-01, -5.2934e-01, -5.4647e-01, -4.9786e-02, -6.4025e-01,\n",
       "        -9.7139e-01, -5.9532e-01, -6.8598e-01, -9.8266e-01, -5.1791e-01,\n",
       "        -8.0426e-01, -5.4703e-02, -3.3819e-01, -1.4997e+00, -8.7063e-02,\n",
       "         7.8336e-01,  1.8172e-01, -1.3916e+00,  3.4479e-01, -3.8924e-01,\n",
       "         3.4260e-01, -7.5479e-01, -2.4759e-01, -1.2203e+00, -3.1355e-01,\n",
       "        -1.4609e+00, -7.2805e-01, -1.0728e+00, -6.9453e-01,  1.1543e-01,\n",
       "        -6.4193e-01, -7.3469e-01, -2.2880e-01,  3.4348e-01, -3.0457e-01,\n",
       "        -9.8983e-01, -7.3001e-01, -8.3035e-01,  3.5685e-01, -1.0087e+00,\n",
       "        -9.8384e-02, -6.1926e-01,  1.2985e+00,  6.1748e-02,  2.8520e-02,\n",
       "        -8.1802e-01, -7.0881e-01, -1.2255e+00, -8.1761e-02, -2.6913e-01,\n",
       "        -4.9389e-01, -1.8076e+00, -1.4906e-01,  2.3427e-02, -3.9147e-01,\n",
       "        -2.9703e-01,  2.0634e-01,  1.5337e-01, -2.0059e+00, -4.7897e-02,\n",
       "        -1.9507e+00,  1.1855e-01,  1.2728e+00,  2.3178e-01, -7.8420e-01,\n",
       "        -1.6848e-01, -1.8764e+00, -2.2509e-01, -5.6551e-01, -2.2481e-01,\n",
       "        -3.3103e-01, -2.6866e-01, -1.0775e+00, -5.0604e-01,  5.5744e-02,\n",
       "        -1.1647e+00, -9.1015e-01, -3.6980e-01,  7.4085e-01, -1.4502e+00,\n",
       "        -6.8681e-01, -1.1337e+00, -3.3471e-01, -3.5911e-01,  9.4598e-01,\n",
       "        -4.3554e-01, -1.0409e+00, -2.7653e+00,  3.7836e-01, -1.8115e+00,\n",
       "        -1.7648e-01, -7.7114e-03,  1.3110e-01, -8.0895e-01, -5.8537e-01,\n",
       "        -2.3919e-01,  1.0434e+00, -2.3618e+00, -7.3349e-01,  6.3400e-01,\n",
       "        -4.0796e-01, -5.8447e-02,  2.5582e-01, -1.6989e+00,  4.2131e-01,\n",
       "        -1.6332e+00, -9.0152e-01, -5.0122e-01, -5.1611e-01, -1.6272e+00,\n",
       "        -1.7382e+00,  2.2306e-01, -6.4881e-01, -8.1934e-01, -1.0097e+00,\n",
       "        -7.9471e-01, -1.6671e+00,  8.3987e-01,  4.2374e-01, -2.0719e-01,\n",
       "         5.3709e-01, -2.4564e-01, -1.6170e+00, -1.4676e+00, -3.9083e-01,\n",
       "        -1.5056e+00,  3.5732e-01, -3.2795e-01, -1.3843e+00,  9.7755e-01,\n",
       "        -4.8353e-01, -5.7558e-01, -2.0136e-02, -1.8013e-01, -5.9100e-01,\n",
       "        -9.5264e-02,  1.1280e-01, -4.2374e-01, -4.5490e-01, -1.5269e-01,\n",
       "        -1.1343e+00,  4.7785e-01, -3.0309e-01, -1.2773e+00, -2.2282e-01,\n",
       "         2.0824e-01, -4.5415e-01,  1.5478e-01,  1.6319e-01, -2.4885e-01,\n",
       "         1.3297e+00, -4.8118e-01, -1.2636e+00, -2.9711e-01, -8.2851e-01,\n",
       "        -2.6341e+00, -2.7359e-01,  2.0813e-01, -3.3307e-01,  1.1782e+00,\n",
       "        -7.5555e-01, -1.5495e-01, -4.7106e-01, -1.0429e-02, -4.8533e-01,\n",
       "        -5.2078e-01, -3.3312e-01, -8.0151e-01, -1.0627e+00, -8.5868e-01,\n",
       "        -7.0142e-01, -1.5184e+00,  5.6680e-01, -2.8738e-01,  1.2959e-01,\n",
       "        -5.6129e-01, -5.3964e-01,  2.8012e-01, -8.3667e-01, -9.5430e-01,\n",
       "        -1.3647e-01, -1.4147e+00, -1.0708e-01, -1.3300e-01, -4.2328e-01,\n",
       "        -3.0104e-01, -3.2971e-01,  2.3749e-01, -1.4113e+00, -1.4727e+00,\n",
       "        -1.1727e+00, -1.7816e-01,  4.1638e-01, -1.0724e+00, -1.8103e+00,\n",
       "        -1.0171e-01, -3.9810e-01, -5.9692e-02, -8.7505e-01, -4.2786e-02,\n",
       "        -9.8237e-01, -6.0879e-01, -7.2479e-01, -1.2463e+00, -3.1568e-01,\n",
       "        -1.4965e+00,  5.7961e-02,  4.8915e-01,  6.1071e-01, -1.7284e-01,\n",
       "        -3.5342e-01, -1.5437e+00, -1.2031e+00, -4.7120e-01, -5.5726e-01,\n",
       "        -1.0500e+00, -2.4381e-02, -7.7493e-01, -1.6604e+00, -1.6068e+00,\n",
       "        -1.0445e+00, -1.3452e+00, -3.1012e-01, -2.0166e-01, -2.2817e+00,\n",
       "         5.7737e-02, -4.8339e-01,  8.7603e-03,  5.7545e-01, -1.9147e+00,\n",
       "        -3.7181e-01, -1.0153e+00,  8.3701e-01, -7.6535e-01, -7.2183e-02,\n",
       "         1.9830e-01, -1.7424e-01, -6.5728e-01, -1.1490e+00, -2.7559e-01,\n",
       "         2.4868e-03, -1.1468e+00, -1.6718e-01, -1.2405e+00, -4.8778e-01,\n",
       "        -1.4164e+00, -8.7111e-01, -4.5297e-02, -2.4240e-02,  3.0867e-01,\n",
       "         1.7332e+00, -1.4758e+00, -5.8761e-01,  1.1294e+00, -9.6296e-01,\n",
       "        -1.0998e+00,  1.6389e-01,  4.6663e-01, -8.6818e-01, -8.3025e-01,\n",
       "        -3.6824e-01,  1.4241e+00,  7.9018e-01, -4.8177e-01,  9.8994e-01,\n",
       "         1.3431e+00, -5.8209e-01,  1.2688e-02,  1.3479e+00,  1.0849e-01,\n",
       "        -1.0629e+00,  1.2751e-01, -2.0505e+00,  6.6694e-01, -1.0501e+00,\n",
       "         8.6224e-02,  1.9672e-02, -1.0365e+00,  3.1860e-01, -7.4412e-01,\n",
       "         1.4929e-01, -1.7340e-01, -4.3394e-01, -5.0584e-02,  4.4811e-01,\n",
       "        -1.3316e+00, -7.9912e-01, -3.1795e-01, -6.3018e-01, -7.6636e-01,\n",
       "        -3.1813e-01, -7.9693e-01, -1.2051e+00, -5.8467e-01,  5.5570e-01,\n",
       "         4.0848e-01, -7.5533e-01, -1.5824e+00, -1.5407e+00, -1.5031e-01,\n",
       "         2.3081e-01, -2.5714e-01, -6.4619e-01, -3.2302e-01, -2.6363e-01,\n",
       "        -4.3053e-01, -4.2191e-01, -3.2794e-01, -6.9609e-01],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hetero_gnn = Model(hidden_channels=64)\n",
    "# hetero_gnn(x_dict, het.edge_index_dict)\n",
    "hetero_gnn(het)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
