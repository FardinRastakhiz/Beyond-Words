{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Scripts.DataManager.DabasePreparations.AmazonReviewSentiGraph import AmazonReviewSentiGraph\n",
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "config = Config(r'C:\\Users\\fardin\\Projects\\ColorIntelligence')\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "# os.environ['TORCH_USE_CUDA_DSA']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = 'cuda'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from typing import List, Dict, Tuple\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from torch_geometric.utils import to_networkx\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor\n",
    "from torch_geometric.data import Data , HeteroData\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "import spacy\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class TagDepTokenGraphConstructor(GraphConstructor):\n",
    "\n",
    "    class _Variables(GraphConstructor._Variables):\n",
    "        def __init__(self):\n",
    "            super(TagDepTokenGraphConstructor._Variables, self).__init__()\n",
    "            self.nlp_pipeline: str = ''\n",
    "    def __init__(self, texts: List[str], save_path: str, config: Config,\n",
    "                 lazy_construction=True, load_preprocessed_data=False, naming_prepend='' , use_compression=True, use_sentence_nodes=False, use_general_node=True):\n",
    "\n",
    "        super(TagDepTokenGraphConstructor, self)\\\n",
    "            .__init__(texts, self._Variables(), save_path, config, lazy_construction, load_preprocessed_data,\n",
    "                      naming_prepend , use_compression)\n",
    "        self.settings = {\"dep_token_weight\" : 1, \"token_token_weight\" : 2, \"tag_token_weight\" : 1, \"general_token_weight\" : 1, \"general_sentence_weight\" : 1, \"sentence_token_weight\" : 1}\n",
    "        self.use_sentence_nodes = use_sentence_nodes\n",
    "        self.use_general_node = use_general_node\n",
    "        self.var.nlp_pipeline = self.config.spacy.pipeline\n",
    "        self.var.graph_num = len(self.raw_data)\n",
    "        self.nlp = spacy.load(self.var.nlp_pipeline)\n",
    "        self.dependencies = self.nlp.get_pipe(\"parser\").labels\n",
    "        self.tags = self.nlp.get_pipe(\"tagger\").labels\n",
    "            \n",
    "    def setup(self, load_preprocessed_data = True):\n",
    "        self.load_preprocessed_data = True\n",
    "        if load_preprocessed_data:\n",
    "            self.load_var()\n",
    "            self.num_data_load = self.var.graph_num if self.num_data_load > self.var.graph_num else self.num_data_load\n",
    "            if not self.lazy_construction:\n",
    "                for i in range(self.num_data_load):\n",
    "                    if i%100 == 0:\n",
    "                        print(f' {i} graph loaded')\n",
    "                    if (self._graphs[i] is None) and (i in self.var.graphs_name):\n",
    "                        self.load_data_compressed(i)\n",
    "                    else:\n",
    "                        self._graphs[i] = self.to_graph(self.raw_data[i])\n",
    "                        self.var.graphs_name[i] = f'{self.naming_prepend}_{i}'\n",
    "                        self.save_data_compressed(i)\n",
    "                self.var.save_to_file(os.path.join(self.save_path, f'{self.naming_prepend}_var.txt'))\n",
    "        else:\n",
    "            if not self.lazy_construction:\n",
    "                save_start = 0\n",
    "                self.num_data_load = len(self.raw_data) if self.num_data_load > len(self.raw_data) else self.num_data_load\n",
    "                for i in range(self.num_data_load):\n",
    "                    if i not in self._graphs:\n",
    "                        if i % 100 == 0:\n",
    "                            self.save_data_range(save_start, i)\n",
    "                            save_start = i\n",
    "                            print(f'i: {i}')\n",
    "                        self._graphs[i] = self.to_graph(self.raw_data[i])\n",
    "                        self.var.graphs_name[i] = f'{self.naming_prepend}_{i}'\n",
    "                self.save_data_range(save_start, self.num_data_load)\n",
    "            self.var.save_to_file(os.path.join(self.save_path, f'{self.naming_prepend}_var.txt'))\n",
    "\n",
    "\n",
    "    def to_graph(self, text: str):\n",
    "        doc = self.nlp(text)\n",
    "        if len(doc) < 2:\n",
    "            return\n",
    "        if self.use_sentence_nodes:\n",
    "            self.__create_graph_with_sentences(doc)\n",
    "        else:\n",
    "            self.__create_graph(doc,use_general_node=self.use_general_node)\n",
    "    \n",
    "    def __find_dep_index(self , dependency : str):\n",
    "        for dep_idx in range(len(self.dependencies)):\n",
    "            if self.dependencies[dep_idx] == dependency:\n",
    "                return dep_idx\n",
    "        return -1 # means not found\n",
    "    def __build_initial_dependency_vectors(self , dep_length : int):\n",
    "        return torch.zeros((dep_length, self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "    def __find_tag_index(self , tag : str):\n",
    "        for tag_idx in range(len(self.tags)):\n",
    "            if self.tags[tag_idx] == tag:\n",
    "                return tag_idx\n",
    "        return -1 # means not found\n",
    "    def __build_initial_tag_vectors(self , tags_length : int):\n",
    "        return torch.zeros((tags_length, self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "    def __build_initial_general_vector(self):\n",
    "        return torch.zeros((1 , self.nlp.vocab.vectors_length), dtype=torch.float32)    \n",
    "    def __create_graph_with_sentences(self , doc , for_compression=False):\n",
    "        graph = self.create_graph(doc,for_compression,False)\n",
    "        # TODO : add sentence nodes here\n",
    "        pass\n",
    "    def __create_graph(self , doc , for_compression=False, use_general_node=True):\n",
    "        # nodes size is dependencies + tokens\n",
    "        data = HeteroData()\n",
    "        dep_length = len(self.dependencies)\n",
    "        tag_length = len(self.tags)\n",
    "        if for_compression:\n",
    "            data['dep'].x = torch.full((dep_length,),-1, dtype=torch.float32)\n",
    "            data['word'].x = torch.full((len(doc),),-1, dtype=torch.float32)\n",
    "            data['tag'].x = torch.full((tag_length,), -1, dtype=torch.float32)\n",
    "            if use_general_node:\n",
    "                data['general'].x = torch.full((1,),-1, dtype=torch.float32)\n",
    "        else:\n",
    "            data['dep'].x = self.__build_initial_dependency_vectors(dep_length)\n",
    "            data['word'].x = torch.zeros((len(doc) , self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "            data['tag'].x = self.__build_initial_tag_vectors(tag_length)\n",
    "            if use_general_node:\n",
    "                data['general'].x = self.__build_initial_general_vector()\n",
    "        word_dep_edge_index = []\n",
    "        dep_word_edge_index = []\n",
    "        word_tag_edge_index = []\n",
    "        tag_word_edge_index = []\n",
    "        word_word_edge_index = []\n",
    "        word_general_edge_index = []\n",
    "        general_word_edge_index = []\n",
    "        word_dep_edge_attr = []\n",
    "        dep_word_edge_attr = []\n",
    "        word_tag_edge_attr = []\n",
    "        tag_word_edge_attr = []\n",
    "        word_word_edge_attr = []\n",
    "        word_general_edge_attr = []\n",
    "        general_word_edge_attr = []\n",
    "        for token in doc:\n",
    "            token_id = self.nlp.vocab.strings[token.lemma_]\n",
    "            if token_id in self.nlp.vocab.vectors:\n",
    "                if for_compression:\n",
    "                    data['word'].x[token.i] = torch.tensor(token_id , dtype=torch.float32)\n",
    "                else:\n",
    "                    data['word'].x[token.i] = torch.tensor(self.nlp.vocab.vectors[token_id])\n",
    "            # adding dependency edges\n",
    "            if token.dep_ != 'ROOT':\n",
    "                dep_idx = self.__find_dep_index(token.dep_)\n",
    "                if dep_idx != -1:\n",
    "                    word_dep_edge_index.append([token.head.i , dep_idx])\n",
    "                    word_dep_edge_attr.append(self.settings[\"dep_token_weight\"])\n",
    "                    dep_word_edge_index.append([dep_idx , token.i])\n",
    "                    dep_word_edge_attr.append(self.settings[\"dep_token_weight\"])\n",
    "            # adding tag edges\n",
    "            tag_idx = self.__find_tag_index(token.tag_)\n",
    "            if tag_idx != -1:\n",
    "                word_tag_edge_index.append([token.i , tag_idx])\n",
    "                word_tag_edge_attr.append(self.settings[\"tag_token_weight\"])\n",
    "                tag_word_edge_index.append([tag_idx , token.i])\n",
    "                tag_word_edge_attr.append(self.settings[\"tag_token_weight\"])\n",
    "            # adding sequence edges\n",
    "            if token.i != len(doc) - 1:\n",
    "                # using zero vectors for edge features\n",
    "                word_word_edge_index.append([token.i , token.i + 1])\n",
    "                word_word_edge_attr.append(self.settings[\"token_token_weight\"])\n",
    "                word_word_edge_index.append([token.i + 1 , token.i])\n",
    "                word_word_edge_attr.append(self.settings[\"token_token_weight\"])\n",
    "            # adding general node edges\n",
    "            if use_general_node:\n",
    "                word_general_edge_index.append([token.i , 0])\n",
    "                word_general_edge_attr.append(self.settings[\"general_token_weight\"])\n",
    "                general_word_edge_index.append([0 , token.i])\n",
    "                general_word_edge_attr.append(self.settings[\"general_token_weight\"])\n",
    "        data['dep' , 'dep_word' , 'word'].edge_index = torch.transpose(torch.tensor(dep_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "        data['word' , 'word_dep' , 'dep'].edge_index = torch.transpose(torch.tensor(word_dep_edge_index, dtype=torch.long) , 0 , 1)\n",
    "        data['tag', 'tag_word', 'word'].edge_index = torch.transpose(torch.tensor(tag_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "        data['word', 'word_tag', 'tag'].edge_index = torch.transpose(torch.tensor(word_tag_edge_index, dtype=torch.long) , 0 , 1)\n",
    "        data['word' , 'seq' , 'word'].edge_index = torch.transpose(torch.tensor(word_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "        data['dep' , 'dep_word' , 'word'].edge_attr = dep_word_edge_attr\n",
    "        data['word' , 'word_dep' , 'dep'].edge_attr = word_dep_edge_attr\n",
    "        data['tag', 'tag_word', 'word'].edge_attr = tag_word_edge_attr\n",
    "        data['word', 'word_tag', 'tag'].edge_attr = word_tag_edge_attr\n",
    "        data['word' , 'seq' , 'word'].edge_attr = word_word_edge_attr\n",
    "        if use_general_node:\n",
    "            data['general' , 'general_word' , 'word'].edge_index = torch.transpose(torch.tensor(general_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "            data['word' , 'word_general' , 'general'].edge_index = torch.transpose(torch.tensor(word_general_edge_index, dtype=torch.long) , 0 , 1)\n",
    "            data['general' , 'general_word' , 'word'].edge_attr = general_word_edge_attr\n",
    "            data['word' , 'word_general' , 'general'].edge_attr = word_general_edge_attr\n",
    "        return data\n",
    "    def draw_graph(self , idx : int):\n",
    "        # TODO : do this part if needed\n",
    "        pass\n",
    "    def to_graph_indexed(self, text: str):\n",
    "        doc = self.nlp(text)\n",
    "        if len(doc) < 2:\n",
    "            return\n",
    "        if self.use_sentence_nodes:\n",
    "            return self.__create_graph_with_sentences(doc , for_compression=True)\n",
    "        else:\n",
    "            return self.__create_graph(doc,for_compression=True,use_general_node=self.use_general_node)\n",
    "    def convert_indexed_nodes_to_vector_nodes(self, graph):\n",
    "        print(graph)\n",
    "        if self.use_sentence_nodes:\n",
    "            # TODO : complete this part with sentences later\n",
    "            pass\n",
    "        else:\n",
    "            words = torch.zeros((len(graph['word'].x) , self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "            for i in range(len(graph['word'].x)):\n",
    "                if graph['word'].x[i] in self.nlp.vocab.vectors:\n",
    "                    words[i] = torch.tensor(self.nlp.vocab.vectors[graph['word'].x[i]])\n",
    "                else:\n",
    "                    words[i] = torch.zeros((self.nlp.vocab.vectors_length) , dtype=torch.float32)\n",
    "            graph['word'].x = words\n",
    "            graph['dep'].x = self.__build_initial_dependency_vectors(len(self.dependencies))\n",
    "            graph['tag'].x = self.__build_initial_tag_vectors(len(self.tags))\n",
    "            if self.use_general_node:\n",
    "                graph['general'].x = self.__build_initial_general_vector()\n",
    "        return graph\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True False\n",
      "False True True False\n"
     ]
    }
   ],
   "source": [
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "test_type = TextGraphType.CO_OCCURRENCE | TextGraphType.DEPENDENCY | TextGraphType.SEQUENTIAL\n",
    "print((TextGraphType.CO_OCCURRENCE in test_type), (TextGraphType.DEPENDENCY in test_type), (TextGraphType.SEQUENTIAL in test_type), (TextGraphType.TAGS in test_type))\n",
    "test_type = test_type - (TextGraphType.TAGS | TextGraphType.CO_OCCURRENCE)\n",
    "print((TextGraphType.CO_OCCURRENCE in test_type), (TextGraphType.DEPENDENCY in test_type), (TextGraphType.SEQUENTIAL in test_type), (TextGraphType.TAGS in test_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import numpy as np\n",
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "\n",
    "from Scripts.DataManager.GraphConstructor.CoOccurrenceGraphConstructor import CoOccurrenceGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.TagDepTokenGraphConstructor import TagDepTokenGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.DependencyGraphConstructor import DependencyGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.TagsGraphConstructor import TagsGraphConstructor\n",
    "\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor, TextGraphType\n",
    "from Scripts.DataManager.GraphLoader.GraphDataModule import GraphDataModule\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "from Scripts.DataManager.Datasets.GraphConstructorDataset import GraphConstructorDataset\n",
    "\n",
    "class AmazonReviewGraphDataModule(GraphDataModule):\n",
    "\n",
    "    def __init__(self, config: Config, has_val: bool, has_test: bool, test_size=0.2, val_size=0.2, num_workers=2,\n",
    "                 drop_last=True, train_data_path='', test_data_path='', graphs_path='', batch_size = 32,\n",
    "                 device='cpu', shuffle = False, num_data_load=-1,\n",
    "                 graph_type: TextGraphType = TextGraphType.CO_OCCURRENCE | TextGraphType.DEPENDENCY | TextGraphType.TAGS, *args, **kwargs):\n",
    "        # kwargs['num_workers'] = num_workers\n",
    "        # kwargs['batch_size'] = batch_size\n",
    "        # kwargs['shuffle'] = shuffle\n",
    "        super(AmazonReviewGraphDataModule, self)\\\n",
    "            .__init__(config, device, has_val, has_test, test_size, val_size, *args, **kwargs)\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.graph_type = graph_type\n",
    "        self.train_data_path = 'data/Amazon-Review/train_sm.csv' if train_data_path == '' else train_data_path\n",
    "        self.test_data_path = 'data/Amazon-Review/test_sm.csv' if test_data_path == '' else test_data_path\n",
    "        self.train_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.test_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.labels = None\n",
    "        self.dataset = None\n",
    "        self.shuffle = shuffle\n",
    "        self.num_node_features = 0\n",
    "        self.num_classes = 0\n",
    "        self.df: pd.DataFrame = pd.DataFrame()\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset = None, None, None\n",
    "        self.train_df = pd.read_csv(path.join(self.config.root, self.train_data_path))\n",
    "        self.test_df = pd.read_csv(path.join(self.config.root, self.test_data_path))\n",
    "        self.train_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.test_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.train_df = self.train_df[['Polarity', 'Review']]\n",
    "        self.test_df = self.test_df[['Polarity', 'Review']]\n",
    "        self.df = pd.concat([self.train_df, self.test_df])\n",
    "        self.num_data_load = num_data_load if num_data_load>0 else self.df.shape[0]\n",
    "        self.num_data_load = num_data_load if self.num_data_load < self.df.shape[0] else self.df.shape[0] \n",
    "        self.df = self.df.iloc[:self.num_data_load]\n",
    "        self.df.index = np.arange(0, self.num_data_load)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.graph_constructors = self.__set_graph_constructors(self.graph_type)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        graph_constructor.setup()\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        labels = self.df['Polarity'][:self.num_data_load]\n",
    "        labels = labels.apply(lambda p: 0 if p == 1 else 1).to_numpy()\n",
    "        labels = torch.from_numpy(labels)\n",
    "        self.labels = labels.to(torch.float32).view(-1, 1).to(self.device)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        \n",
    "        print(f'self.labels.shape: {self.labels.shape}')\n",
    "        self.dataset = GraphConstructorDataset(graph_constructor, self.labels)\n",
    "        sample_graph = graph_constructor.get_first()\n",
    "        self.num_node_features = sample_graph.num_features\n",
    "        self.num_classes = len(torch.unique(self.labels))\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset =\\\n",
    "            random_split(self.dataset, [1-self.val_size-self.test_size, self.val_size, self.test_size])\n",
    "        self.__train_dataloader =  DataLoader(self.__train_dataset, batch_size=self.batch_size, drop_last=self.drop_last, shuffle=self.shuffle, num_workers=0, persistent_workers=False)\n",
    "        self.__test_dataloader =  DataLoader(self.__test_dataset, batch_size=self.batch_size, num_workers=0, persistent_workers=False)\n",
    "        self.__val_dataloader =  DataLoader(self.__val_dataset, batch_size=self.batch_size, num_workers=0, persistent_workers=False)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.__train_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.__test_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.__val_dataloader \n",
    "\n",
    "    def __set_graph_constructors(self, graph_type: TextGraphType):\n",
    "        graph_type = copy(graph_type)\n",
    "        graph_constructors: Dict[TextGraphType, GraphConstructor] = {}\n",
    "        \n",
    "        if TextGraphType.CO_OCCURRENCE in graph_type:\n",
    "            graph_constructors[TextGraphType.CO_OCCURRENCE] = self.__get_co_occurrence_graph()\n",
    "            graph_type = graph_type - TextGraphType.CO_OCCURRENCE\n",
    "        \n",
    "        tag_dep_seq = TextGraphType.DEPENDENCY | TextGraphType.TAGS | TextGraphType.SEQUENTIAL\n",
    "        if tag_dep_seq in graph_type:\n",
    "            graph_constructors[tag_dep_seq] = self.__get_dep_and_tag_graph()\n",
    "            graph_type = graph_type - tag_dep_seq\n",
    "        \n",
    "        if TextGraphType.DEPENDENCY in graph_type:\n",
    "            graph_constructors[TextGraphType.DEPENDENCY] = self.__get_dependency_graph()\n",
    "            graph_type = graph_type - (TextGraphType.DEPENDENCY | TextGraphType.SEQUENTIAL)\n",
    "        \n",
    "        if TextGraphType.TAGS in graph_type:\n",
    "            graph_constructors[TextGraphType.TAGS] = self.__get_tag_graph()\n",
    "            graph_type = graph_type - (TextGraphType.TAGS | TextGraphType.SEQUENTIAL)\n",
    "        \n",
    "        if TextGraphType.SEQUENTIAL in graph_type:\n",
    "            graph_constructors[TextGraphType.SEQUENTIAL] = self.__get_Sequential_graph()\n",
    "            graph_type = graph_type - TextGraphType.SEQUENTIAL\n",
    "            \n",
    "        return graph_constructors\n",
    "    \n",
    "    \n",
    "\n",
    "    def __get_co_occurrence_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return CoOccurrenceGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def __get_dependency_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return DependencyGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def __get_tag_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return TagsGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def __get_Sequential_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return DependencyGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def __get_dep_and_tag_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return TagDepTokenGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview2', self.config, lazy_construction=False, load_preprocessed_data=False, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    \n",
    "    def zero_rule_baseline(self):\n",
    "        return f'zero_rule baseline: {(len(self.labels[self.labels>0.5])* 100.0 / len(self.labels))  : .2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_data_load: 100\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TagDepTokenGraphConstructor.__init__() got an unexpected keyword argument 'num_data_load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Practices\\Tasks\\HeterogeneousGraphs\\TestHeterogeneousGraphClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tag_dep_seq \u001b[39m=\u001b[39m TextGraphType\u001b[39m.\u001b[39mDEPENDENCY \u001b[39m|\u001b[39m TextGraphType\u001b[39m.\u001b[39mTAGS \u001b[39m|\u001b[39m TextGraphType\u001b[39m.\u001b[39mSEQUENTIAL\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_manager \u001b[39m=\u001b[39m AmazonReviewGraphDataModule(config, \u001b[39mTrue\u001b[39;49;00m, \u001b[39mTrue\u001b[39;49;00m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_data_load \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size, graph_type\u001b[39m=\u001b[39;49mtag_dep_seq)\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Practices\\Tasks\\HeterogeneousGraphs\\TestHeterogeneousGraphClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39miloc[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_data_load]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdf\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_data_load)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_constructors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__set_graph_constructors(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph_type)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m graph_constructor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgraph_constructors[TextGraphType\u001b[39m.\u001b[39mCO_OCCURRENCE]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m graph_constructor\u001b[39m.\u001b[39msetup()\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Practices\\Tasks\\HeterogeneousGraphs\\TestHeterogeneousGraphClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m tag_dep_seq \u001b[39m=\u001b[39m TextGraphType\u001b[39m.\u001b[39mDEPENDENCY \u001b[39m|\u001b[39m TextGraphType\u001b[39m.\u001b[39mTAGS \u001b[39m|\u001b[39m TextGraphType\u001b[39m.\u001b[39mSEQUENTIAL\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m \u001b[39mif\u001b[39;00m tag_dep_seq \u001b[39min\u001b[39;00m graph_type:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m     graph_constructors[tag_dep_seq] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_dep_and_tag_graph()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m     graph_type \u001b[39m=\u001b[39m graph_type \u001b[39m-\u001b[39m tag_dep_seq\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m TextGraphType\u001b[39m.\u001b[39mDEPENDENCY \u001b[39min\u001b[39;00m graph_type:\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Practices\\Tasks\\HeterogeneousGraphs\\TestHeterogeneousGraphClassifier.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_dep_and_tag_graph\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mself.num_data_load: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_data_load\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Practices/Tasks/HeterogeneousGraphs/TestHeterogeneousGraphClassifier.ipynb#W3sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m TagDepTokenGraphConstructor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdf[\u001b[39m'\u001b[39;49m\u001b[39mReview\u001b[39;49m\u001b[39m'\u001b[39;49m][:\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_data_load], \u001b[39m'\u001b[39;49m\u001b[39mdata/GraphData/AmazonReview2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig, lazy_construction\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, load_preprocessed_data\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, naming_prepend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgraph\u001b[39;49m\u001b[39m'\u001b[39;49m, num_data_load\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_data_load, device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n",
      "\u001b[1;31mTypeError\u001b[0m: TagDepTokenGraphConstructor.__init__() got an unexpected keyword argument 'num_data_load'"
     ]
    }
   ],
   "source": [
    "tag_dep_seq = TextGraphType.DEPENDENCY | TextGraphType.TAGS | TextGraphType.SEQUENTIAL\n",
    "data_manager = AmazonReviewGraphDataModule(config, True, True, shuffle=True, num_data_load = 100, device='cpu', batch_size=batch_size, graph_type=tag_dep_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataloader = data_manager.train_dataloader()\n",
    "v_dataloader = data_manager.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = next(iter(t_dataloader))\n",
    "X2, y2 = next(iter(v_dataloader))\n",
    "print(X1[0].x.device)\n",
    "print(X2[0].x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroGCNConv(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, dropout = 0.57) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        self.conv2 = GATv2Conv(out_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        \n",
    "        self.batch_norm = BatchNorm(out_feature)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_weights: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_weights))\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weights)\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm, global_mean_pool, global_add_pool, global_max_pool, MemPooling, SAGEConv, to_hetero, HeteroBatchNorm\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HeteroGcnGatModel1(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_feature: int, out_features: int,\n",
    "                 metadata,\n",
    "                 base_hidden_feature: int=256,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(GNN, self).__init__()\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.bsh: int = base_hidden_feature\n",
    "        bsh2: int = int(self.bsh/2)\n",
    "        bsh4: int = int(self.bsh/4)\n",
    "        bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "        self.encoder = GSequential('x_dict, edge_index, edge_weights', [\n",
    "            (to_hetero(HeteroGCNConv(input_feature, self.bsh), metadata), 'x_dict, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, bsh2), metadata), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh4), metadata), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh8), metadata), 'x3, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "            (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "            (BatchNorm(bsh4), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "            (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "            (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (BatchNorm(bsh2), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "            (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "            (BatchNorm(self.bsh), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.input_features), 'x1, edge_index, edge_weights -> x1'),\n",
    "        ])\n",
    "        \n",
    "                \n",
    "        self.user_emb = torch.nn.Embedding(data['user'].num_nodes, self.input_features)\n",
    "        self.movie_emb = torch.nn.Embedding(data['movie'].num_nodes, self.input_features)\n",
    "        self.movie_lin = torch.nn.Linear(20, self.input_features)\n",
    "        \n",
    "    def forward(self, x: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(x[\"user\"].node_id),\n",
    "          \"movie\": self.movie_lin(x[\"movie\"].x) + self.movie_emb(x[\"movie\"].node_id)\n",
    "        }\n",
    "\n",
    "        x1_dict, x2_dict, x3_dict, x4_dict = self.encoder(x_dict, x.edge_index_dict, x.edge_attr_dict)\n",
    "        print(f'x4_dict[\"movie\"]: {x4_dict[\"movie\"].shape}, x_dict[\"movie\"]: {x_dict[\"movie\"].shape}')\n",
    "        x_att, x4 = self.attention(x3_dict[\"movie\"], x4_dict[\"movie\"], \n",
    "                                   x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                                   x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        x_dec = self.decoder(x1_dict[\"movie\"], x2_dict[\"movie\"], x_att, \n",
    "                             x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                             x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        return x_dict #x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = GcnGatModel1(300, 1, 1024, dropout=0.2)\n",
    "torch_model = torch_model.to(device)\n",
    "# print(next(iter(torch_model.parameters())).device)\n",
    "pre = torch_model(X1.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from Scripts.Models.LightningModels.LightningModels import BinaryLightningModel\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "    EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = BinaryLightningModel(torch_model,\n",
    "                                 torch.optim.Adam(torch_model.parameters(), lr=0.00017, weight_decay=0.00055),\n",
    "                                       torch.nn.BCEWithLogitsLoss(),\n",
    "                                       learning_rate=0.00017,\n",
    "                                       batch_size=batch_size,\n",
    "                                       ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=5000,\n",
    "            accelerator='gpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='GcnGatSentiment3'),\n",
    "            num_sanity_val_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n",
    "results = tuner.lr_find(lightning_model, datamodule=data_manager, min_lr=0.00001,max_lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = results.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(lightning_model, datamodule=data_manager)\n",
    "trainer.fit(lightning_model,\n",
    "            datamodule=data_manager\n",
    "            # train_dataloaders=data_manager.train_dataloader(),\n",
    "            # val_dataloaders=data_manager.val_dataloader(),\n",
    "            # logger=CSVLogger(save_dir='logs/', name='sample_model'),\n",
    "            # default_root_dir=\"~/Desktop\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "'''From torch lightning tutorials'''\n",
    "def plot_csv_logger(csv_path, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "    metrics = pd.read_csv(csv_path)\n",
    "    \n",
    "    aggregation_metrics = []\n",
    "    agg_col = 'epoch'\n",
    "    for i, dfg in metrics.groupby(agg_col):\n",
    "        agg = dict(dfg.mean())\n",
    "        agg[agg_col] = i\n",
    "        aggregation_metrics.append(agg)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "    df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "    df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_csv_logger(r'C:\\Users\\fardin\\Projects\\ColorIntelligence\\logs\\GcnGatSentiment3\\version_4\\metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
