{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from Scripts.DataManager.DabasePreparations.AmazonReviewSentiGraph import AmazonReviewSentiGraph\n",
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "config = Config(r'C:\\Users\\fardin\\Projects\\ColorIntelligence')\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "# os.environ['TORCH_USE_CUDA_DSA']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = 'cuda'\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from typing import List, Dict, Tuple\n",
    "# import networkx as nx\n",
    "# import pandas as pd\n",
    "# from torch_geometric.utils import to_networkx\n",
    "# from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor\n",
    "# from torch_geometric.data import Data , HeteroData\n",
    "# from Scripts.Configs.ConfigClass import Config\n",
    "# import spacy\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# class TagDepTokenGraphConstructor(GraphConstructor):\n",
    "\n",
    "#     class _Variables(GraphConstructor._Variables):\n",
    "#         def __init__(self):\n",
    "#             super(TagDepTokenGraphConstructor._Variables, self).__init__()\n",
    "#             self.nlp_pipeline: str = ''\n",
    "#     def __init__(self, texts: List[str], save_path: str, config: Config,\n",
    "#                  lazy_construction=True, load_preprocessed_data=False, naming_prepend='' , use_compression=True, use_sentence_nodes=False, use_general_node=True):\n",
    "\n",
    "#         super(TagDepTokenGraphConstructor, self)\\\n",
    "#             .__init__(texts, self._Variables(), save_path, config, lazy_construction, load_preprocessed_data,\n",
    "#                       naming_prepend , use_compression)\n",
    "#         self.settings = {\"dep_token_weight\" : 1, \"token_token_weight\" : 2, \"tag_token_weight\" : 1, \"general_token_weight\" : 1, \"general_sentence_weight\" : 1, \"sentence_token_weight\" : 1}\n",
    "#         self.use_sentence_nodes = use_sentence_nodes\n",
    "#         self.use_general_node = use_general_node\n",
    "#         self.var.nlp_pipeline = self.config.spacy.pipeline\n",
    "#         self.var.graph_num = len(self.raw_data)\n",
    "#         self.nlp = spacy.load(self.var.nlp_pipeline)\n",
    "#         self.dependencies = self.nlp.get_pipe(\"parser\").labels\n",
    "#         self.tags = self.nlp.get_pipe(\"tagger\").labels\n",
    "            \n",
    "#     def setup(self, load_preprocessed_data = True):\n",
    "#         self.load_preprocessed_data = True\n",
    "#         if load_preprocessed_data:\n",
    "#             self.load_var()\n",
    "#             self.num_data_load = self.var.graph_num if self.num_data_load > self.var.graph_num else self.num_data_load\n",
    "#             if not self.lazy_construction:\n",
    "#                 for i in range(self.num_data_load):\n",
    "#                     if i%100 == 0:\n",
    "#                         print(f' {i} graph loaded')\n",
    "#                     if (self._graphs[i] is None) and (i in self.var.graphs_name):\n",
    "#                         self.load_data_compressed(i)\n",
    "#                     else:\n",
    "#                         self._graphs[i] = self.to_graph(self.raw_data[i])\n",
    "#                         self.var.graphs_name[i] = f'{self.naming_prepend}_{i}'\n",
    "#                         self.save_data_compressed(i)\n",
    "#                 self.var.save_to_file(os.path.join(self.save_path, f'{self.naming_prepend}_var.txt'))\n",
    "#         else:\n",
    "#             if not self.lazy_construction:\n",
    "#                 save_start = 0\n",
    "#                 self.num_data_load = len(self.raw_data) if self.num_data_load > len(self.raw_data) else self.num_data_load\n",
    "#                 for i in range(self.num_data_load):\n",
    "#                     if i not in self._graphs:\n",
    "#                         if i % 100 == 0:\n",
    "#                             self.save_data_range(save_start, i)\n",
    "#                             save_start = i\n",
    "#                             print(f'i: {i}')\n",
    "#                         self._graphs[i] = self.to_graph(self.raw_data[i])\n",
    "#                         self.var.graphs_name[i] = f'{self.naming_prepend}_{i}'\n",
    "#                 self.save_data_range(save_start, self.num_data_load)\n",
    "#             self.var.save_to_file(os.path.join(self.save_path, f'{self.naming_prepend}_var.txt'))\n",
    "\n",
    "\n",
    "#     def to_graph(self, text: str):\n",
    "#         doc = self.nlp(text)\n",
    "#         if len(doc) < 2:\n",
    "#             return\n",
    "#         if self.use_sentence_nodes:\n",
    "#             self.__create_graph_with_sentences(doc)\n",
    "#         else:\n",
    "#             self.__create_graph(doc,use_general_node=self.use_general_node)\n",
    "    \n",
    "#     def __find_dep_index(self , dependency : str):\n",
    "#         for dep_idx in range(len(self.dependencies)):\n",
    "#             if self.dependencies[dep_idx] == dependency:\n",
    "#                 return dep_idx\n",
    "#         return -1 # means not found\n",
    "#     def __build_initial_dependency_vectors(self , dep_length : int):\n",
    "#         return torch.zeros((dep_length, self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "#     def __find_tag_index(self , tag : str):\n",
    "#         for tag_idx in range(len(self.tags)):\n",
    "#             if self.tags[tag_idx] == tag:\n",
    "#                 return tag_idx\n",
    "#         return -1 # means not found\n",
    "#     def __build_initial_tag_vectors(self , tags_length : int):\n",
    "#         return torch.zeros((tags_length, self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "#     def __build_initial_general_vector(self):\n",
    "#         return torch.zeros((1 , self.nlp.vocab.vectors_length), dtype=torch.float32)    \n",
    "#     def __create_graph_with_sentences(self , doc , for_compression=False):\n",
    "#         graph = self.create_graph(doc,for_compression,False)\n",
    "#         # TODO : add sentence nodes here\n",
    "#         pass\n",
    "#     def __create_graph(self , doc , for_compression=False, use_general_node=True):\n",
    "#         # nodes size is dependencies + tokens\n",
    "#         data = HeteroData()\n",
    "#         dep_length = len(self.dependencies)\n",
    "#         tag_length = len(self.tags)\n",
    "#         if for_compression:\n",
    "#             data['dep'].x = torch.full((dep_length,),-1, dtype=torch.float32)\n",
    "#             data['word'].x = torch.full((len(doc),),-1, dtype=torch.float32)\n",
    "#             data['tag'].x = torch.full((tag_length,), -1, dtype=torch.float32)\n",
    "#             if use_general_node:\n",
    "#                 data['general'].x = torch.full((1,),-1, dtype=torch.float32)\n",
    "#         else:\n",
    "#             data['dep'].x = self.__build_initial_dependency_vectors(dep_length)\n",
    "#             data['word'].x = torch.zeros((len(doc) , self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "#             data['tag'].x = self.__build_initial_tag_vectors(tag_length)\n",
    "#             if use_general_node:\n",
    "#                 data['general'].x = self.__build_initial_general_vector()\n",
    "#         word_dep_edge_index = []\n",
    "#         dep_word_edge_index = []\n",
    "#         word_tag_edge_index = []\n",
    "#         tag_word_edge_index = []\n",
    "#         word_word_edge_index = []\n",
    "#         word_general_edge_index = []\n",
    "#         general_word_edge_index = []\n",
    "#         word_dep_edge_attr = []\n",
    "#         dep_word_edge_attr = []\n",
    "#         word_tag_edge_attr = []\n",
    "#         tag_word_edge_attr = []\n",
    "#         word_word_edge_attr = []\n",
    "#         word_general_edge_attr = []\n",
    "#         general_word_edge_attr = []\n",
    "#         for token in doc:\n",
    "#             token_id = self.nlp.vocab.strings[token.lemma_]\n",
    "#             if token_id in self.nlp.vocab.vectors:\n",
    "#                 if for_compression:\n",
    "#                     data['word'].x[token.i] = torch.tensor(token_id , dtype=torch.float32)\n",
    "#                 else:\n",
    "#                     data['word'].x[token.i] = torch.tensor(self.nlp.vocab.vectors[token_id])\n",
    "#             # adding dependency edges\n",
    "#             if token.dep_ != 'ROOT':\n",
    "#                 dep_idx = self.__find_dep_index(token.dep_)\n",
    "#                 if dep_idx != -1:\n",
    "#                     word_dep_edge_index.append([token.head.i , dep_idx])\n",
    "#                     word_dep_edge_attr.append(self.settings[\"dep_token_weight\"])\n",
    "#                     dep_word_edge_index.append([dep_idx , token.i])\n",
    "#                     dep_word_edge_attr.append(self.settings[\"dep_token_weight\"])\n",
    "#             # adding tag edges\n",
    "#             tag_idx = self.__find_tag_index(token.tag_)\n",
    "#             if tag_idx != -1:\n",
    "#                 word_tag_edge_index.append([token.i , tag_idx])\n",
    "#                 word_tag_edge_attr.append(self.settings[\"tag_token_weight\"])\n",
    "#                 tag_word_edge_index.append([tag_idx , token.i])\n",
    "#                 tag_word_edge_attr.append(self.settings[\"tag_token_weight\"])\n",
    "#             # adding sequence edges\n",
    "#             if token.i != len(doc) - 1:\n",
    "#                 # using zero vectors for edge features\n",
    "#                 word_word_edge_index.append([token.i , token.i + 1])\n",
    "#                 word_word_edge_attr.append(self.settings[\"token_token_weight\"])\n",
    "#                 word_word_edge_index.append([token.i + 1 , token.i])\n",
    "#                 word_word_edge_attr.append(self.settings[\"token_token_weight\"])\n",
    "#             # adding general node edges\n",
    "#             if use_general_node:\n",
    "#                 word_general_edge_index.append([token.i , 0])\n",
    "#                 word_general_edge_attr.append(self.settings[\"general_token_weight\"])\n",
    "#                 general_word_edge_index.append([0 , token.i])\n",
    "#                 general_word_edge_attr.append(self.settings[\"general_token_weight\"])\n",
    "#         data['dep' , 'dep_word' , 'word'].edge_index = torch.transpose(torch.tensor(dep_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#         data['word' , 'word_dep' , 'dep'].edge_index = torch.transpose(torch.tensor(word_dep_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#         data['tag', 'tag_word', 'word'].edge_index = torch.transpose(torch.tensor(tag_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#         data['word', 'word_tag', 'tag'].edge_index = torch.transpose(torch.tensor(word_tag_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#         data['word' , 'seq' , 'word'].edge_index = torch.transpose(torch.tensor(word_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#         data['dep' , 'dep_word' , 'word'].edge_attr = dep_word_edge_attr\n",
    "#         data['word' , 'word_dep' , 'dep'].edge_attr = word_dep_edge_attr\n",
    "#         data['tag', 'tag_word', 'word'].edge_attr = tag_word_edge_attr\n",
    "#         data['word', 'word_tag', 'tag'].edge_attr = word_tag_edge_attr\n",
    "#         data['word' , 'seq' , 'word'].edge_attr = word_word_edge_attr\n",
    "#         if use_general_node:\n",
    "#             data['general' , 'general_word' , 'word'].edge_index = torch.transpose(torch.tensor(general_word_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#             data['word' , 'word_general' , 'general'].edge_index = torch.transpose(torch.tensor(word_general_edge_index, dtype=torch.long) , 0 , 1)\n",
    "#             data['general' , 'general_word' , 'word'].edge_attr = general_word_edge_attr\n",
    "#             data['word' , 'word_general' , 'general'].edge_attr = word_general_edge_attr\n",
    "#         return data\n",
    "#     def draw_graph(self , idx : int):\n",
    "#         # TODO : do this part if needed\n",
    "#         pass\n",
    "#     def to_graph_indexed(self, text: str):\n",
    "#         doc = self.nlp(text)\n",
    "#         if len(doc) < 2:\n",
    "#             return\n",
    "#         if self.use_sentence_nodes:\n",
    "#             return self.__create_graph_with_sentences(doc , for_compression=True)\n",
    "#         else:\n",
    "#             return self.__create_graph(doc,for_compression=True,use_general_node=self.use_general_node)\n",
    "#     def convert_indexed_nodes_to_vector_nodes(self, graph):\n",
    "#         print(graph)\n",
    "#         if self.use_sentence_nodes:\n",
    "#             # TODO : complete this part with sentences later\n",
    "#             pass\n",
    "#         else:\n",
    "#             words = torch.zeros((len(graph['word'].x) , self.nlp.vocab.vectors_length), dtype=torch.float32)\n",
    "#             for i in range(len(graph['word'].x)):\n",
    "#                 if graph['word'].x[i] in self.nlp.vocab.vectors:\n",
    "#                     words[i] = torch.tensor(self.nlp.vocab.vectors[graph['word'].x[i]])\n",
    "#                 else:\n",
    "#                     words[i] = torch.zeros((self.nlp.vocab.vectors_length) , dtype=torch.float32)\n",
    "#             graph['word'].x = words\n",
    "#             graph['dep'].x = self.__build_initial_dependency_vectors(len(self.dependencies))\n",
    "#             graph['tag'].x = self.__build_initial_tag_vectors(len(self.tags))\n",
    "#             if self.use_general_node:\n",
    "#                 graph['general'].x = self.__build_initial_general_vector()\n",
    "#         return graph\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True True False\n",
      "False True True False\n"
     ]
    }
   ],
   "source": [
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "test_type = TextGraphType.CO_OCCURRENCE | TextGraphType.DEPENDENCY | TextGraphType.SEQUENTIAL\n",
    "print((TextGraphType.CO_OCCURRENCE in test_type), (TextGraphType.DEPENDENCY in test_type), (TextGraphType.SEQUENTIAL in test_type), (TextGraphType.TAGS in test_type))\n",
    "test_type = test_type - (TextGraphType.TAGS | TextGraphType.CO_OCCURRENCE)\n",
    "print((TextGraphType.CO_OCCURRENCE in test_type), (TextGraphType.DEPENDENCY in test_type), (TextGraphType.SEQUENTIAL in test_type), (TextGraphType.TAGS in test_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.DataManager.GraphLoader.AmazonReviewGraphDataModule import AmazonReviewGraphDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_data_load: 200\n",
      "i: 0\n",
      "i: 100\n"
     ]
    }
   ],
   "source": [
    "tag_dep_seq = TextGraphType.DEPENDENCY | TextGraphType.TAGS | TextGraphType.SEQUENTIAL\n",
    "data_manager = AmazonReviewGraphDataModule(config, True, True, shuffle=True, num_data_load = 200, device='cpu', batch_size=batch_size, graph_type=tag_dep_seq, load_preprocessed_data = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataloader = data_manager.train_dataloader()\n",
    "v_dataloader = data_manager.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = next(iter(t_dataloader))\n",
    "X2, y2 = next(iter(v_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, BatchNorm\n",
    "class HeteroGCNConv(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, dropout = 0.57) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        self.conv2 = GATv2Conv(out_feature, out_feature, edge_dim=1, add_self_loops=False)\n",
    "        \n",
    "        self.batch_norm = BatchNorm(out_feature)\n",
    "        self.dropout= nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, edge_weights: Tensor) -> Tensor:\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_weights))\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_weights)\n",
    "        x = self.batch_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from typing import Dict\n",
    "import torch_geometric\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm, global_mean_pool, global_add_pool, global_max_pool, MemPooling, SAGEConv, to_hetero, HeteroBatchNorm\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "class HeteroGcnGatModel1(torch.nn.Module):\n",
    "    def __init__(self, num_nodes_dict:Dict['str', int],\n",
    "                 input_feature: int, out_features: int,\n",
    "                 metadata,\n",
    "                 base_hidden_feature: int=256,\n",
    "                 dropout=0.1):\n",
    "        \n",
    "        super(HeteroGcnGatModel1, self).__init__()\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.bsh: int = base_hidden_feature\n",
    "        bsh2: int = int(self.bsh/2)\n",
    "        bsh4: int = int(self.bsh/4)\n",
    "        bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "        self.encoder = GSequential('x_dict, edge_index, edge_weights', [\n",
    "            (to_hetero(HeteroGCNConv(input_feature, self.bsh), metadata), 'x_dict, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, self.bsh), metadata), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (to_hetero(HeteroGCNConv(self.bsh, bsh2), metadata), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh2), metadata), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (to_hetero(HeteroGCNConv(bsh2, bsh4), metadata), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh4), metadata), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (to_hetero(HeteroGCNConv(bsh4, bsh8), metadata), 'x3, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (to_hetero(HeteroGCNConv(bsh8, bsh8), metadata), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "            (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "            (BatchNorm(bsh4), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "            (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "            (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (BatchNorm(bsh2), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "            (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "            (BatchNorm(self.bsh), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.input_features), 'x1, edge_index, edge_weights -> x1'),\n",
    "        ])\n",
    "        \n",
    "                \n",
    "        self.user_emb = torch.nn.Embedding(num_nodes_dict['user'].num_nodes, self.input_features)\n",
    "        self.movie_emb = torch.nn.Embedding(num_nodes_dict['movie'].num_nodes, self.input_features)\n",
    "        self.movie_lin = torch.nn.Linear(20, self.input_features)\n",
    "        \n",
    "    def forward(self, x: HeteroData) -> Tensor:\n",
    "        x_dict = {\n",
    "          \"user\": self.user_emb(x[\"user\"].node_id),\n",
    "          \"movie\": self.movie_lin(x[\"movie\"].x) + self.movie_emb(x[\"movie\"].node_id)\n",
    "        }\n",
    "\n",
    "        x1_dict, x2_dict, x3_dict, x4_dict = self.encoder(x_dict, x.edge_index_dict, x.edge_attr_dict)\n",
    "        print(f'x4_dict[\"movie\"]: {x4_dict[\"movie\"].shape}, x_dict[\"movie\"]: {x_dict[\"movie\"].shape}')\n",
    "        x_att, x4 = self.attention(x3_dict[\"movie\"], x4_dict[\"movie\"], \n",
    "                                   x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                                   x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        x_dec = self.decoder(x1_dict[\"movie\"], x2_dict[\"movie\"], x_att, \n",
    "                             x.edge_index_dict[('movie','rev_rates','user')],\n",
    "                             x.edge_attr_dict[('movie','rev_rates','user')])\n",
    "        return x_dict #x_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = HeteroGcnGatModel1(\n",
    "    {'user': data['user'].num_nodes, 'movie': data['movie'].num_nodes},\n",
    "    300, 1, 1024, dropout=0.2)\n",
    "torch_model = torch_model.to(device)\n",
    "# print(next(iter(torch_model.parameters())).device)\n",
    "pre = torch_model(X1.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from Scripts.Models.LightningModels.LightningModels import BinaryLightningModel\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "    EarlyStopping(patience=50, mode='max', monitor='val_acc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightning_model = BinaryLightningModel(torch_model,\n",
    "                                 torch.optim.Adam(torch_model.parameters(), lr=0.00017, weight_decay=0.00055),\n",
    "                                       torch.nn.BCEWithLogitsLoss(),\n",
    "                                       learning_rate=0.00017,\n",
    "                                       batch_size=batch_size,\n",
    "                                       ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=5000,\n",
    "            accelerator='gpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='GcnGatSentiment3'),\n",
    "            num_sanity_val_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(trainer)\n",
    "results = tuner.lr_find(lightning_model, datamodule=data_manager, min_lr=0.00001,max_lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = results.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.fit(lightning_model, datamodule=data_manager)\n",
    "trainer.fit(lightning_model,\n",
    "            datamodule=data_manager\n",
    "            # train_dataloaders=data_manager.train_dataloader(),\n",
    "            # val_dataloaders=data_manager.val_dataloader(),\n",
    "            # logger=CSVLogger(save_dir='logs/', name='sample_model'),\n",
    "            # default_root_dir=\"~/Desktop\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "'''From torch lightning tutorials'''\n",
    "def plot_csv_logger(csv_path, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "    metrics = pd.read_csv(csv_path)\n",
    "    \n",
    "    aggregation_metrics = []\n",
    "    agg_col = 'epoch'\n",
    "    for i, dfg in metrics.groupby(agg_col):\n",
    "        agg = dict(dfg.mean())\n",
    "        agg[agg_col] = i\n",
    "        aggregation_metrics.append(agg)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "    df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='loss')\n",
    "    df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabel='accuracy')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_csv_logger(r'C:\\Users\\fardin\\Projects\\ColorIntelligence\\logs\\GcnGatSentiment3\\version_4\\metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
