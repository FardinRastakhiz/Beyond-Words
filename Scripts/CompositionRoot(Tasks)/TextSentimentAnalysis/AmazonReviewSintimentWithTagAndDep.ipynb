{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# from Scripts.DataManager.DabasePreparations.AmazonReviewSentiGraph import AmazonReviewSentiGraph\n",
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(r'C:\\Users\\fardin\\Projects\\ColorIntelligence')\n",
    "# os.environ['TORCH_USE_CUDA_DSA']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = 'cuda'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Tuple, Any\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "# from torch_geometric.data.lightning.datamodule import LightningDataModule\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch_geometric.utils import augmentation\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import subgraph, train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.Utils.GraphCollection.GraphCollection import GraphCollection\n",
    "\n",
    "class GraphDataModule(LightningDataModule):\n",
    "\n",
    "    def __init__(self, config: Config, device, has_val: bool, has_test: bool, test_size=0.2, val_size=0.15, *args, **kwargs):\n",
    "        super(GraphDataModule, self).__init__() #has_val, has_test, **kwargs)\n",
    "        self.config = config\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.device = device\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def zero_rule_baseline():\n",
    "        pass\n",
    "\n",
    "    # def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "    # def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\n",
    "    # def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\n",
    "    # def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.DataManager.GraphConstructor.CoOccurrenceGraphConstructor import CoOccurrenceGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor, TextGraphType\n",
    "# from Scripts.DataManager.GraphLoader.GraphLoader import GraphLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "from Scripts.DataManager.Datasets.GraphConstructorDataset import GraphConstructorDataset\n",
    "\n",
    "class AmazonReviewGraphDataModule(GraphDataModule):\n",
    "\n",
    "    def __init__(self, config: Config, has_val: bool, has_test: bool, test_size=0.2, val_size=0.2, num_workers=2,\n",
    "                 drop_last=True, train_data_path='', test_data_path='', graphs_path='', batch_size = 32,\n",
    "                 device='cpu', shuffle = False, num_data_load=-1,\n",
    "                 graph_type: TextGraphType = TextGraphType.CO_OCCURRENCE, *args, **kwargs):\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['batch_size'] = batch_size\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['shuffle'] = shuffle\n",
    "        super(AmazonReviewGraphDataModule, self)\\\n",
    "            .__init__(config, device, has_val, has_test, test_size, val_size, *args, **kwargs)\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.graph_type = graph_type\n",
    "        self.train_data_path = 'data/Amazon-Review/train_sm.csv' if train_data_path == '' else train_data_path\n",
    "        self.test_data_path = 'data/Amazon-Review/test_sm.csv' if test_data_path == '' else test_data_path\n",
    "        self.train_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.test_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.labels = None\n",
    "        self.dataset = None\n",
    "        self.shuffle = shuffle\n",
    "        self.num_node_features = 0\n",
    "        self.num_classes = 0\n",
    "        self.df: pd.DataFrame = pd.DataFrame()\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset = None, None, None\n",
    "        self.train_df = pd.read_csv(path.join(self.config.root, self.train_data_path))\n",
    "        self.test_df = pd.read_csv(path.join(self.config.root, self.test_data_path))\n",
    "        self.train_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.test_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.train_df = self.train_df[['Polarity', 'Review']]\n",
    "        self.test_df = self.test_df[['Polarity', 'Review']]\n",
    "        self.df = pd.concat([self.train_df, self.test_df])\n",
    "        self.num_data_load = num_data_load if num_data_load>0 else self.df.shape[0]\n",
    "        self.num_data_load = num_data_load if self.num_data_load < self.df.shape[0] else self.df.shape[0] \n",
    "        self.df = self.df.iloc[:self.num_data_load]\n",
    "        self.df.index = np.arange(0, self.num_data_load)\n",
    "        self.graph_constructors = self.__set_graph_constructors(self.graph_type)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        graph_constructor.setup()\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        labels = self.df['Polarity'][:self.num_data_load]\n",
    "        labels = labels.apply(lambda p: 0 if p == 1 else 1).to_numpy()\n",
    "        labels = torch.from_numpy(labels)\n",
    "        self.labels = labels.to(torch.float32).view(-1, 1).to(self.device)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        \n",
    "        print(f'self.labels.shape: {self.labels.shape}')\n",
    "        self.dataset = GraphConstructorDataset(graph_constructor, self.labels)\n",
    "        sample_graph = graph_constructor.get_first()\n",
    "        self.num_node_features = sample_graph.num_features\n",
    "        self.num_classes = len(torch.unique(self.labels))\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset =\\\n",
    "            random_split(self.dataset, [1-self.val_size-self.test_size, self.val_size, self.test_size])\n",
    "        self.__train_dataloader =  DataLoader(self.__train_dataset, batch_size=self.batch_size, drop_last=self.drop_last, shuffle=self.shuffle, num_workers=0, persistent_workers=False)\n",
    "        self.__test_dataloader =  DataLoader(self.__test_dataset, batch_size=self.batch_size, num_workers=0, persistent_workers=False)\n",
    "        self.__val_dataloader =  DataLoader(self.__val_dataset, batch_size=self.batch_size, num_workers=0, persistent_workers=False)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.__train_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.__test_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.__val_dataloader \n",
    "\n",
    "    def __set_graph_constructors(self, graph_type: TextGraphType):\n",
    "        graph_constructors: Dict[TextGraphType, GraphConstructor] = {}\n",
    "        if TextGraphType.CO_OCCURRENCE in graph_type:\n",
    "            graph_constructors[TextGraphType.CO_OCCURRENCE] = self.__get_co_occurrence_graph()\n",
    "        if TextGraphType.DEPENDENCY in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.SEQUENTIAL in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.TAGS in graph_type:\n",
    "            pass\n",
    "        return graph_constructors\n",
    "\n",
    "    def __get_co_occurrence_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return CoOccurrenceGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def zero_rule_baseline(self):\n",
    "        return f'zero_rule baseline: {(len(self.labels[self.labels>0.5])* 100.0 / len(self.labels))  : .2f}%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_data_load: 1000\n",
      "filename: C:\\Users\\fardin\\Projects\\ColorIntelligence\\data/GraphData/AmazonReview\\graph_var.txt\n",
      " 0 graph loaded\n",
      " 100 graph loaded\n",
      " 200 graph loaded\n",
      " 300 graph loaded\n",
      " 400 graph loaded\n",
      " 500 graph loaded\n",
      " 600 graph loaded\n",
      " 700 graph loaded\n",
      " 800 graph loaded\n",
      " 900 graph loaded\n",
      "self.num_data_load: 1000\n",
      "self.labels.shape: torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "data_manager = AmazonReviewGraphDataModule(config, True, True, num_workers=2, shuffle=True, num_data_load = 1000, device='cpu', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataloader = data_manager.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = next(iter(t_dataloader))\n",
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "# from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from Scripts.Models.LightningModels.LightningModels import BinaryLightningModel\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn\n",
    "\n",
    "class GcnGatModel1(nn.Module):\n",
    "    r\"\"\"\n",
    "    This class is for graph level classification or graph level regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_feature: int, out_features: int, base_hidden_feature: int=256, dropout=0.1, *args, **kwargs):\n",
    "        super(GcnGatModel1, self).__init__(*args, **kwargs)\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.bsh: int = base_hidden_feature\n",
    "        bsh2: int = int(self.bsh/2)\n",
    "        bsh4: int = int(self.bsh/4)\n",
    "        bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "        self.encoder = GSequential('x, edge_index, edge_weights', [\n",
    "            (GCNConv(input_feature, self.bsh), 'x, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            \n",
    "            (GCNConv(self.bsh, bsh2), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCNConv(bsh2, bsh4), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            \n",
    "            (GCNConv(bsh4, bsh8), 'x3, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (nn.Dropout(dropout), 'x4->x4'),\n",
    "            (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (nn.Dropout(dropout), 'x4->x4'),\n",
    "            (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "            (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "            (BatchNorm(bsh4), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "            (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "            (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (BatchNorm(bsh2), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "            (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "            (BatchNorm(self.bsh), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (lambda x1, x2, x3: (x1, x2, x3), 'x1, x2, x3 -> x, x2, x3')\n",
    "        ])\n",
    "\n",
    "        self.pooling_layer1 = GCNConv(self.bsh, 5)\n",
    "        self.pooling_layer2 = DenseGCNConv(self.bsh, 1)\n",
    "        self.output_layer = Linear(self.bsh, self.num_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x_enc = self.encoder(x.x, x.edge_index, x.edge_attr)\n",
    "        x_att, x4 = self.attention(x3, x_enc, x.edge_index, x.edge_attr)\n",
    "        x_dec, x2, x3 = self.decoder(x1, x2, x_att, x.edge_index, x.edge_attr)\n",
    "        \n",
    "        st = time.time()\n",
    "        \n",
    "        all_s = self.pooling_layer1(x_dec, x.edge_index, x.edge_attr)\n",
    "        \n",
    "        all_x = [x[i] for i in range(len(x))]\n",
    "        ci = [all_x[i].x.shape[0] for i in range(len(x))]\n",
    "        # ci = torch.tensor([x[i].x.shape[0] for i in range(len(x))], dtype=torch.int, device=x_dec.device).cumsum(0, dtype=torch.int)\n",
    "        x_list = torch.split(x_dec, ci)\n",
    "        # x_list = [x_dec[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        s_i = torch.split(all_s, ci)\n",
    "        # s_i = [all_s[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        x_pooled = torch.zeros((len(x_list), self.bsh), dtype=x_dec.dtype, device=x_dec.device)\n",
    "        \n",
    "        print(f'duration1: {time.time() - st:.9f}')\n",
    "        st = time.time()\n",
    "        for i in range(len(ci)):\n",
    "            print(f'duration11: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            s = s_i[i]# self.pooling_layer1(x2[i], x[i].edge_index, x[i].edge_attr)\n",
    "            print(f'duration12: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            adj = torch.zeros((ci[i], ci[i]), device=x_dec.device)\n",
    "            print(f'duration13: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            # adj = torch.zeros((x[i].x.shape[0], x[i].x.shape[0]), device=x_dec.device)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            adj[all_x[i].edge_index[0], all_x[i].edge_index[1]] = x[i].edge_attr\n",
    "            print(f'duration14: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            # adj = to_dense_adj(edge_index=x[i].edge_index, max_num_nodes=x[i].x.shape[0], edge_attr=x[i].edge_attr)\n",
    "            nodes, adj, _, _ = dense_diff_pool(x_list[i], adj, s=s)         # ***\n",
    "            print(f'duration15: {time.time() - st:.9f}')            \n",
    "            st = time.time()\n",
    "            s = self.pooling_layer2(nodes, adj)\n",
    "            print(f'duration16: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            nodes, _, _, _ = dense_diff_pool(nodes, adj, s=s)           #  *****\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            print(f'duration17: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "            x_pooled[i] = torch.squeeze(nodes)\n",
    "            print(f'duration18: {time.time() - st:.9f}')\n",
    "            st = time.time()\n",
    "\n",
    "        print(f'duration2: {time.time() - st:.9f}')\n",
    "        \n",
    "        return self.output_layer(x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = GcnGatModel1(300, 1, 256, 0.4).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration1: 0.054996014\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.003000021\n",
      "duration16: 0.001999855\n",
      "duration17: 0.003001213\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999451\n",
      "duration15: 0.000999928\n",
      "duration16: 0.001998186\n",
      "duration17: 0.001001596\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000404\n",
      "duration15: 0.001998901\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001000166\n",
      "duration18: 0.001000166\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000093\n",
      "duration16: 0.001000166\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000404\n",
      "duration15: 0.000997782\n",
      "duration16: 0.001999617\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998186\n",
      "duration16: 0.001001596\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000999451\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001997948\n",
      "duration16: 0.000999451\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.002001524\n",
      "duration16: 0.000999689\n",
      "duration17: 0.000998020\n",
      "duration18: 0.001000404\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999855\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000166\n",
      "duration15: 0.000998974\n",
      "duration16: 0.002001762\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000998974\n",
      "duration15: 0.000998735\n",
      "duration16: 0.001000881\n",
      "duration17: 0.002000570\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999212\n",
      "duration15: 0.000999689\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.001000881\n",
      "duration16: 0.000998020\n",
      "duration17: 0.001001120\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000166\n",
      "duration16: 0.001000166\n",
      "duration17: 0.001998663\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001001835\n",
      "duration15: 0.002000332\n",
      "duration16: 0.000998259\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000332\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000998974\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000166\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999689\n",
      "duration16: 0.002000570\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000998974\n",
      "duration16: 0.002000093\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999689\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001999855\n",
      "duration17: 0.001999855\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002999544\n",
      "duration16: 0.002000093\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.001999140\n",
      "duration16: 0.001000404\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000093\n",
      "duration16: 0.000999689\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000166\n",
      "duration16: 0.001001835\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999855\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001001120\n",
      "duration15: 0.001999855\n",
      "duration16: 0.000999451\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999689\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000999451\n",
      "duration17: 0.002000809\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.001003981\n",
      "duration16: 0.000994444\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000332\n",
      "duration16: 0.001001596\n",
      "duration17: 0.000997305\n",
      "duration18: 0.001000643\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999689\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.001998901\n",
      "duration16: 0.001000643\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999451\n",
      "duration15: 0.001000166\n",
      "duration16: 0.001000404\n",
      "duration17: 0.001999140\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999451\n",
      "duration15: 0.000999928\n",
      "duration16: 0.001000166\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000166\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000404\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001999855\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.000998974\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000570\n",
      "duration16: 0.001002073\n",
      "duration17: 0.000996351\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000881\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001999140\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000093\n",
      "duration16: 0.001000404\n",
      "duration17: 0.002000332\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998901\n",
      "duration16: 0.001000643\n",
      "duration17: 0.001000404\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000999451\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001999855\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002001762\n",
      "duration16: 0.000998735\n",
      "duration17: 0.000998735\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001001120\n",
      "duration15: 0.001999378\n",
      "duration16: 0.001000881\n",
      "duration17: 0.001998186\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001001596\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002001047\n",
      "duration16: 0.000997782\n",
      "duration17: 0.000999451\n",
      "duration18: 0.001000881\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999378\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001000643\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999140\n",
      "duration16: 0.001000643\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001002312\n",
      "duration15: 0.001997471\n",
      "duration16: 0.001001358\n",
      "duration17: 0.001000166\n",
      "duration18: 0.001000643\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002998829\n",
      "duration16: 0.001000404\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000999689\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999378\n",
      "duration16: 0.000998259\n",
      "duration17: 0.001999855\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001001358\n",
      "duration15: 0.001000643\n",
      "duration16: 0.001999378\n",
      "duration17: 0.000998020\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000404\n",
      "duration15: 0.002000809\n",
      "duration16: 0.000998735\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.001000166\n",
      "duration16: 0.002000332\n",
      "duration17: 0.001000643\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000999928\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999855\n",
      "duration16: 0.000998259\n",
      "duration17: 0.001001596\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000332\n",
      "duration16: 0.001998901\n",
      "duration17: 0.001998663\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.001001835\n",
      "duration16: 0.002000093\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000999928\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001997948\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001002073\n",
      "duration18: 0.001000166\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002998352\n",
      "duration16: 0.000999451\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998186\n",
      "duration16: 0.001001120\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.001999617\n",
      "duration16: 0.000999212\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.001000166\n",
      "duration16: 0.000999212\n",
      "duration17: 0.002000809\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000999689\n",
      "duration16: 0.000999689\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999212\n",
      "duration15: 0.000999928\n",
      "duration16: 0.001000404\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000881\n",
      "duration16: 0.000999689\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000999689\n",
      "duration16: 0.001999617\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000999689\n",
      "duration16: 0.001000166\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999378\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000998974\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.001000643\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001998901\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999689\n",
      "duration15: 0.001001835\n",
      "duration16: 0.000998259\n",
      "duration17: 0.001000881\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000166\n",
      "duration16: 0.001000643\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000166\n",
      "duration15: 0.000998735\n",
      "duration16: 0.001000404\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998901\n",
      "duration16: 0.001001120\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001997709\n",
      "duration16: 0.001002073\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000332\n",
      "duration16: 0.000999689\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001001358\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000998259\n",
      "duration17: 0.002000570\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001002312\n",
      "duration15: 0.000997305\n",
      "duration16: 0.001000404\n",
      "duration17: 0.001999855\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999617\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000643\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999451\n",
      "duration16: 0.002000570\n",
      "duration17: 0.000998497\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.001999617\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001000643\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000998020\n",
      "duration16: 0.001000643\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000999212\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001000881\n",
      "duration18: 0.000999689\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998663\n",
      "duration16: 0.001000166\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000809\n",
      "duration16: 0.000998974\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000404\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000166\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002999306\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001000643\n",
      "duration18: 0.000999212\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000093\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001000404\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000404\n",
      "duration15: 0.000999928\n",
      "duration16: 0.000998735\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000999928\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000166\n",
      "duration16: 0.002000332\n",
      "duration17: 0.000999928\n",
      "duration18: 0.001000404\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001997948\n",
      "duration16: 0.001001358\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001001835\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.000998020\n",
      "duration16: 0.000999451\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999689\n",
      "duration16: 0.001000404\n",
      "duration17: 0.001999855\n",
      "duration18: 0.001000166\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.000999689\n",
      "duration16: 0.001999617\n",
      "duration17: 0.000999928\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999140\n",
      "duration16: 0.001001120\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001998901\n",
      "duration16: 0.000999689\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001001358\n",
      "duration16: 0.000999451\n",
      "duration17: 0.001999617\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000809\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000998974\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000999928\n",
      "duration16: 0.001000404\n",
      "duration17: 0.000998735\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.001000881\n",
      "duration16: 0.001999140\n",
      "duration17: 0.001998425\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999689\n",
      "duration15: 0.001001835\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001998425\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000998735\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000999451\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000404\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001999378\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002002478\n",
      "duration16: 0.000999451\n",
      "duration17: 0.000998259\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001001120\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001998663\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.000999212\n",
      "duration16: 0.000999928\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000999928\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001000166\n",
      "duration16: 0.000999212\n",
      "duration17: 0.002001524\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000998735\n",
      "duration15: 0.000999928\n",
      "duration16: 0.001000404\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000999928\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000093\n",
      "duration16: 0.000999451\n",
      "duration17: 0.002000809\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000998974\n",
      "duration16: 0.002001286\n",
      "duration17: 0.000998259\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000999689\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002001047\n",
      "duration16: 0.001000643\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000166\n",
      "duration15: 0.002001047\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001998425\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001001358\n",
      "duration17: 0.001998901\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000809\n",
      "duration16: 0.001999855\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.000998497\n",
      "duration16: 0.001999617\n",
      "duration17: 0.001001358\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000998497\n",
      "duration15: 0.000999451\n",
      "duration16: 0.000999928\n",
      "duration17: 0.002000570\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999140\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000999689\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999855\n",
      "duration16: 0.001000404\n",
      "duration17: 0.000999689\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.000998974\n",
      "duration16: 0.002000332\n",
      "duration17: 0.000998735\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.001000166\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000999451\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999689\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001000166\n",
      "duration17: 0.001000404\n",
      "duration18: 0.000999928\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999617\n",
      "duration16: 0.001000166\n",
      "duration17: 0.000999212\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000643\n",
      "duration15: 0.000999451\n",
      "duration16: 0.000998974\n",
      "duration17: 0.002000093\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002000332\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001000404\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000999928\n",
      "duration15: 0.002000332\n",
      "duration16: 0.000998497\n",
      "duration17: 0.001000404\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000881\n",
      "duration15: 0.001998901\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.001000166\n",
      "duration15: 0.000999689\n",
      "duration16: 0.001000643\n",
      "duration17: 0.000998020\n",
      "duration18: 0.001001120\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999855\n",
      "duration16: 0.001999378\n",
      "duration17: 0.001000166\n",
      "duration18: 0.001000166\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.000000000\n",
      "duration14: 0.000000000\n",
      "duration15: 0.001999378\n",
      "duration16: 0.000999689\n",
      "duration17: 0.001000166\n",
      "duration18: 0.000000000\n",
      "duration11: 0.000000000\n",
      "duration12: 0.000000000\n",
      "duration13: 0.001000166\n",
      "duration14: 0.000000000\n",
      "duration15: 0.002001286\n",
      "duration16: 0.000999928\n",
      "duration17: 0.001998186\n",
      "duration18: 0.000000000\n",
      "duration2: 0.000000000\n"
     ]
    }
   ],
   "source": [
    "test = torch_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "duration1: 0.068994522\n",
      "duration2: 0.704936504\n",
      "1\n",
      "duration1: 0.056977272\n",
      "duration2: 0.672939062\n",
      "2\n",
      "duration1: 0.051994801\n",
      "duration2: 0.610945463\n",
      "3\n",
      "duration1: 0.049994707\n",
      "duration2: 0.631943941\n",
      "4\n",
      "duration1: 0.049995661\n",
      "duration2: 0.656940222\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'{i}')\n",
    "    test = torch_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.\n"
     ]
    }
   ],
   "source": [
    "torch_model = GcnGatModel1(300, 1, 256, 0.4).to(device)\n",
    "lightning_model = BinaryLightningModel(torch_model,\n",
    "                                       torch.optim.Adam(torch_model.parameters(), lr=0.0001, weight_decay=0.00055),\n",
    "                                       torch.nn.BCEWithLogitsLoss(),\n",
    "                                       learning_rate=0.0001,\n",
    "                                       batch_size=batch_size,\n",
    "                                       ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "            max_epochs=1000,\n",
    "            accelerator='gpu',\n",
    "            num_sanity_val_steps=0,\n",
    "            log_every_n_steps=2000,\n",
    "            logger=CSVLogger(save_dir='logs/', name='DepTag2')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<lightning.pytorch.loggers.csv_logs.CSVLogger object at 0x0000028083E81D50>\n"
     ]
    }
   ],
   "source": [
    "for hp in trainer.loggers:\n",
    "    print(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if trainer.logger is not None:\n",
    "#     trainer.logger.log_hyperparams(ignore=[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:43: attribute 'model' removed from hparams because it cannot be pickled\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | GcnGatModel1      | 595 K \n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "2 | train_acc | BinaryAccuracy    | 0     \n",
      "3 | val_acc   | BinaryAccuracy    | 0     \n",
      "4 | test_acc  | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "595 K     Trainable params\n",
      "0         Non-trainable params\n",
      "595 K     Total params\n",
      "2.383     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbb\n",
      "train_fn: <bound method Trainer._fit_impl of <lightning.pytorch.trainer.trainer.Trainer object at 0x0000028083E81C00>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (257) is smaller than the logging interval Trainer(log_every_n_steps=2000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff52997f8e7c4a1e82dcaf3f0035743b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:61: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=lightning_model, datamodule=data_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch.nn import Linear\n",
    "# from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm\n",
    "# from torch_geometric.nn import Sequential as GSequential\n",
    "# from torch_geometric.utils import to_dense_adj\n",
    "# from torch import nn\n",
    "\n",
    "# class DepTagModel1(nn.Module):\n",
    "#     r\"\"\"\n",
    "#     This class is for graph level classification or graph level regression\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, input_feature: int, out_features: int, base_hidden_feature: int=256, dropout=0.1, *args, **kwargs):\n",
    "#         super(DepTagModel1, self).__init__(*args, **kwargs)\n",
    "#         self.input_features = input_feature\n",
    "#         self.num_out_features = out_features\n",
    "#         self.bsh: int = base_hidden_feature\n",
    "#         bsh2: int = int(self.bsh/2)\n",
    "#         bsh4: int = int(self.bsh/4)\n",
    "#         bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "#         self.composition_layer = GSequential('x_text, edge_index_text, edge_weights_text, x_tag, edge_index_tag, edge_weights_tag, x_dep, edge_index_dep, edge_weights_dep', [\n",
    "#             (GCNConv(input_feature, self.bsh), 'x_text, edge_index_text, edge_weights_text ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (GCNConv(input_feature, self.bsh), 'x_tag, edge_index_tag, edge_weights_tag ->x2'),\n",
    "#             (BatchNorm(self.bsh), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (GCNConv(input_feature, self.bsh), 'x_tag, edge_index_tag, edge_weights_tag ->x3'),\n",
    "#             (BatchNorm(self.bsh), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             ()\n",
    "#         ])\n",
    "        \n",
    "#         self.encoder = GSequential('x, edge_index, edge_weights', [\n",
    "#             (GCNConv(input_feature, self.bsh), 'x, edge_index, edge_weights ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "#             (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "#             (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "            \n",
    "#             (GCNConv(self.bsh, bsh2), 'x1, edge_index, edge_weights -> x2'),\n",
    "#             (BatchNorm(bsh2), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "#             (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "#             (BatchNorm(bsh2), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "#             (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "#             (BatchNorm(bsh2), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "#             (GCNConv(bsh2, bsh4), 'x2, edge_index, edge_weights -> x3'),\n",
    "#             (BatchNorm(bsh4), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             (nn.Dropout(dropout), 'x3->x3'),\n",
    "#             (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "#             (BatchNorm(bsh4), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             (nn.Dropout(dropout), 'x3->x3'),\n",
    "#             (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "#             (BatchNorm(bsh4), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             (nn.Dropout(dropout), 'x3->x3'),\n",
    "            \n",
    "#             (GCNConv(bsh4, bsh8), 'x3, edge_index, edge_weights -> x4'),\n",
    "#             (BatchNorm(bsh8), 'x4->x4'),\n",
    "#             (nn.ReLU(), 'x4->x4'),\n",
    "#             (nn.Dropout(dropout), 'x4->x4'),\n",
    "#             (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "#             (BatchNorm(bsh8), 'x4->x4'),\n",
    "#             (nn.ReLU(), 'x4->x4'),\n",
    "#             (nn.Dropout(dropout), 'x4->x4'),\n",
    "#             (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "#             (BatchNorm(bsh8), 'x4->x4'),\n",
    "#             (nn.ReLU(), 'x4->x4'),\n",
    "#             (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "#         ])\n",
    "        \n",
    "#         self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "#             (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "#             (BatchNorm(bsh4), 'x4->x4'),\n",
    "#             (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "#             (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "#             (BatchNorm(bsh4), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "#             (BatchNorm(bsh4), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "#             (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "#             (BatchNorm(bsh2), 'x3->x3'),\n",
    "#             (nn.ReLU(), 'x3->x3'),\n",
    "#             (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "#         ])\n",
    "        \n",
    "#         self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "#             (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "#             (BatchNorm(bsh2), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "#             (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "#             (BatchNorm(bsh2), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "#             (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "#             (BatchNorm(self.bsh), 'x2->x2'),\n",
    "#             (nn.ReLU(), 'x2->x2'),\n",
    "#             (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "#             (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "#             (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "#             (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "#             (BatchNorm(self.bsh), 'x1->x1'),\n",
    "#             (nn.ReLU(), 'x1->x1'),\n",
    "#             (nn.Dropout(dropout), 'x1->x1'),\n",
    "#             (lambda x1, x2, x3: (x1, x2, x3), 'x1, x2, x3 -> x, x2, x3')\n",
    "#         ])\n",
    "\n",
    "#         self.pooling_layer1 = GCNConv(self.bsh, 5)\n",
    "#         self.pooling_layer2 = DenseGCNConv(self.bsh, 1)\n",
    "#         self.output_layer = Linear(self.bsh, self.num_out_features)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1, x2, x3, x_enc = self.encoder(x.x, x.edge_index, x.edge_attr)\n",
    "#         x_att, x4 = self.attention(x3, x_enc, x.edge_index, x.edge_attr)\n",
    "#         x_dec, x2, x3 = self.decoder(x1, x2, x_att, x.edge_index, x.edge_attr)\n",
    "        \n",
    "#         all_s = self.pooling_layer1(x_dec, x.edge_index, x.edge_attr)\n",
    "#         ci = torch.tensor([x[i].x.shape[0] for i in range(len(x))], dtype=torch.int, device=x_dec.device).cumsum(0, dtype=torch.int)\n",
    "#         x_list = [x_dec[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "#         s_i = [all_s[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "#         x_pooled = torch.zeros((len(x_list), self.bsh), dtype=x_dec.dtype, device=x_dec.device)\n",
    "        \n",
    "#         for i in range(len(ci)):\n",
    "#             s = s_i[i]# self.pooling_layer1(x2[i], x[i].edge_index, x[i].edge_attr)\n",
    "#             adj = torch.zeros((x[i].x.shape[0], x[i].x.shape[0]), device=x_dec.device)\n",
    "#             adj[x[i].edge_index[0],x[i].edge_index[1]] = x[i].edge_attr\n",
    "#             # adj = to_dense_adj(edge_index=x[i].edge_index, max_num_nodes=x[i].x.shape[0], edge_attr=x[i].edge_attr)\n",
    "#             nodes, adj, _, _ = dense_diff_pool(x_list[i], adj, s=s)\n",
    "#             s = self.pooling_layer2(nodes, adj)\n",
    "#             nodes, _, _, _ = dense_diff_pool(nodes, adj, s=s)\n",
    "#             x_pooled[i] = torch.squeeze(nodes)\n",
    "\n",
    "#         # return x1\n",
    "#         return self.output_layer(x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model = DepTagModel1(300, 1, 1024, dropout=0.2)\n",
    "# torch_model = torch_model.to(device)\n",
    "# next(iter(torch_model.parameters())).device\n",
    "# t_dataloader = data_manager.train_dataloader()\n",
    "# v_dataloader = data_manager.val_dataloader()\n",
    "# print(next(iter(torch_model.parameters())).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1, y1 = next(iter(t_dataloader))\n",
    "# X2, y2 = next(iter(v_dataloader))\n",
    "# print(X1[0].x.device)\n",
    "# print(X2[0].x.device)\n",
    "# print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "# print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "# print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# def train_model(target_model, num_epoch = 2, target_device='cpu'):\n",
    "#     target_model = target_model.to(target_device)\n",
    "#     loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "#     optimizer = torch.optim.Adam(target_model.parameters(), 0.001, weight_decay=0.0055)\n",
    "#     for i in range(num_epoch):\n",
    "        \n",
    "#         batch_acc=[]\n",
    "#         batch_loss = []\n",
    "#         begin = time.time()\n",
    "#         target_model.train()\n",
    "#         i = 0\n",
    "#         for X,y in t_dataloader:\n",
    "#             print(f'i: {i}')\n",
    "#             i = i + 1\n",
    "#             X = X.to(target_device)\n",
    "#             y = y.to(target_device)\n",
    "#             optimizer.zero_grad()\n",
    "#             predicted = target_model(X)\n",
    "#             loss = loss_func(predicted, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             predicted = torch.nn.functional.sigmoid(predicted)\n",
    "#             predicted[predicted<0.5] = 0\n",
    "#             predicted[predicted>=0.5] = 1\n",
    "#             accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "#             batch_acc.append(accuracy)\n",
    "#             batch_loss.append(loss.item())\n",
    "#             torch.cuda.empty_cache()\n",
    "#         print(f'train accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')\n",
    "        \n",
    "#         target_model.eval()\n",
    "#         batch_acc=[]\n",
    "#         batch_loss = []\n",
    "#         i = 0\n",
    "#         for X,y in v_dataloader:\n",
    "#             print(f'i: {i}')\n",
    "#             i = i + 1\n",
    "#             X = X.to(target_device)\n",
    "#             y = y.to(target_device)\n",
    "#             with torch.inference_mode():\n",
    "#                 predicted = target_model(X)\n",
    "#                 loss = loss_func(predicted, y)\n",
    "#             predicted = torch.nn.functional.sigmoid(predicted)\n",
    "#             predicted[predicted<0.5] = 0\n",
    "#             predicted[predicted>=0.5] = 1\n",
    "#             accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "#             batch_acc.append(accuracy)\n",
    "#             batch_loss.append(loss.item())\n",
    "#             torch.cuda.empty_cache()\n",
    "#         print(f'validation accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(torch_model, 5, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(torch_model, 5, 'cuda')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
