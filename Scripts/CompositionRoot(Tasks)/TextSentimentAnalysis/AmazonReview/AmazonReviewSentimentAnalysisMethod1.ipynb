{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Scripts.DataManager.DabasePreparations.AmazonReviewSentiGraph import AmazonReviewSentiGraph\n",
    "from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "config = Config(r'C:\\Users\\fardin\\Projects\\ColorIntelligence')\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "# os.environ['TORCH_USE_CUDA_DSA']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = 'cpu'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Tuple, Any\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "# from torch_geometric.data.lightning.datamodule import LightningDataModule\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch_geometric.utils import augmentation\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import subgraph, train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.Utils.GraphCollection.GraphCollection import GraphCollection\n",
    "\n",
    "\n",
    "class GraphLoader(LightningDataModule):\n",
    "\n",
    "    def __init__(self, config: Config, device, has_val: bool, has_test: bool, test_size=0.2, val_size=0.15, *args, **kwargs):\n",
    "        super(GraphLoader, self).__init__() #has_val, has_test, **kwargs)\n",
    "        self.config = config\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.device = device\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    # def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "    # def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\n",
    "    # def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\n",
    "    # def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.DataManager.GraphConstructor.CoOccurrenceGraphConstructor import CoOccurrenceGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor, TextGraphType\n",
    "# from Scripts.DataManager.GraphLoader.GraphLoader import GraphLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "from Scripts.DataManager.Datasets.GraphConstructorDataset import GraphConstructorDataset\n",
    "\n",
    "class AmazonReviewGraphLoader(GraphLoader):\n",
    "\n",
    "    def __init__(self, config: Config, has_val: bool, has_test: bool, test_size=0.2, val_size=0.2, num_workers=2,\n",
    "                 drop_last=True, train_data_path='', test_data_path='', graphs_path='', batch_size = 32,\n",
    "                 device='cpu', shuffle = False, num_data_load=-1,\n",
    "                 graph_type: TextGraphType = TextGraphType.CO_OCCURRENCE, *args, **kwargs):\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['batch_size'] = batch_size\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['shuffle'] = shuffle\n",
    "        super(AmazonReviewGraphLoader, self)\\\n",
    "            .__init__(config, device, has_val, has_test, test_size, val_size, *args, **kwargs)\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.graph_type = graph_type\n",
    "        self.train_data_path = 'data/Amazon-Review/train_sm.csv' if train_data_path == '' else train_data_path\n",
    "        self.test_data_path = 'data/Amazon-Review/test_sm.csv' if test_data_path == '' else test_data_path\n",
    "        self.train_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.test_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.labels = None\n",
    "        self.dataset = None\n",
    "        self.shuffle = shuffle\n",
    "        self.num_node_features = 0\n",
    "        self.num_classes = 0\n",
    "        self.df: pd.DataFrame = pd.DataFrame()\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset = None, None, None\n",
    "        self.train_df = pd.read_csv(path.join(self.config.root, self.train_data_path))\n",
    "        self.test_df = pd.read_csv(path.join(self.config.root, self.test_data_path))\n",
    "        self.train_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.test_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.train_df = self.train_df[['Polarity', 'Review']]\n",
    "        self.test_df = self.test_df[['Polarity', 'Review']]\n",
    "        self.df = pd.concat([self.train_df, self.test_df])\n",
    "        self.num_data_load = num_data_load if num_data_load>0 else self.df.shape[0]\n",
    "        self.num_data_load = num_data_load if self.num_data_load < self.df.shape[0] else self.df.shape[0] \n",
    "        self.df = self.df.iloc[:self.num_data_load]\n",
    "        self.df.index = np.arange(0, self.num_data_load)\n",
    "        self.graph_constructors = self.__set_graph_constructors(self.graph_type)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        graph_constructor.setup()\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        labels = self.df['Polarity'][:self.num_data_load]\n",
    "        labels = labels.apply(lambda p: 0 if p == 1 else 1).to_numpy()\n",
    "        labels = torch.from_numpy(labels)\n",
    "        self.labels = labels.to(torch.float32).view(-1, 1).to(self.device)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        \n",
    "        print(f'self.labels.shape: {self.labels.shape}')\n",
    "        self.dataset = GraphConstructorDataset(graph_constructor, self.labels)\n",
    "        sample_graph = graph_constructor.get_first()\n",
    "        self.num_node_features = sample_graph.num_features\n",
    "        self.num_classes = len(torch.unique(self.labels))\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset =\\\n",
    "            random_split(self.dataset, [1-self.val_size-self.test_size, self.val_size, self.test_size])\n",
    "        self.__train_dataloader =  DataLoader(self.__train_dataset, batch_size=self.batch_size, drop_last=self.drop_last, shuffle=self.shuffle, num_workers=self.num_workers, persistent_workers=True)\n",
    "        self.__test_dataloader =  DataLoader(self.__test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n",
    "        self.__val_dataloader =  DataLoader(self.__val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.__train_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.__test_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.__val_dataloader \n",
    "\n",
    "    def __set_graph_constructors(self, graph_type: TextGraphType):\n",
    "        graph_constructors: Dict[TextGraphType, GraphConstructor] = {}\n",
    "        if TextGraphType.CO_OCCURRENCE in graph_type:\n",
    "            graph_constructors[TextGraphType.CO_OCCURRENCE] = self.__get_co_occurrence_graph()\n",
    "        if TextGraphType.DEPENDENCY in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.SEQUENTIAL in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.TAGS in graph_type:\n",
    "            pass\n",
    "        return graph_constructors\n",
    "\n",
    "    def __get_co_occurrence_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return CoOccurrenceGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_data_load: 55000\n",
      "cpu\n",
      "cpu\n",
      "filename: C:\\Users\\fardin\\Projects\\ColorIntelligence\\data/GraphData/AmazonReview\\graph_var.txt\n",
      " 0 graph loaded\n",
      " 100 graph loaded\n",
      " 200 graph loaded\n",
      " 300 graph loaded\n",
      " 400 graph loaded\n",
      " 500 graph loaded\n",
      " 600 graph loaded\n",
      " 700 graph loaded\n",
      " 800 graph loaded\n",
      " 900 graph loaded\n",
      " 1000 graph loaded\n",
      " 1100 graph loaded\n",
      " 1200 graph loaded\n",
      " 1300 graph loaded\n",
      " 1400 graph loaded\n",
      " 1500 graph loaded\n",
      " 1600 graph loaded\n",
      " 1700 graph loaded\n",
      " 1800 graph loaded\n",
      " 1900 graph loaded\n",
      " 2000 graph loaded\n",
      " 2100 graph loaded\n",
      " 2200 graph loaded\n",
      " 2300 graph loaded\n",
      " 2400 graph loaded\n",
      " 2500 graph loaded\n",
      " 2600 graph loaded\n",
      " 2700 graph loaded\n",
      " 2800 graph loaded\n",
      " 2900 graph loaded\n",
      " 3000 graph loaded\n",
      " 3100 graph loaded\n",
      " 3200 graph loaded\n",
      " 3300 graph loaded\n",
      " 3400 graph loaded\n",
      " 3500 graph loaded\n",
      " 3600 graph loaded\n",
      " 3700 graph loaded\n",
      " 3800 graph loaded\n",
      " 3900 graph loaded\n",
      " 4000 graph loaded\n",
      " 4100 graph loaded\n",
      " 4200 graph loaded\n",
      " 4300 graph loaded\n",
      " 4400 graph loaded\n",
      " 4500 graph loaded\n",
      " 4600 graph loaded\n",
      " 4700 graph loaded\n",
      " 4800 graph loaded\n",
      " 4900 graph loaded\n",
      " 5000 graph loaded\n",
      " 5100 graph loaded\n",
      " 5200 graph loaded\n",
      " 5300 graph loaded\n",
      " 5400 graph loaded\n",
      " 5500 graph loaded\n",
      " 5600 graph loaded\n",
      " 5700 graph loaded\n",
      " 5800 graph loaded\n",
      " 5900 graph loaded\n",
      " 6000 graph loaded\n",
      " 6100 graph loaded\n",
      " 6200 graph loaded\n",
      " 6300 graph loaded\n",
      " 6400 graph loaded\n",
      " 6500 graph loaded\n",
      " 6600 graph loaded\n",
      " 6700 graph loaded\n",
      " 6800 graph loaded\n",
      " 6900 graph loaded\n",
      " 7000 graph loaded\n",
      " 7100 graph loaded\n",
      " 7200 graph loaded\n",
      " 7300 graph loaded\n",
      " 7400 graph loaded\n",
      " 7500 graph loaded\n",
      " 7600 graph loaded\n",
      " 7700 graph loaded\n",
      " 7800 graph loaded\n",
      " 7900 graph loaded\n",
      " 8000 graph loaded\n",
      " 8100 graph loaded\n",
      " 8200 graph loaded\n",
      " 8300 graph loaded\n",
      " 8400 graph loaded\n",
      " 8500 graph loaded\n",
      " 8600 graph loaded\n",
      " 8700 graph loaded\n",
      " 8800 graph loaded\n",
      " 8900 graph loaded\n",
      " 9000 graph loaded\n",
      " 9100 graph loaded\n",
      " 9200 graph loaded\n",
      " 9300 graph loaded\n",
      " 9400 graph loaded\n",
      " 9500 graph loaded\n",
      " 9600 graph loaded\n",
      " 9700 graph loaded\n",
      " 9800 graph loaded\n",
      " 9900 graph loaded\n",
      " 10000 graph loaded\n",
      " 10100 graph loaded\n",
      " 10200 graph loaded\n",
      " 10300 graph loaded\n",
      " 10400 graph loaded\n",
      " 10500 graph loaded\n",
      " 10600 graph loaded\n",
      " 10700 graph loaded\n",
      " 10800 graph loaded\n",
      " 10900 graph loaded\n",
      " 11000 graph loaded\n",
      " 11100 graph loaded\n",
      " 11200 graph loaded\n",
      " 11300 graph loaded\n",
      " 11400 graph loaded\n",
      " 11500 graph loaded\n",
      " 11600 graph loaded\n",
      " 11700 graph loaded\n",
      " 11800 graph loaded\n",
      " 11900 graph loaded\n",
      " 12000 graph loaded\n",
      " 12100 graph loaded\n",
      " 12200 graph loaded\n",
      " 12300 graph loaded\n",
      " 12400 graph loaded\n",
      " 12500 graph loaded\n",
      " 12600 graph loaded\n",
      " 12700 graph loaded\n",
      " 12800 graph loaded\n",
      " 12900 graph loaded\n",
      " 13000 graph loaded\n",
      " 13100 graph loaded\n",
      " 13200 graph loaded\n",
      " 13300 graph loaded\n",
      " 13400 graph loaded\n",
      " 13500 graph loaded\n",
      " 13600 graph loaded\n",
      " 13700 graph loaded\n",
      " 13800 graph loaded\n",
      " 13900 graph loaded\n",
      " 14000 graph loaded\n",
      " 14100 graph loaded\n",
      " 14200 graph loaded\n",
      " 14300 graph loaded\n",
      " 14400 graph loaded\n",
      " 14500 graph loaded\n",
      " 14600 graph loaded\n",
      " 14700 graph loaded\n",
      " 14800 graph loaded\n",
      " 14900 graph loaded\n",
      " 15000 graph loaded\n",
      " 15100 graph loaded\n",
      " 15200 graph loaded\n",
      " 15300 graph loaded\n",
      " 15400 graph loaded\n",
      " 15500 graph loaded\n",
      " 15600 graph loaded\n",
      " 15700 graph loaded\n",
      " 15800 graph loaded\n",
      " 15900 graph loaded\n",
      " 16000 graph loaded\n",
      " 16100 graph loaded\n",
      " 16200 graph loaded\n",
      " 16300 graph loaded\n",
      " 16400 graph loaded\n",
      " 16500 graph loaded\n",
      " 16600 graph loaded\n",
      " 16700 graph loaded\n",
      " 16800 graph loaded\n",
      " 16900 graph loaded\n",
      " 17000 graph loaded\n",
      " 17100 graph loaded\n",
      " 17200 graph loaded\n",
      " 17300 graph loaded\n",
      " 17400 graph loaded\n",
      " 17500 graph loaded\n",
      " 17600 graph loaded\n",
      " 17700 graph loaded\n",
      " 17800 graph loaded\n",
      " 17900 graph loaded\n",
      " 18000 graph loaded\n",
      " 18100 graph loaded\n",
      " 18200 graph loaded\n",
      " 18300 graph loaded\n",
      " 18400 graph loaded\n",
      " 18500 graph loaded\n",
      " 18600 graph loaded\n",
      " 18700 graph loaded\n",
      " 18800 graph loaded\n",
      " 18900 graph loaded\n",
      " 19000 graph loaded\n",
      " 19100 graph loaded\n",
      " 19200 graph loaded\n",
      " 19300 graph loaded\n",
      " 19400 graph loaded\n",
      " 19500 graph loaded\n",
      " 19600 graph loaded\n",
      " 19700 graph loaded\n",
      " 19800 graph loaded\n",
      " 19900 graph loaded\n",
      " 20000 graph loaded\n",
      " 20100 graph loaded\n",
      " 20200 graph loaded\n",
      " 20300 graph loaded\n",
      " 20400 graph loaded\n",
      " 20500 graph loaded\n",
      " 20600 graph loaded\n",
      " 20700 graph loaded\n",
      " 20800 graph loaded\n",
      " 20900 graph loaded\n",
      " 21000 graph loaded\n",
      " 21100 graph loaded\n",
      " 21200 graph loaded\n",
      " 21300 graph loaded\n",
      " 21400 graph loaded\n",
      " 21500 graph loaded\n",
      " 21600 graph loaded\n",
      " 21700 graph loaded\n",
      " 21800 graph loaded\n",
      " 21900 graph loaded\n",
      " 22000 graph loaded\n",
      " 22100 graph loaded\n",
      " 22200 graph loaded\n",
      " 22300 graph loaded\n",
      " 22400 graph loaded\n",
      " 22500 graph loaded\n",
      " 22600 graph loaded\n",
      " 22700 graph loaded\n",
      " 22800 graph loaded\n",
      " 22900 graph loaded\n",
      " 23000 graph loaded\n",
      " 23100 graph loaded\n",
      " 23200 graph loaded\n",
      " 23300 graph loaded\n",
      " 23400 graph loaded\n",
      " 23500 graph loaded\n",
      " 23600 graph loaded\n",
      " 23700 graph loaded\n",
      " 23800 graph loaded\n",
      " 23900 graph loaded\n",
      " 24000 graph loaded\n",
      " 24100 graph loaded\n",
      " 24200 graph loaded\n",
      " 24300 graph loaded\n",
      " 24400 graph loaded\n",
      " 24500 graph loaded\n",
      " 24600 graph loaded\n",
      " 24700 graph loaded\n",
      " 24800 graph loaded\n",
      " 24900 graph loaded\n",
      " 25000 graph loaded\n",
      " 25100 graph loaded\n",
      " 25200 graph loaded\n",
      " 25300 graph loaded\n",
      " 25400 graph loaded\n",
      " 25500 graph loaded\n",
      " 25600 graph loaded\n",
      " 25700 graph loaded\n",
      " 25800 graph loaded\n",
      " 25900 graph loaded\n",
      " 26000 graph loaded\n",
      " 26100 graph loaded\n",
      " 26200 graph loaded\n",
      " 26300 graph loaded\n",
      " 26400 graph loaded\n",
      " 26500 graph loaded\n",
      " 26600 graph loaded\n",
      " 26700 graph loaded\n",
      " 26800 graph loaded\n",
      " 26900 graph loaded\n",
      " 27000 graph loaded\n",
      " 27100 graph loaded\n",
      " 27200 graph loaded\n",
      " 27300 graph loaded\n",
      " 27400 graph loaded\n",
      " 27500 graph loaded\n",
      " 27600 graph loaded\n",
      " 27700 graph loaded\n",
      " 27800 graph loaded\n",
      " 27900 graph loaded\n",
      " 28000 graph loaded\n",
      " 28100 graph loaded\n",
      " 28200 graph loaded\n",
      " 28300 graph loaded\n",
      " 28400 graph loaded\n",
      " 28500 graph loaded\n",
      " 28600 graph loaded\n",
      " 28700 graph loaded\n",
      " 28800 graph loaded\n",
      " 28900 graph loaded\n",
      " 29000 graph loaded\n",
      " 29100 graph loaded\n",
      " 29200 graph loaded\n",
      " 29300 graph loaded\n",
      " 29400 graph loaded\n",
      " 29500 graph loaded\n",
      " 29600 graph loaded\n",
      " 29700 graph loaded\n",
      " 29800 graph loaded\n",
      " 29900 graph loaded\n",
      " 30000 graph loaded\n",
      " 30100 graph loaded\n",
      " 30200 graph loaded\n",
      " 30300 graph loaded\n",
      " 30400 graph loaded\n",
      " 30500 graph loaded\n",
      " 30600 graph loaded\n",
      " 30700 graph loaded\n",
      " 30800 graph loaded\n",
      " 30900 graph loaded\n",
      " 31000 graph loaded\n",
      " 31100 graph loaded\n",
      " 31200 graph loaded\n",
      " 31300 graph loaded\n",
      " 31400 graph loaded\n",
      " 31500 graph loaded\n",
      " 31600 graph loaded\n",
      " 31700 graph loaded\n",
      " 31800 graph loaded\n",
      " 31900 graph loaded\n",
      " 32000 graph loaded\n",
      " 32100 graph loaded\n",
      " 32200 graph loaded\n",
      " 32300 graph loaded\n",
      " 32400 graph loaded\n",
      " 32500 graph loaded\n",
      " 32600 graph loaded\n",
      " 32700 graph loaded\n",
      " 32800 graph loaded\n",
      " 32900 graph loaded\n",
      " 33000 graph loaded\n",
      " 33100 graph loaded\n",
      " 33200 graph loaded\n",
      " 33300 graph loaded\n",
      " 33400 graph loaded\n",
      " 33500 graph loaded\n",
      " 33600 graph loaded\n",
      " 33700 graph loaded\n",
      " 33800 graph loaded\n",
      " 33900 graph loaded\n",
      " 34000 graph loaded\n",
      " 34100 graph loaded\n",
      " 34200 graph loaded\n",
      " 34300 graph loaded\n",
      " 34400 graph loaded\n",
      " 34500 graph loaded\n",
      " 34600 graph loaded\n",
      " 34700 graph loaded\n",
      " 34800 graph loaded\n",
      " 34900 graph loaded\n",
      " 35000 graph loaded\n",
      " 35100 graph loaded\n",
      " 35200 graph loaded\n",
      " 35300 graph loaded\n",
      " 35400 graph loaded\n",
      " 35500 graph loaded\n",
      " 35600 graph loaded\n",
      " 35700 graph loaded\n",
      " 35800 graph loaded\n",
      " 35900 graph loaded\n",
      " 36000 graph loaded\n",
      " 36100 graph loaded\n",
      " 36200 graph loaded\n",
      " 36300 graph loaded\n",
      " 36400 graph loaded\n",
      " 36500 graph loaded\n",
      " 36600 graph loaded\n",
      " 36700 graph loaded\n",
      " 36800 graph loaded\n",
      " 36900 graph loaded\n",
      " 37000 graph loaded\n",
      " 37100 graph loaded\n",
      " 37200 graph loaded\n",
      " 37300 graph loaded\n",
      " 37400 graph loaded\n",
      " 37500 graph loaded\n",
      " 37600 graph loaded\n",
      " 37700 graph loaded\n",
      " 37800 graph loaded\n",
      " 37900 graph loaded\n",
      " 38000 graph loaded\n",
      " 38100 graph loaded\n",
      " 38200 graph loaded\n",
      " 38300 graph loaded\n",
      " 38400 graph loaded\n",
      " 38500 graph loaded\n",
      " 38600 graph loaded\n",
      " 38700 graph loaded\n",
      " 38800 graph loaded\n",
      " 38900 graph loaded\n",
      " 39000 graph loaded\n",
      " 39100 graph loaded\n",
      " 39200 graph loaded\n",
      " 39300 graph loaded\n",
      " 39400 graph loaded\n",
      " 39500 graph loaded\n",
      " 39600 graph loaded\n",
      " 39700 graph loaded\n",
      " 39800 graph loaded\n",
      " 39900 graph loaded\n",
      " 40000 graph loaded\n",
      " 40100 graph loaded\n",
      " 40200 graph loaded\n",
      " 40300 graph loaded\n",
      " 40400 graph loaded\n",
      " 40500 graph loaded\n",
      " 40600 graph loaded\n",
      " 40700 graph loaded\n",
      " 40800 graph loaded\n",
      " 40900 graph loaded\n",
      " 41000 graph loaded\n",
      " 41100 graph loaded\n",
      " 41200 graph loaded\n",
      " 41300 graph loaded\n",
      " 41400 graph loaded\n",
      " 41500 graph loaded\n",
      " 41600 graph loaded\n",
      " 41700 graph loaded\n",
      " 41800 graph loaded\n",
      " 41900 graph loaded\n",
      " 42000 graph loaded\n",
      " 42100 graph loaded\n",
      " 42200 graph loaded\n",
      " 42300 graph loaded\n",
      " 42400 graph loaded\n",
      " 42500 graph loaded\n",
      " 42600 graph loaded\n",
      " 42700 graph loaded\n",
      " 42800 graph loaded\n",
      " 42900 graph loaded\n",
      " 43000 graph loaded\n",
      " 43100 graph loaded\n",
      " 43200 graph loaded\n",
      " 43300 graph loaded\n",
      " 43400 graph loaded\n",
      " 43500 graph loaded\n",
      " 43600 graph loaded\n",
      " 43700 graph loaded\n",
      " 43800 graph loaded\n",
      " 43900 graph loaded\n",
      " 44000 graph loaded\n",
      " 44100 graph loaded\n",
      " 44200 graph loaded\n",
      " 44300 graph loaded\n",
      " 44400 graph loaded\n",
      " 44500 graph loaded\n",
      " 44600 graph loaded\n",
      " 44700 graph loaded\n",
      " 44800 graph loaded\n",
      " 44900 graph loaded\n",
      " 45000 graph loaded\n",
      " 45100 graph loaded\n",
      " 45200 graph loaded\n",
      " 45300 graph loaded\n",
      " 45400 graph loaded\n",
      " 45500 graph loaded\n",
      " 45600 graph loaded\n",
      " 45700 graph loaded\n",
      " 45800 graph loaded\n",
      " 45900 graph loaded\n",
      " 46000 graph loaded\n",
      " 46100 graph loaded\n",
      " 46200 graph loaded\n",
      " 46300 graph loaded\n",
      " 46400 graph loaded\n",
      " 46500 graph loaded\n",
      " 46600 graph loaded\n",
      " 46700 graph loaded\n",
      " 46800 graph loaded\n",
      " 46900 graph loaded\n",
      " 47000 graph loaded\n",
      " 47100 graph loaded\n",
      " 47200 graph loaded\n",
      " 47300 graph loaded\n",
      " 47400 graph loaded\n",
      " 47500 graph loaded\n",
      " 47600 graph loaded\n",
      " 47700 graph loaded\n",
      " 47800 graph loaded\n",
      " 47900 graph loaded\n",
      " 48000 graph loaded\n",
      " 48100 graph loaded\n",
      " 48200 graph loaded\n",
      " 48300 graph loaded\n",
      " 48400 graph loaded\n",
      " 48500 graph loaded\n",
      " 48600 graph loaded\n",
      " 48700 graph loaded\n",
      " 48800 graph loaded\n",
      " 48900 graph loaded\n",
      " 49000 graph loaded\n",
      " 49100 graph loaded\n",
      " 49200 graph loaded\n",
      " 49300 graph loaded\n",
      " 49400 graph loaded\n",
      " 49500 graph loaded\n",
      " 49600 graph loaded\n",
      " 49700 graph loaded\n",
      " 49800 graph loaded\n",
      " 49900 graph loaded\n",
      " 50000 graph loaded\n",
      " 50100 graph loaded\n",
      " 50200 graph loaded\n",
      " 50300 graph loaded\n",
      " 50400 graph loaded\n",
      " 50500 graph loaded\n",
      " 50600 graph loaded\n",
      " 50700 graph loaded\n",
      " 50800 graph loaded\n",
      " 50900 graph loaded\n",
      " 51000 graph loaded\n",
      " 51100 graph loaded\n",
      " 51200 graph loaded\n",
      " 51300 graph loaded\n",
      " 51400 graph loaded\n",
      " 51500 graph loaded\n",
      " 51600 graph loaded\n",
      " 51700 graph loaded\n",
      " 51800 graph loaded\n",
      " 51900 graph loaded\n",
      " 52000 graph loaded\n",
      " 52100 graph loaded\n",
      " 52200 graph loaded\n",
      " 52300 graph loaded\n",
      " 52400 graph loaded\n",
      " 52500 graph loaded\n",
      " 52600 graph loaded\n",
      " 52700 graph loaded\n",
      " 52800 graph loaded\n",
      " 52900 graph loaded\n",
      " 53000 graph loaded\n",
      " 53100 graph loaded\n",
      " 53200 graph loaded\n",
      " 53300 graph loaded\n",
      " 53400 graph loaded\n",
      " 53500 graph loaded\n",
      " 53600 graph loaded\n",
      " 53700 graph loaded\n",
      " 53800 graph loaded\n",
      " 53900 graph loaded\n",
      " 54000 graph loaded\n",
      " 54100 graph loaded\n",
      " 54200 graph loaded\n",
      " 54300 graph loaded\n",
      " 54400 graph loaded\n",
      " 54500 graph loaded\n",
      " 54600 graph loaded\n",
      " 54700 graph loaded\n",
      " 54800 graph loaded\n",
      " 54900 graph loaded\n",
      "self.num_data_load: 55000\n",
      "self.labels.shape: torch.Size([55000, 1])\n"
     ]
    }
   ],
   "source": [
    "data_manager = AmazonReviewGraphLoader(config, True, True, num_workers=2, shuffle=True, num_data_load = -1, device=device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_manager.graph_constructors[TextGraphType.CO_OCCURRENCE].device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from Scripts.Models.LightningModels.LightningModels import BinaryLightningModel\n",
    "import torch\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn\n",
    "\n",
    "class GcnGatModel1(nn.Module):\n",
    "    r\"\"\"\n",
    "    This class is for graph level classification or graph level regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_feature: int, out_features: int, dropout=0.1, *args, **kwargs):\n",
    "        super(GcnGatModel1, self).__init__(*args, **kwargs)\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.encoder = GSequential('x, edge_index, edge_weights', [\n",
    "            (GCNConv(input_feature, 256), 'x, edge_index, edge_weights ->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (GCNConv(256, 128), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (GCNConv(128, 64), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(64, 32), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GATv2Conv(32, 32, 4, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCN2Conv(128, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(128, 256), 'x3, edge_index->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCN2Conv(256, 0.5, 0.1, 2), 'x3, x1, edge_index, edge_weights->x3'),\n",
    "            (nn.ReLU(), 'x3->x3')\n",
    "        ])\n",
    "\n",
    "        self.pooling_layer1 = GCNConv(256, 5)\n",
    "        self.pooling_layer2 = DenseGCNConv(256, 1)\n",
    "        self.output_layer = Linear(256, self.num_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.encoder(x.x, x.edge_index, x.edge_attr)\n",
    "        all_s = self.pooling_layer1(x1, x.edge_index, x.edge_attr)\n",
    "        ci = torch.tensor([x[i].x.shape[0] for i in range(len(x))], dtype=torch.int, device=x1.device).cumsum(0, dtype=torch.int)\n",
    "        x2 = [x1[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        s_i = [all_s[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        x3 = torch.zeros((len(x2), 256), dtype=x1.dtype, device=x1.device)\n",
    "        \n",
    "        for i in range(len(ci)):\n",
    "            s = s_i[i]# self.pooling_layer1(x2[i], x[i].edge_index, x[i].edge_attr)\n",
    "            adj = torch.zeros((x[i].x.shape[0], x[i].x.shape[0]), device=x1.device)\n",
    "            adj[x[i].edge_index[0],x[i].edge_index[1]] = x[i].edge_attr\n",
    "            # adj = to_dense_adj(edge_index=x[i].edge_index, max_num_nodes=x[i].x.shape[0], edge_attr=x[i].edge_attr)\n",
    "            nodes, adj, _, _ = dense_diff_pool(x2[i], adj, s=s)\n",
    "            s = self.pooling_layer2(nodes, adj)\n",
    "            nodes, _, _, _ = dense_diff_pool(nodes, adj, s=s)\n",
    "            x3[i] = torch.squeeze(nodes)\n",
    "\n",
    "        # return x1\n",
    "        return self.output_layer(x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = GcnGatModel1(300, 1)\n",
    "torch_model = torch_model.to('cpu')\n",
    "# next(iter(torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataloader = data_manager.train_dataloader()\n",
    "# # X,y = next(iter(t_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_dataloader = data_manager.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(t_dataloader))\n",
    "X[0].x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = next(iter(v_dataloader))\n",
    "X[0].x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n",
      "i: 31\n",
      "i: 32\n",
      "i: 33\n",
      "i: 34\n",
      "i: 35\n",
      "i: 36\n",
      "i: 37\n",
      "i: 38\n",
      "i: 39\n",
      "i: 40\n",
      "i: 41\n",
      "i: 42\n",
      "i: 43\n",
      "i: 44\n",
      "i: 45\n",
      "i: 46\n",
      "i: 47\n",
      "i: 48\n",
      "i: 49\n",
      "i: 50\n",
      "i: 51\n",
      "i: 52\n",
      "i: 53\n",
      "i: 54\n",
      "i: 55\n",
      "i: 56\n",
      "i: 57\n",
      "i: 58\n",
      "i: 59\n",
      "i: 60\n",
      "i: 61\n",
      "i: 62\n",
      "i: 63\n",
      "i: 64\n",
      "i: 65\n",
      "i: 66\n",
      "i: 67\n",
      "i: 68\n",
      "i: 69\n",
      "i: 70\n",
      "i: 71\n",
      "i: 72\n",
      "i: 73\n",
      "i: 74\n",
      "i: 75\n",
      "i: 76\n",
      "i: 77\n",
      "i: 78\n",
      "i: 79\n",
      "i: 80\n",
      "i: 81\n",
      "i: 82\n",
      "i: 83\n",
      "i: 84\n",
      "i: 85\n",
      "i: 86\n",
      "i: 87\n",
      "i: 88\n",
      "i: 89\n",
      "i: 90\n",
      "i: 91\n",
      "i: 92\n",
      "i: 93\n",
      "i: 94\n",
      "i: 95\n",
      "i: 96\n",
      "i: 97\n",
      "i: 98\n",
      "i: 99\n",
      "i: 100\n",
      "i: 101\n",
      "i: 102\n",
      "i: 103\n",
      "i: 104\n",
      "i: 105\n",
      "i: 106\n",
      "i: 107\n",
      "i: 108\n",
      "i: 109\n",
      "i: 110\n",
      "i: 111\n",
      "i: 112\n",
      "i: 113\n",
      "i: 114\n",
      "i: 115\n",
      "i: 116\n",
      "i: 117\n",
      "i: 118\n",
      "i: 119\n",
      "i: 120\n",
      "i: 121\n",
      "i: 122\n",
      "i: 123\n",
      "i: 124\n",
      "i: 125\n",
      "i: 126\n",
      "i: 127\n",
      "i: 128\n",
      "i: 129\n",
      "i: 130\n",
      "i: 131\n",
      "i: 132\n",
      "i: 133\n",
      "i: 134\n",
      "i: 135\n",
      "i: 136\n",
      "i: 137\n",
      "i: 138\n",
      "i: 139\n",
      "i: 140\n",
      "i: 141\n",
      "i: 142\n",
      "i: 143\n",
      "i: 144\n",
      "i: 145\n",
      "i: 146\n",
      "i: 147\n",
      "i: 148\n",
      "i: 149\n",
      "i: 150\n",
      "i: 151\n",
      "i: 152\n",
      "i: 153\n",
      "i: 154\n",
      "i: 155\n",
      "i: 156\n",
      "i: 157\n",
      "i: 158\n",
      "i: 159\n",
      "i: 160\n",
      "i: 161\n",
      "i: 162\n",
      "i: 163\n",
      "i: 164\n",
      "i: 165\n",
      "i: 166\n",
      "i: 167\n",
      "i: 168\n",
      "i: 169\n",
      "i: 170\n",
      "i: 171\n",
      "i: 172\n",
      "i: 173\n",
      "i: 174\n",
      "i: 175\n",
      "i: 176\n",
      "i: 177\n",
      "i: 178\n",
      "i: 179\n",
      "i: 180\n",
      "i: 181\n",
      "i: 182\n",
      "i: 183\n",
      "i: 184\n",
      "i: 185\n",
      "i: 186\n",
      "i: 187\n",
      "i: 188\n",
      "i: 189\n",
      "i: 190\n",
      "i: 191\n",
      "i: 192\n",
      "i: 193\n",
      "i: 194\n",
      "i: 195\n",
      "i: 196\n",
      "i: 197\n",
      "i: 198\n",
      "i: 199\n",
      "i: 200\n",
      "i: 201\n",
      "i: 202\n",
      "i: 203\n",
      "i: 204\n",
      "i: 205\n",
      "i: 206\n",
      "i: 207\n",
      "i: 208\n",
      "i: 209\n",
      "i: 210\n",
      "i: 211\n",
      "i: 212\n",
      "i: 213\n",
      "i: 214\n",
      "i: 215\n",
      "i: 216\n",
      "i: 217\n",
      "i: 218\n",
      "i: 219\n",
      "i: 220\n",
      "i: 221\n",
      "i: 222\n",
      "i: 223\n",
      "i: 224\n",
      "i: 225\n",
      "i: 226\n",
      "i: 227\n",
      "i: 228\n",
      "i: 229\n",
      "i: 230\n",
      "i: 231\n",
      "i: 232\n",
      "i: 233\n",
      "i: 234\n",
      "i: 235\n",
      "i: 236\n",
      "i: 237\n",
      "i: 238\n",
      "i: 239\n",
      "i: 240\n",
      "i: 241\n",
      "i: 242\n",
      "i: 243\n",
      "i: 244\n",
      "i: 245\n",
      "i: 246\n",
      "i: 247\n",
      "i: 248\n",
      "i: 249\n",
      "i: 250\n",
      "i: 251\n",
      "i: 252\n",
      "i: 253\n",
      "i: 254\n",
      "i: 255\n",
      "i: 256\n",
      "train accuracy: 0.7686344844357976, loss: 0.7205719642833976, duration: 449.1848\n",
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n",
      "i: 31\n",
      "i: 32\n",
      "i: 33\n",
      "i: 34\n",
      "i: 35\n",
      "i: 36\n",
      "i: 37\n",
      "i: 38\n",
      "i: 39\n",
      "i: 40\n",
      "i: 41\n",
      "i: 42\n",
      "i: 43\n",
      "i: 44\n",
      "i: 45\n",
      "i: 46\n",
      "i: 47\n",
      "i: 48\n",
      "i: 49\n",
      "i: 50\n",
      "i: 51\n",
      "i: 52\n",
      "i: 53\n",
      "i: 54\n",
      "i: 55\n",
      "i: 56\n",
      "i: 57\n",
      "i: 58\n",
      "i: 59\n",
      "i: 60\n",
      "i: 61\n",
      "i: 62\n",
      "i: 63\n",
      "i: 64\n",
      "i: 65\n",
      "i: 66\n",
      "i: 67\n",
      "i: 68\n",
      "i: 69\n",
      "i: 70\n",
      "i: 71\n",
      "i: 72\n",
      "i: 73\n",
      "i: 74\n",
      "i: 75\n",
      "i: 76\n",
      "i: 77\n",
      "i: 78\n",
      "i: 79\n",
      "i: 80\n",
      "i: 81\n",
      "i: 82\n",
      "i: 83\n",
      "i: 84\n",
      "i: 85\n",
      "validation accuracy: 0.8130329459212547, loss: 0.4242337536673213, duration: 524.7510\n",
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n",
      "i: 31\n",
      "i: 32\n",
      "i: 33\n",
      "i: 34\n",
      "i: 35\n",
      "i: 36\n",
      "i: 37\n",
      "i: 38\n",
      "i: 39\n",
      "i: 40\n",
      "i: 41\n",
      "i: 42\n",
      "i: 43\n",
      "i: 44\n",
      "i: 45\n",
      "i: 46\n",
      "i: 47\n",
      "i: 48\n",
      "i: 49\n",
      "i: 50\n",
      "i: 51\n",
      "i: 52\n",
      "i: 53\n",
      "i: 54\n",
      "i: 55\n",
      "i: 56\n",
      "i: 57\n",
      "i: 58\n",
      "i: 59\n",
      "i: 60\n",
      "i: 61\n",
      "i: 62\n",
      "i: 63\n",
      "i: 64\n",
      "i: 65\n",
      "i: 66\n",
      "i: 67\n",
      "i: 68\n",
      "i: 69\n",
      "i: 70\n",
      "i: 71\n",
      "i: 72\n",
      "i: 73\n",
      "i: 74\n",
      "i: 75\n",
      "i: 76\n",
      "i: 77\n",
      "i: 78\n",
      "i: 79\n",
      "i: 80\n",
      "i: 81\n",
      "i: 82\n",
      "i: 83\n",
      "i: 84\n",
      "i: 85\n",
      "i: 86\n",
      "i: 87\n",
      "i: 88\n",
      "i: 89\n",
      "i: 90\n",
      "i: 91\n",
      "i: 92\n",
      "i: 93\n",
      "i: 94\n",
      "i: 95\n",
      "i: 96\n",
      "i: 97\n",
      "i: 98\n",
      "i: 99\n",
      "i: 100\n",
      "i: 101\n",
      "i: 102\n",
      "i: 103\n",
      "i: 104\n",
      "i: 105\n",
      "i: 106\n",
      "i: 107\n",
      "i: 108\n",
      "i: 109\n",
      "i: 110\n",
      "i: 111\n",
      "i: 112\n",
      "i: 113\n",
      "i: 114\n",
      "i: 115\n",
      "i: 116\n",
      "i: 117\n",
      "i: 118\n",
      "i: 119\n",
      "i: 120\n",
      "i: 121\n",
      "i: 122\n",
      "i: 123\n",
      "i: 124\n",
      "i: 125\n",
      "i: 126\n",
      "i: 127\n",
      "i: 128\n",
      "i: 129\n",
      "i: 130\n",
      "i: 131\n",
      "i: 132\n",
      "i: 133\n",
      "i: 134\n",
      "i: 135\n",
      "i: 136\n",
      "i: 137\n",
      "i: 138\n",
      "i: 139\n",
      "i: 140\n",
      "i: 141\n",
      "i: 142\n",
      "i: 143\n",
      "i: 144\n",
      "i: 145\n",
      "i: 146\n",
      "i: 147\n",
      "i: 148\n",
      "i: 149\n",
      "i: 150\n",
      "i: 151\n",
      "i: 152\n",
      "i: 153\n",
      "i: 154\n",
      "i: 155\n",
      "i: 156\n",
      "i: 157\n",
      "i: 158\n",
      "i: 159\n",
      "i: 160\n",
      "i: 161\n",
      "i: 162\n",
      "i: 163\n",
      "i: 164\n",
      "i: 165\n",
      "i: 166\n",
      "i: 167\n",
      "i: 168\n",
      "i: 169\n",
      "i: 170\n",
      "i: 171\n",
      "i: 172\n",
      "i: 173\n",
      "i: 174\n",
      "i: 175\n",
      "i: 176\n",
      "i: 177\n",
      "i: 178\n",
      "i: 179\n",
      "i: 180\n",
      "i: 181\n",
      "i: 182\n",
      "i: 183\n",
      "i: 184\n",
      "i: 185\n",
      "i: 186\n",
      "i: 187\n",
      "i: 188\n",
      "i: 189\n",
      "i: 190\n",
      "i: 191\n",
      "i: 192\n",
      "i: 193\n",
      "i: 194\n",
      "i: 195\n",
      "i: 196\n",
      "i: 197\n",
      "i: 198\n",
      "i: 199\n",
      "i: 200\n",
      "i: 201\n",
      "i: 202\n",
      "i: 203\n",
      "i: 204\n",
      "i: 205\n",
      "i: 206\n",
      "i: 207\n",
      "i: 208\n",
      "i: 209\n",
      "i: 210\n",
      "i: 211\n",
      "i: 212\n",
      "i: 213\n",
      "i: 214\n",
      "i: 215\n",
      "i: 216\n",
      "i: 217\n",
      "i: 218\n",
      "i: 219\n",
      "i: 220\n",
      "i: 221\n",
      "i: 222\n",
      "i: 223\n",
      "i: 224\n",
      "i: 225\n",
      "i: 226\n",
      "i: 227\n",
      "i: 228\n",
      "i: 229\n",
      "i: 230\n",
      "i: 231\n",
      "i: 232\n",
      "i: 233\n",
      "i: 234\n",
      "i: 235\n",
      "i: 236\n",
      "i: 237\n",
      "i: 238\n",
      "i: 239\n",
      "i: 240\n",
      "i: 241\n",
      "i: 242\n",
      "i: 243\n",
      "i: 244\n",
      "i: 245\n",
      "i: 246\n",
      "i: 247\n",
      "i: 248\n",
      "i: 249\n",
      "i: 250\n",
      "i: 251\n",
      "i: 252\n",
      "i: 253\n",
      "i: 254\n",
      "i: 255\n",
      "i: 256\n",
      "train accuracy: 0.8069978112840467, loss: 0.4469593851482822, duration: 425.2336\n",
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n",
      "i: 31\n",
      "i: 32\n",
      "i: 33\n",
      "i: 34\n",
      "i: 35\n",
      "i: 36\n",
      "i: 37\n",
      "i: 38\n",
      "i: 39\n",
      "i: 40\n",
      "i: 41\n",
      "i: 42\n",
      "i: 43\n",
      "i: 44\n",
      "i: 45\n",
      "i: 46\n",
      "i: 47\n",
      "i: 48\n",
      "i: 49\n",
      "i: 50\n",
      "i: 51\n",
      "i: 52\n",
      "i: 53\n",
      "i: 54\n",
      "i: 55\n",
      "i: 56\n",
      "i: 57\n",
      "i: 58\n",
      "i: 59\n",
      "i: 60\n",
      "i: 61\n",
      "i: 62\n",
      "i: 63\n",
      "i: 64\n",
      "i: 65\n",
      "i: 66\n",
      "i: 67\n",
      "i: 68\n",
      "i: 69\n",
      "i: 70\n",
      "i: 71\n",
      "i: 72\n",
      "i: 73\n",
      "i: 74\n",
      "i: 75\n",
      "i: 76\n",
      "i: 77\n",
      "i: 78\n",
      "i: 79\n",
      "i: 80\n",
      "i: 81\n",
      "i: 82\n",
      "i: 83\n",
      "i: 84\n",
      "i: 85\n",
      "validation accuracy: 0.7411276649597079, loss: 0.619371029873227, duration: 502.0306\n",
      "i: 0\n",
      "i: 1\n",
      "i: 2\n",
      "i: 3\n",
      "i: 4\n",
      "i: 5\n",
      "i: 6\n",
      "i: 7\n",
      "i: 8\n",
      "i: 9\n",
      "i: 10\n",
      "i: 11\n",
      "i: 12\n",
      "i: 13\n",
      "i: 14\n",
      "i: 15\n",
      "i: 16\n",
      "i: 17\n",
      "i: 18\n",
      "i: 19\n",
      "i: 20\n",
      "i: 21\n",
      "i: 22\n",
      "i: 23\n",
      "i: 24\n",
      "i: 25\n",
      "i: 26\n",
      "i: 27\n",
      "i: 28\n",
      "i: 29\n",
      "i: 30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#Y103sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m predicted \u001b[39m=\u001b[39m torch_model(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#Y103sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(predicted, y)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#Y103sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#Y103sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#Y103sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msigmoid(predicted)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "torch_model = torch_model.to('cuda')\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(torch_model.parameters(), 0.001, weight_decay=0.0055)\n",
    "for i in range(1000):\n",
    "    \n",
    "    batch_acc=[]\n",
    "    batch_loss = []\n",
    "    begin = time.time()\n",
    "    torch_model.train()\n",
    "    i = 0\n",
    "    for X,y in t_dataloader:\n",
    "        print(f'i: {i}')\n",
    "        i = i + 1\n",
    "        X = X.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        predicted = torch_model(X)\n",
    "        loss = loss_func(predicted, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = torch.nn.functional.sigmoid(predicted)\n",
    "        predicted[predicted<0.5] = 0\n",
    "        predicted[predicted>=0.5] = 1\n",
    "        accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "        batch_acc.append(accuracy)\n",
    "        batch_loss.append(loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f'train accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')\n",
    "    \n",
    "    torch_model.eval()\n",
    "    batch_acc=[]\n",
    "    batch_loss = []\n",
    "    i = 0\n",
    "    for X,y in v_dataloader:\n",
    "        print(f'i: {i}')\n",
    "        i = i + 1\n",
    "        X = X.to('cuda')\n",
    "        y = y.to('cuda')\n",
    "        with torch.inference_mode():\n",
    "            predicted = torch_model(X)\n",
    "            loss = loss_func(predicted, y)\n",
    "        predicted = torch.nn.functional.sigmoid(predicted)\n",
    "        predicted[predicted<0.5] = 0\n",
    "        predicted[predicted>=0.5] = 1\n",
    "        accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "        batch_acc.append(accuracy)\n",
    "        batch_loss.append(loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "    print(f'validation accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in t_dataloader:\n",
    "    X = X.to('cuda')\n",
    "    # for i in range(len(X)):\n",
    "    #     X[i].x = X[i].x.to(device)\n",
    "    #     X[i].edge_index = X[i].edge_index.to(device)\n",
    "    #     X[i].edge_attr = X[i].edge_attr.to(device)\n",
    "    torch_model(X)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    }
   ],
   "source": [
    "lightning_model = BinaryLightningModel(torch_model,\n",
    "                                 torch.optim.Adam(torch_model.parameters(), lr=0.001, weight_decay=0.005),\n",
    "                                       torch.nn.BCEWithLogitsLoss(),\n",
    "                                       batch_size=batch_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightning_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_dataloader = data_manager.train_dataloader()\n",
    "# loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(torch_model.parameters(), 0.001, weight_decay=0.0055)\n",
    "# for X,y in t_dataloader:\n",
    "#     optimizer.zero_grad()\n",
    "#     predicted = torch_model(X)\n",
    "#     loss = loss_func(predicted, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | GcnGatModel1      | 245 K \n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "2 | train_acc | BinaryAccuracy    | 0     \n",
      "3 | val_acc   | BinaryAccuracy    | 0     \n",
      "4 | test_acc  | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "245 K     Trainable params\n",
      "0         Non-trainable params\n",
      "245 K     Total params\n",
      "0.983     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbb\n",
      "model2: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n",
      "arg: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n",
      "arg: <class 'NoneType'>\n",
      "arg: <class 'NoneType'>\n",
      "arg: <class '__main__.AmazonReviewGraphLoader'>\n",
      "arg: <class 'NoneType'>\n",
      "kwargs: {}\n",
      "trainer_fn1: <bound method Trainer._fit_impl of <lightning.pytorch.trainer.trainer.Trainer object at 0x000001880B249870>>\n",
      "trainer_fn2: <class 'method'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58cbda42653e4f299c932cfc5457d318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7bff5a72ac4b7fbb9d32283aef2630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrainer_fn2: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(trainer_fn)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:583\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    577\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    578\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    579\u001b[0m     ckpt_path,\n\u001b[0;32m    580\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    582\u001b[0m )\n\u001b[1;32m--> 583\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    585\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:992\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1038\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1038\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1039\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:137\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m--> 137\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end(data_fetcher)\n\u001b[0;32m    138\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:285\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    283\u001b[0m     call\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer, \u001b[39m\"\u001b[39m\u001b[39mon_validation_model_zero_grad\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mval_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[39mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[39mreturn\u001b[39;00m loop_run(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[39m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    386\u001b[0m step_args \u001b[39m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[39melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, hook_name, \u001b[39m*\u001b[39;49mstep_args)\n\u001b[0;32m    393\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:314\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 314\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mvalidation_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mvalidation_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:81\u001b[0m, in \u001b[0;36mBinaryLightningModel.validation_step\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidation_step\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 81\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(BinaryLightningModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mvalidation_step(data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     82\u001b[0m     predicted_labels \u001b[39m=\u001b[39m out_features \u001b[39mif\u001b[39;00m out_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39margmax(out_features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:32\u001b[0m, in \u001b[0;36mBaseLightningModel.validation_step\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m data, labels \u001b[39m=\u001b[39m data_batch\n\u001b[1;32m---> 32\u001b[0m out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(data)\n\u001b[0;32m     33\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_func(out_features, labels\u001b[39m.\u001b[39mview(out_features\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:21\u001b[0m, in \u001b[0;36mBaseLightningModel.forward\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data_batch)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m adj \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((x[i]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x[i]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), device\u001b[39m=\u001b[39mx1\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m adj[x[i]\u001b[39m.\u001b[39;49medge_index[\u001b[39m0\u001b[39;49m],x[i]\u001b[39m.\u001b[39;49medge_index[\u001b[39m1\u001b[39;49m]] \u001b[39m=\u001b[39m x[i]\u001b[39m.\u001b[39medge_attr\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# adj = to_dense_adj(edge_index=x[i].edge_index, max_num_nodes=x[i].x.shape[0], edge_attr=x[i].edge_attr)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, accelerator\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m, num_sanity_val_steps\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# trainer.fit(lightning_model, datamodule=data_manager)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lightning_model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             datamodule\u001b[39m=\u001b[39;49mdata_manager\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39m# train_dataloaders=data_manager.train_dataloader(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             \u001b[39m# val_dataloaders=data_manager.val_dataloader(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m# logger=CSVLogger(save_dir='logs/', name='sample_model'),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             \u001b[39m# default_root_dir=\"~/Desktop\"\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:547\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel2: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    548\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:73\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mfor\u001b[39;00m logger \u001b[39min\u001b[39;00m trainer\u001b[39m.\u001b[39mloggers:\n\u001b[0;32m     72\u001b[0m     logger\u001b[39m.\u001b[39mfinalize(\u001b[39m\"\u001b[39m\u001b[39mfailed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 73\u001b[0m trainer\u001b[39m.\u001b[39;49m_teardown()\n\u001b[0;32m     74\u001b[0m \u001b[39m# teardown might access the stage so we reset it after\u001b[39;00m\n\u001b[0;32m     75\u001b[0m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstage \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1015\u001b[0m, in \u001b[0;36mTrainer._teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1012\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_teardown\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1013\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and Callback;\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[39m    those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1015\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49mteardown()\n\u001b[0;32m   1016\u001b[0m     loop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_active_loop\n\u001b[0;32m   1017\u001b[0m     \u001b[39m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:524\u001b[0m, in \u001b[0;36mStrategy.teardown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mteardown\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    519\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This method is called to teardown the training process.\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \n\u001b[0;32m    521\u001b[0m \u001b[39m    It is the right place to release memory and free other resources.\u001b[39;00m\n\u001b[0;32m    522\u001b[0m \n\u001b[0;32m    523\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 524\u001b[0m     _optimizers_to_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizers, torch\u001b[39m.\u001b[39;49mdevice(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    526\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    527\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: moving model to CPU\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\utilities\\optimizer.py:28\u001b[0m, in \u001b[0;36m_optimizers_to_device\u001b[1;34m(optimizers, device)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Moves optimizer states for a sequence of optimizers to the device.\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m optimizers:\n\u001b[1;32m---> 28\u001b[0m     _optimizer_to_device(opt, device)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\utilities\\optimizer.py:34\u001b[0m, in \u001b[0;36m_optimizer_to_device\u001b[1;34m(optimizer, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Moves the state of a single optimizer to the device.\"\"\"\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m p, v \u001b[39min\u001b[39;00m optimizer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 34\u001b[0m     optimizer\u001b[39m.\u001b[39mstate[p] \u001b[39m=\u001b[39m apply_to_collection(v, Tensor, move_data_to_device, device, allow_frozen\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning_utilities\\core\\apply_func.py:59\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m out \u001b[39m=\u001b[39m []\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m data\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 59\u001b[0m     v \u001b[39m=\u001b[39m apply_to_collection(\n\u001b[0;32m     60\u001b[0m         v,\n\u001b[0;32m     61\u001b[0m         dtype,\n\u001b[0;32m     62\u001b[0m         function,\n\u001b[0;32m     63\u001b[0m         \u001b[39m*\u001b[39margs,\n\u001b[0;32m     64\u001b[0m         wrong_dtype\u001b[39m=\u001b[39mwrong_dtype,\n\u001b[0;32m     65\u001b[0m         include_none\u001b[39m=\u001b[39minclude_none,\n\u001b[0;32m     66\u001b[0m         allow_frozen\u001b[39m=\u001b[39mallow_frozen,\n\u001b[0;32m     67\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     69\u001b[0m     \u001b[39mif\u001b[39;00m include_none \u001b[39mor\u001b[39;00m v \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m         out\u001b[39m.\u001b[39mappend((k, v))\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning_utilities\\core\\apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py:102\u001b[0m, in \u001b[0;36mmove_data_to_device\u001b[1;34m(batch, device)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# user wrongly implemented the `_TransferableDataType` and forgot to return `self`.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m apply_to_collection(batch, dtype\u001b[39m=\u001b[39;49m_TransferableDataType, function\u001b[39m=\u001b[39;49mbatch_to)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning_utilities\\core\\apply_func.py:51\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39m# Breaking condition\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, dtype) \u001b[39mand\u001b[39;00m (wrong_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, wrong_dtype)):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     53\u001b[0m elem_type \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(data)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Recursively apply to collection items\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py:96\u001b[0m, in \u001b[0;36mmove_data_to_device.<locals>.batch_to\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Tensor) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mdevice) \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _BLOCKING_DEVICE_TYPES:\n\u001b[0;32m     95\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mnon_blocking\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m data_output \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m data_output \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[39mreturn\u001b[39;00m data_output\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=1000, accelerator='gpu', num_sanity_val_steps=0)\n",
    "# trainer.fit(lightning_model, datamodule=data_manager)\n",
    "trainer.fit(lightning_model,\n",
    "            datamodule=data_manager\n",
    "            # train_dataloaders=data_manager.train_dataloader(),\n",
    "            # val_dataloaders=data_manager.val_dataloader(),\n",
    "            # logger=CSVLogger(save_dir='logs/', name='sample_model'),\n",
    "            # default_root_dir=\"~/Desktop\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLightningModel(\n",
       "  (model): GcnGatModel1(\n",
       "    (encoder): Sequential(\n",
       "      (0): GCNConv(300, 256)\n",
       "      (1): ReLU()\n",
       "      (2): GCNConv(256, 128)\n",
       "      (3): ReLU()\n",
       "      (4): GCNConv(128, 64)\n",
       "      (5): ReLU()\n",
       "      (6): GCNConv(64, 32)\n",
       "      (7): ReLU()\n",
       "      (8): GATv2Conv(32, 32, heads=4)\n",
       "      (9): ReLU()\n",
       "      (10): GCN2Conv(128, alpha=0.5, beta=0.04879016416943204)\n",
       "      (11): ReLU()\n",
       "      (12): GCNConv(128, 256)\n",
       "      (13): ReLU()\n",
       "      (14): GCN2Conv(256, alpha=0.5, beta=0.04879016416943204)\n",
       "      (15): ReLU()\n",
       "    )\n",
       "    (pooling_layer1): GCNConv(256, 5)\n",
       "    (pooling_layer2): DenseGCNConv(256, 1)\n",
       "    (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_func): BCEWithLogitsLoss()\n",
       "  (train_acc): BinaryAccuracy()\n",
       "  (val_acc): BinaryAccuracy()\n",
       "  (test_acc): BinaryAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_checkpoint('example.ckpt')\n",
    "BinaryLightningModel.load_from_checkpoint(checkpoint_path='example.ckpt', model=torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lightning_model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     logits \u001b[39m=\u001b[39m lightning_model(\u001b[39m'\u001b[39;49m\u001b[39mnew_data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:21\u001b[0m, in \u001b[0;36mBaseLightningModel.forward\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data_batch)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x\u001b[39m.\u001b[39;49mx, x\u001b[39m.\u001b[39medge_index, x\u001b[39m.\u001b[39medge_attr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     all_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling_layer1(x1, x\u001b[39m.\u001b[39medge_index, x\u001b[39m.\u001b[39medge_attr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     ci \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([x[i]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x))], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint, device\u001b[39m=\u001b[39mx1\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mcumsum(\u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "lightning_model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = lightning_model('new_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import summary\n",
    "from tqdm import tqdm\n",
    "from Scripts.Models.LightningModels.LightningModels import BaseLightningModel\n",
    "from Scripts.Models.ModelsManager.ModelManager import ModelManager\n",
    "from Scripts.Models.BaseModels.GATGCNClassifierSimple import GNNClassifier\n",
    "from Scripts.DataManager.GraphLoader.NLabeledGraphLoader import NLabeledGraphLoader\n",
    "from Scripts.Utils.enums import Optimizer, LossType\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class SimpleNodeClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self, graph_handler: NLabeledGraphLoader, device=torch.device('cpu'),\n",
    "                 lr=0.01, weight_decay=0.001, optimizer_type: Optimizer = Optimizer.ADAM,\n",
    "                 loss_type: LossType = LossType.CROSS_ENTROPY):\n",
    "        super(SimpleNodeClassifierModelManager, self).__init__(lr, weight_decay, device)\n",
    "        self.graph_handler = graph_handler\n",
    "        self.num_output_classes = self.graph_handler.num_classes\n",
    "        self.num_input_features = self.graph_handler.num_features\n",
    "        self.loss_type = loss_type\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.model, self.optimizer, self.loss_func = self._create_model(lr, weight_decay, optimizer_type, loss_type)\n",
    "        self.lightning_model = BaseLightningModel(self.model, self.optimizer, self.loss_func)\n",
    "\n",
    "    def train(self, epoch_num: int = 100, lr: float = None, l2_norm: float = None, optimizer: Optimizer = None):\n",
    "\n",
    "        trainer = L.Trainer(max_epochs=100, accelerator='gpu', devices=1)\n",
    "        trainer.fit(self.lightning_model,\n",
    "                    train_dataloaders=self.graph_handler.get_train_data(),\n",
    "                    val_dataloaders=self.graph_handler.get_val_data()\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "        if lr or l2_norm or optimizer:\n",
    "            self.set_optimizer(lr, l2_norm, optimizer)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        train_node_x, train_node_y, train_edges = self.graph_handler.get_train_data()\n",
    "        test_node_x, test_node_y, test_edges = self.graph_handler.get_test_data()\n",
    "        for i in tqdm(range(epoch_num)):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            my_node, my_label, my_edges = self.graph_handler.extract_random_sub_edges_graph()\n",
    "            y_hat: torch.Tensor = self.model(my_node, my_edges)\n",
    "            loss = self.loss_func(F.one_hot(my_label, self.num_output_classes).float(), y_hat)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            accuracy = (torch.sum((y_hat.argmax(dim=1) == my_label).float()) / len(my_label)).cpu().numpy()\n",
    "            train_accuracies.append(accuracy)\n",
    "\n",
    "            my_node, my_label, my_edges = self.graph_handler.extract_random_sub_edges_graph()\n",
    "            y_hat: torch.Tensor = self.model(my_node, my_edges)\n",
    "            loss = self.loss_func(F.one_hot(my_label, self.graph_handler.num_classes).float(), y_hat)\n",
    "            loss_value = loss.item()\n",
    "            test_losses.append(loss_value)\n",
    "            accuracy = (torch.sum((y_hat.argmax(dim=1) == my_label).float()) / len(my_label)).cpu().numpy()\n",
    "            test_accuracies.append(accuracy)\n",
    "        self.history['train_losses'] = train_losses\n",
    "        self.history['train_accuracies'] = train_accuracies\n",
    "        self.history['test_losses'] = test_losses\n",
    "        self.history['test_accuracies'] = test_accuracies\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self):\n",
    "        node_x, node_y, edges = self.graph_handler.get_val_data()\n",
    "        self.model.eval()\n",
    "        y_hat: torch.Tensor = self.model(node_x, edges)\n",
    "        loss = self.loss_func(F.one_hot(node_y, self.num_output_classes).float(), y_hat)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def predict(self, node_x, edge_index):\n",
    "        self.model.eval()\n",
    "        y_hat: torch.Tensor = self.model(node_x, edge_index)\n",
    "        return y_hat\n",
    "\n",
    "    def draw_summary(self):\n",
    "        nodes_x, nodes_y, edge_indices_test = self.graph_handler.get_test_data()\n",
    "        nodes_x, nodes_y, edge_indices_test = self.graph_handler.extract_random_sub_edges_graph(2)\n",
    "        print(summary(self.model, nodes_x, edge_indices_test))\n",
    "\n",
    "    def _create_model(self, lr, l2_norm, optimizer_type, loss_type):\n",
    "        model = GNNClassifier(input_feature=self.num_input_features, class_counts=self.num_output_classes)\n",
    "        optimizer = ModelManager._create_optimizer(model, lr, l2_norm, optimizer_type)\n",
    "        loss_func = ModelManager._create_loss_func(loss_type)\n",
    "        model.to(self.device)\n",
    "        return model, optimizer, loss_func\n",
    "\n",
    "    def set_optimizer(self, lr, l2_norm, optimizer=Optimizer.ADAM):\n",
    "        if lr:\n",
    "            self.lr = lr\n",
    "        if l2_norm:\n",
    "            self.l2_norm = l2_norm\n",
    "        if optimizer:\n",
    "            self.optimizer_type = optimizer\n",
    "        self.optimizer = ModelManager._create_optimizer(self.model, self.lr, self.l2_norm, self.optimizer_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "\n",
    "if 'Train Data':\n",
    "    model_manager = SimpleGraphClassifierModelManager(\n",
    "        data_manager.graph_constructors[TextGraphType.CO_OCCURRENCE])\n",
    "    model_manager.draw_summary()\n",
    "    model_manager.train()\n",
    "    model_manager.draw_training_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Draw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch import LightningModel\n",
    "mymodel = GcnGatModel1(100, 10, config)\n",
    "print(type(mymodel))\n",
    "model = LightningModel(mymodel)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def sample_func() -> Tuple[int, float]:\n",
    "    return 1, 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "targets = sample_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Scripts.Utils.enums import Optimizer\n",
    "\n",
    "type(Optimizer.ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_func():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aa: Callable = None\n",
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
