{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from Scripts.DataManager.DabasePreparations.AmazonReviewSentiGraph import AmazonReviewSentiGraph\n",
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "config = Config(r'C:\\Users\\fardin\\Projects\\ColorIntelligence')\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import TextGraphType\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import os\n",
    "# os.environ['TORCH_USE_CUDA_DSA']\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "device = 'cuda'\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set, Tuple, Any\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS, TRAIN_DATALOADERS\n",
    "# from torch_geometric.data.lightning.datamodule import LightningDataModule\n",
    "from lightning.pytorch import LightningDataModule\n",
    "from torch_geometric.utils import augmentation\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.utils import subgraph, train_test_split_edges\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.Utils.GraphCollection.GraphCollection import GraphCollection\n",
    "\n",
    "\n",
    "class GraphLoader(LightningDataModule):\n",
    "\n",
    "    def __init__(self, config: Config, device, has_val: bool, has_test: bool, test_size=0.2, val_size=0.15, *args, **kwargs):\n",
    "        super(GraphLoader, self).__init__() #has_val, has_test, **kwargs)\n",
    "        self.config = config\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.device = device\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_dataloader(self) -> TRAIN_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def test_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def val_dataloader(self) -> EVAL_DATALOADERS:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def zero_rule_baseline():\n",
    "        pass\n",
    "\n",
    "    # def predict_dataloader(self) -> EVAL_DATALOADERS:\n",
    "    # def transfer_batch_to_device(self, batch: Any, device: torch.device, dataloader_idx: int) -> Any:\n",
    "    # def on_before_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:\n",
    "    # def on_after_batch_transfer(self, batch: Any, dataloader_idx: int) -> Any:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Dict\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from Scripts.Configs.ConfigClass import Config\n",
    "from Scripts.DataManager.GraphConstructor.CoOccurrenceGraphConstructor import CoOccurrenceGraphConstructor\n",
    "from Scripts.DataManager.GraphConstructor.GraphConstructor import GraphConstructor, TextGraphType\n",
    "# from Scripts.DataManager.GraphLoader.GraphLoader import GraphLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "from Scripts.DataManager.Datasets.GraphConstructorDataset import GraphConstructorDataset\n",
    "\n",
    "class AmazonReviewGraphLoader(GraphLoader):\n",
    "\n",
    "    def __init__(self, config: Config, has_val: bool, has_test: bool, test_size=0.2, val_size=0.2, num_workers=2,\n",
    "                 drop_last=True, train_data_path='', test_data_path='', graphs_path='', batch_size = 32,\n",
    "                 device='cpu', shuffle = False, num_data_load=-1,\n",
    "                 graph_type: TextGraphType = TextGraphType.CO_OCCURRENCE, *args, **kwargs):\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['batch_size'] = batch_size\n",
    "        kwargs['num_workers'] = num_workers\n",
    "        kwargs['shuffle'] = shuffle\n",
    "        super(AmazonReviewGraphLoader, self)\\\n",
    "            .__init__(config, device, has_val, has_test, test_size, val_size, *args, **kwargs)\n",
    "\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "        self.graph_type = graph_type\n",
    "        self.train_data_path = 'data/Amazon-Review/train_sm.csv' if train_data_path == '' else train_data_path\n",
    "        self.test_data_path = 'data/Amazon-Review/test_sm.csv' if test_data_path == '' else test_data_path\n",
    "        self.train_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.test_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.labels = None\n",
    "        self.dataset = None\n",
    "        self.shuffle = shuffle\n",
    "        self.num_node_features = 0\n",
    "        self.num_classes = 0\n",
    "        self.df: pd.DataFrame = pd.DataFrame()\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset = None, None, None\n",
    "        self.train_df = pd.read_csv(path.join(self.config.root, self.train_data_path))\n",
    "        self.test_df = pd.read_csv(path.join(self.config.root, self.test_data_path))\n",
    "        self.train_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.test_df.columns = ['Polarity', 'Title', 'Review']\n",
    "        self.train_df = self.train_df[['Polarity', 'Review']]\n",
    "        self.test_df = self.test_df[['Polarity', 'Review']]\n",
    "        self.df = pd.concat([self.train_df, self.test_df])\n",
    "        self.num_data_load = num_data_load if num_data_load>0 else self.df.shape[0]\n",
    "        self.num_data_load = num_data_load if self.num_data_load < self.df.shape[0] else self.df.shape[0] \n",
    "        self.df = self.df.iloc[:self.num_data_load]\n",
    "        self.df.index = np.arange(0, self.num_data_load)\n",
    "        self.graph_constructors = self.__set_graph_constructors(self.graph_type)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        graph_constructor.setup()\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        labels = self.df['Polarity'][:self.num_data_load]\n",
    "        labels = labels.apply(lambda p: 0 if p == 1 else 1).to_numpy()\n",
    "        labels = torch.from_numpy(labels)\n",
    "        self.labels = labels.to(torch.float32).view(-1, 1).to(self.device)\n",
    "        graph_constructor = self.graph_constructors[TextGraphType.CO_OCCURRENCE]\n",
    "        \n",
    "        print(f'self.labels.shape: {self.labels.shape}')\n",
    "        self.dataset = GraphConstructorDataset(graph_constructor, self.labels)\n",
    "        sample_graph = graph_constructor.get_first()\n",
    "        self.num_node_features = sample_graph.num_features\n",
    "        self.num_classes = len(torch.unique(self.labels))\n",
    "        self.__train_dataset, self.__val_dataset, self.__test_dataset =\\\n",
    "            random_split(self.dataset, [1-self.val_size-self.test_size, self.val_size, self.test_size])\n",
    "        self.__train_dataloader =  DataLoader(self.__train_dataset, batch_size=self.batch_size, drop_last=self.drop_last, shuffle=self.shuffle, num_workers=self.num_workers, persistent_workers=True)\n",
    "        self.__test_dataloader =  DataLoader(self.__test_dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n",
    "        self.__val_dataloader =  DataLoader(self.__val_dataset, batch_size=self.batch_size, num_workers=self.num_workers, persistent_workers=True)\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def setup(self, stage: str):\n",
    "        pass\n",
    "\n",
    "    def teardown(self, stage: str) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.__train_dataloader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.__test_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self.__val_dataloader \n",
    "\n",
    "    def __set_graph_constructors(self, graph_type: TextGraphType):\n",
    "        graph_constructors: Dict[TextGraphType, GraphConstructor] = {}\n",
    "        if TextGraphType.CO_OCCURRENCE in graph_type:\n",
    "            graph_constructors[TextGraphType.CO_OCCURRENCE] = self.__get_co_occurrence_graph()\n",
    "        if TextGraphType.DEPENDENCY in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.SEQUENTIAL in graph_type:\n",
    "            pass\n",
    "        if TextGraphType.TAGS in graph_type:\n",
    "            pass\n",
    "        return graph_constructors\n",
    "\n",
    "    def __get_co_occurrence_graph(self):\n",
    "        print(f'self.num_data_load: {self.num_data_load}')\n",
    "        return CoOccurrenceGraphConstructor(self.df['Review'][:self.num_data_load], 'data/GraphData/AmazonReview', self.config, lazy_construction=False, load_preprocessed_data=True, naming_prepend='graph', num_data_load=self.num_data_load, device=self.device)\n",
    "    \n",
    "    def zero_rule_baseline(self):\n",
    "        return f'zero_rule baseline: {(len(self.labels[self.labels>0.5])* 100.0 / len(self.labels))  : .2f}%'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.num_data_load: 200\n",
      "filename: C:\\Users\\fardin\\Projects\\ColorIntelligence\\data/GraphData/AmazonReview\\graph_var.txt\n",
      " 0 graph loaded\n",
      " 100 graph loaded\n",
      "self.num_data_load: 200\n",
      "self.labels.shape: torch.Size([200, 1])\n"
     ]
    }
   ],
   "source": [
    "data_manager = AmazonReviewGraphLoader(config, True, True, num_workers=2, shuffle=True, num_data_load = -1, device='cpu', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv, GCN2Conv, DenseGCNConv, dense_diff_pool, BatchNorm\n",
    "from torch_geometric.nn import Sequential as GSequential\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "from torch import nn\n",
    "\n",
    "class GcnGatModel1(nn.Module):\n",
    "    r\"\"\"\n",
    "    This class is for graph level classification or graph level regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_feature: int, out_features: int, base_hidden_feature: int=256, dropout=0.1, *args, **kwargs):\n",
    "        super(GcnGatModel1, self).__init__(*args, **kwargs)\n",
    "        self.input_features = input_feature\n",
    "        self.num_out_features = out_features\n",
    "        self.bsh: int = base_hidden_feature\n",
    "        bsh2: int = int(self.bsh/2)\n",
    "        bsh4: int = int(self.bsh/4)\n",
    "        bsh8: int = int(self.bsh/8)\n",
    "        \n",
    "        self.encoder = GSequential('x, edge_index, edge_weights', [\n",
    "            (GCNConv(input_feature, self.bsh), 'x, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            \n",
    "            (GCNConv(self.bsh, bsh2), 'x1, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCNConv(bsh2, bsh4), 'x2, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (nn.Dropout(dropout), 'x3->x3'),\n",
    "            \n",
    "            (GCNConv(bsh4, bsh8), 'x3, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (nn.Dropout(dropout), 'x4->x4'),\n",
    "            (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (nn.Dropout(dropout), 'x4->x4'),\n",
    "            (GCNConv(bsh8, bsh8), 'x4, edge_index, edge_weights -> x4'),\n",
    "            (BatchNorm(bsh8), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            (lambda x1, x2, x3, x4: (x1, x2, x3, x4), 'x1, x2, x3, x4 -> x1, x2, x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.attention = GSequential('x3, x4, edge_index, edge_weights', [\n",
    "            (GATv2Conv(bsh8, bsh8, 2, dropout=dropout), 'x4, edge_index ->x4'),\n",
    "            (BatchNorm(bsh4), 'x4->x4'),\n",
    "            (nn.ReLU(), 'x4->x4'),\n",
    "            \n",
    "            (GCN2Conv(bsh4, 0.5, 0.1, 2), 'x4, x3, edge_index, edge_weights->x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (GCNConv(bsh4, bsh4), 'x3, edge_index, edge_weights -> x3'),\n",
    "            (BatchNorm(bsh4), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            \n",
    "            (GATv2Conv(bsh4, bsh4, 2, dropout=dropout), 'x3, edge_index ->x3'),\n",
    "            (BatchNorm(bsh2), 'x3->x3'),\n",
    "            (nn.ReLU(), 'x3->x3'),\n",
    "            (lambda x3, x4: (x3, x4), 'x3, x4 -> x3, x4')\n",
    "        ])\n",
    "        \n",
    "        self.decoder = GSequential('x1, x2, x3, edge_index, edge_weights', [\n",
    "            \n",
    "            (GCN2Conv(bsh2, 0.5, 0.1, 2), 'x3, x2, edge_index, edge_weights->x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, bsh2), 'x2, edge_index, edge_weights -> x2'),\n",
    "            (BatchNorm(bsh2), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            (GCNConv(bsh2, self.bsh), 'x2, edge_index->x2'),\n",
    "            (BatchNorm(self.bsh), 'x2->x2'),\n",
    "            (nn.ReLU(), 'x2->x2'),\n",
    "            (nn.Dropout(dropout), 'x2->x2'),\n",
    "            \n",
    "            (GCN2Conv(self.bsh, 0.5, 0.1, 2), 'x2, x1, edge_index, edge_weights->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (GCNConv(self.bsh, self.bsh), 'x1, edge_index, edge_weights ->x1'),\n",
    "            (BatchNorm(self.bsh), 'x1->x1'),\n",
    "            (nn.ReLU(), 'x1->x1'),\n",
    "            (nn.Dropout(dropout), 'x1->x1'),\n",
    "            (lambda x1, x2, x3: (x1, x2, x3), 'x1, x2, x3 -> x, x2, x3')\n",
    "        ])\n",
    "\n",
    "        self.pooling_layer1 = GCNConv(self.bsh, 5)\n",
    "        self.pooling_layer2 = DenseGCNConv(self.bsh, 1)\n",
    "        self.output_layer = Linear(self.bsh, self.num_out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, x3, x_enc = self.encoder(x.x, x.edge_index, x.edge_attr)\n",
    "        x_att, x4 = self.attention(x3, x_enc, x.edge_index, x.edge_attr)\n",
    "        x_dec, x2, x3 = self.decoder(x1, x2, x_att, x.edge_index, x.edge_attr)\n",
    "        \n",
    "        all_s = self.pooling_layer1(x_dec, x.edge_index, x.edge_attr)\n",
    "        ci = torch.tensor([x[i].x.shape[0] for i in range(len(x))], dtype=torch.int, device=x_dec.device).cumsum(0, dtype=torch.int)\n",
    "        x_list = [x_dec[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        s_i = [all_s[0 if i == 0 else ci[i - 1]:ci[i]] for i in range(len(ci))]\n",
    "        x_pooled = torch.zeros((len(x_list), self.bsh), dtype=x_dec.dtype, device=x_dec.device)\n",
    "        \n",
    "        for i in range(len(ci)):\n",
    "            s = s_i[i]# self.pooling_layer1(x2[i], x[i].edge_index, x[i].edge_attr)\n",
    "            adj = torch.zeros((x[i].x.shape[0], x[i].x.shape[0]), device=x_dec.device)\n",
    "            adj[x[i].edge_index[0],x[i].edge_index[1]] = x[i].edge_attr\n",
    "            # adj = to_dense_adj(edge_index=x[i].edge_index, max_num_nodes=x[i].x.shape[0], edge_attr=x[i].edge_attr)\n",
    "            nodes, adj, _, _ = dense_diff_pool(x_list[i], adj, s=s)\n",
    "            s = self.pooling_layer2(nodes, adj)\n",
    "            nodes, _, _, _ = dense_diff_pool(nodes, adj, s=s)\n",
    "            x_pooled[i] = torch.squeeze(nodes)\n",
    "\n",
    "        # return x1\n",
    "        return self.output_layer(x_pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_model = GcnGatModel1(300, 1, 1024)\n",
    "torch_model = torch_model.to(device)\n",
    "# next(iter(torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dataloader = data_manager.train_dataloader()\n",
    "v_dataloader = data_manager.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "X1, y1 = next(iter(t_dataloader))\n",
    "X2, y2 = next(iter(v_dataloader))\n",
    "print(X1[0].x.device)\n",
    "print(X2[0].x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.memory_allocated: 0.031685GB\n",
      "torch.cuda.memory_reserved: 0.046875GB\n",
      "torch.cuda.max_memory_reserved: 0.046875GB\n"
     ]
    }
   ],
   "source": [
    "print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(torch_model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train_model(target_model, num_epoch = 2, target_device='cpu'):\n",
    "    target_model = target_model.to(target_device)\n",
    "    loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(target_model.parameters(), 0.001, weight_decay=0.0055)\n",
    "    for i in range(num_epoch):\n",
    "        \n",
    "        batch_acc=[]\n",
    "        batch_loss = []\n",
    "        begin = time.time()\n",
    "        target_model.train()\n",
    "        i = 0\n",
    "        for X,y in t_dataloader:\n",
    "            print(f'i: {i}')\n",
    "            i = i + 1\n",
    "            X = X.to(target_device)\n",
    "            y = y.to(target_device)\n",
    "            optimizer.zero_grad()\n",
    "            predicted = target_model(X)\n",
    "            loss = loss_func(predicted, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            predicted = torch.nn.functional.sigmoid(predicted)\n",
    "            predicted[predicted<0.5] = 0\n",
    "            predicted[predicted>=0.5] = 1\n",
    "            accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "            batch_acc.append(accuracy)\n",
    "            batch_loss.append(loss.item())\n",
    "            torch.cuda.empty_cache()\n",
    "        print(f'train accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')\n",
    "        \n",
    "        target_model.eval()\n",
    "        batch_acc=[]\n",
    "        batch_loss = []\n",
    "        i = 0\n",
    "        for X,y in v_dataloader:\n",
    "            print(f'i: {i}')\n",
    "            i = i + 1\n",
    "            X = X.to(target_device)\n",
    "            y = y.to(target_device)\n",
    "            with torch.inference_mode():\n",
    "                predicted = target_model(X)\n",
    "                loss = loss_func(predicted, y)\n",
    "            predicted = torch.nn.functional.sigmoid(predicted)\n",
    "            predicted[predicted<0.5] = 0\n",
    "            predicted[predicted>=0.5] = 1\n",
    "            accuracy = torch.mean((predicted == y).to(torch.float32)).detach().item()\n",
    "            batch_acc.append(accuracy)\n",
    "            batch_loss.append(loss.item())\n",
    "            torch.cuda.empty_cache()\n",
    "        print(f'validation accuracy: {np.mean(batch_acc)}, loss: {np.mean(batch_loss)}, duration: {time.time() - begin:0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(torch_model, 5, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_model(torch_model, 5, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for X, y in t_dataloader:\n",
    "#     X = X.to('cuda')\n",
    "#     # for i in range(len(X)):\n",
    "#     #     X[i].x = X[i].x.to(device)\n",
    "#     #     X[i].edge_index = X[i].edge_index.to(device)\n",
    "#     #     X[i].edge_attr = X[i].edge_attr.to(device)\n",
    "#     torch_model(X)\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from Scripts.Models.LightningModels.LightningModels import BinaryLightningModel\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import torch\n",
    "import lightning as L\n",
    "from lightning.pytorch.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    ModelCheckpoint(save_top_k=2, mode='max', monitor='val_acc', save_last=True),\n",
    "    EarlyStopping(patience=15, mode='max', monitor='val_acc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "lightning_model = BinaryLightningModel(torch_model,\n",
    "                                 torch.optim.Adam(torch_model.parameters(), lr=0.0005, weight_decay=0.00055),\n",
    "                                       torch.nn.BCEWithLogitsLoss(),\n",
    "                                       learning_rate=0.0005,\n",
    "                                       batch_size=batch_size,\n",
    "                                       ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(torch_model.parameters())).device)\n",
    "print(lightning_model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_dataloader = data_manager.train_dataloader()\n",
    "# loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(torch_model.parameters(), 0.001, weight_decay=0.0055)\n",
    "# for X,y in t_dataloader:\n",
    "#     optimizer.zero_grad()\n",
    "#     predicted = torch_model(X)\n",
    "#     loss = loss_func(predicted, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=1000,\n",
    "            accelerator='gpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='GcnGatSentiment'),\n",
    "            num_sanity_val_steps=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = Tuner(trainer)\n",
    "# results = tuner.lr_find(lightning_model, datamodule=data_manager, min_lr=0.00001,max_lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = results.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbbb\n",
      "model2: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n",
      "arg: <class 'Scripts.Models.LightningModels.LightningModels.BinaryLightningModel'>\n",
      "arg: <class 'NoneType'>\n",
      "arg: <class 'NoneType'>\n",
      "arg: <class '__main__.AmazonReviewGraphLoader'>\n",
      "arg: <class 'NoneType'>\n",
      "kwargs: {}\n",
      "trainer_fn1: <bound method Trainer._fit_impl of <lightning.pytorch.trainer.trainer.Trainer object at 0x0000021993D57460>>\n",
      "trainer_fn2: <class 'method'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | model     | GcnGatModel1      | 8.5 M \n",
      "1 | loss_func | BCEWithLogitsLoss | 0     \n",
      "2 | train_acc | BinaryAccuracy    | 0     \n",
      "3 | val_acc   | BinaryAccuracy    | 0     \n",
      "4 | test_acc  | BinaryAccuracy    | 0     \n",
      "------------------------------------------------\n",
      "8.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "8.5 M     Total params\n",
      "33.911    Total estimated model params size (MB)\n",
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:293: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa3026c788b400ca410220e76d9c260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# trainer.fit(lightning_model, datamodule=data_manager)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(lightning_model,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             datamodule\u001b[39m=\u001b[39;49mdata_manager\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             \u001b[39m# train_dataloaders=data_manager.train_dataloader(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             \u001b[39m# val_dataloaders=data_manager.val_dataloader(),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             \u001b[39m# logger=CSVLogger(save_dir='logs/', name='sample_model'),\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             \u001b[39m# default_root_dir=\"~/Desktop\"\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:547\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel2: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(model)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 547\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    548\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    549\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:49\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrainer_fn1: \u001b[39m\u001b[39m{\u001b[39;00mtrainer_fn\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     48\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrainer_fn2: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(trainer_fn)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     51\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     52\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:583\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    577\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    578\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    579\u001b[0m     ckpt_path,\n\u001b[0;32m    580\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    582\u001b[0m )\n\u001b[1;32m--> 583\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    585\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    586\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:992\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    989\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 992\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    994\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    997\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1038\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1037\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1038\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1039\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1040\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    358\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    135\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    137\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[0;32m    138\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    239\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], batch_idx, kwargs)\n\u001b[0;32m    241\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m         closure()\n\u001b[0;32m    182\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(batch_idx, closure)\n\u001b[0;32m    189\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    264\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    266\u001b[0m     trainer,\n\u001b[0;32m    267\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    268\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    269\u001b[0m     batch_idx,\n\u001b[0;32m    270\u001b[0m     optimizer,\n\u001b[0;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    272\u001b[0m )\n\u001b[0;32m    274\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    275\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:162\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    161\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 162\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    164\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    165\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\core\\module.py:1282\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1244\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1245\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1248\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1249\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1250\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1251\u001b[0m \u001b[39m    the optimizer.\u001b[39;00m\n\u001b[0;32m   1252\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1280\u001b[0m \n\u001b[0;32m   1281\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1282\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    155\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(optimizer, model\u001b[39m=\u001b[39mmodel, closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision_plugin.py:117\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 121\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    123\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    124\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision_plugin.py:104\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    hook is called.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[0;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 126\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[0;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()  \u001b[39m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:314\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 314\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    316\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    317\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module:\n\u001b[0;32m    381\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_redirection(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, \u001b[39m\"\u001b[39m\u001b[39mtraining_step\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 382\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mtraining_step(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:90\u001b[0m, in \u001b[0;36mBinaryLightningModel.training_step\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):        \n\u001b[1;32m---> 90\u001b[0m     loss, out_features \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(BinaryLightningModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mtraining_step(data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     91\u001b[0m     predicted_labels \u001b[39m=\u001b[39m out_features \u001b[39mif\u001b[39;00m out_features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39margmax(out_features, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_acc(predicted_labels, data_batch[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(predicted_labels\u001b[39m.\u001b[39mshape))\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:26\u001b[0m, in \u001b[0;36mBaseLightningModel.training_step\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 26\u001b[0m     data_batch \u001b[39m=\u001b[39m data_batch\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     27\u001b[0m     data, labels \u001b[39m=\u001b[39m data_batch\n\u001b[0;32m     28\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(data)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "\n",
    "# trainer.fit(lightning_model, datamodule=data_manager)\n",
    "trainer.fit(lightning_model,\n",
    "            datamodule=data_manager\n",
    "            # train_dataloaders=data_manager.train_dataloader(),\n",
    "            # val_dataloaders=data_manager.val_dataloader(),\n",
    "            # logger=CSVLogger(save_dir='logs/', name='sample_model'),\n",
    "            # default_root_dir=\"~/Desktop\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "'''From torch lightning tutorials'''\n",
    "def plot_csv_logger(csv_path, loss_names=['train_loss', 'val_loss'], eval_names=['train_acc', 'val_acc']):\n",
    "    metrics = pd.read_csv(csv_path)\n",
    "    \n",
    "    aggregation_metrics = []\n",
    "    agg_col = 'epoch'\n",
    "    for i, dfg in metrics.groupby(agg_col):\n",
    "        agg = dict(dfg.mean())\n",
    "        agg[agg_col] = i\n",
    "        aggregation_metrics.append(agg)\n",
    "    \n",
    "    df_metrics = pd.DataFrame(aggregation_metrics)\n",
    "    df_metrics[loss_names].plot(grid=True, legend=True, xlabel='Epoch', ylabels='loss')\n",
    "    df_metrics[eval_names].plot(grid=True, legend=True, xlabel='Epoch', ylabels='accuracy')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\fardin\\\\Projects\\\\ColorIntelligence\\\\lightning_logs\\\\version_18\\\\checkpoints\\\\epoch=14-step=105.ckpt'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\utilities\\parsing.py:198: Attribute 'loss_func' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_func'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BinaryLightningModel(\n",
       "  (model): GcnGatModel1(\n",
       "    (encoder): Sequential(\n",
       "      (0): GCNConv(300, 256)\n",
       "      (1): ReLU()\n",
       "      (2): GCNConv(256, 128)\n",
       "      (3): ReLU()\n",
       "      (4): GCNConv(128, 64)\n",
       "      (5): ReLU()\n",
       "      (6): GCNConv(64, 32)\n",
       "      (7): ReLU()\n",
       "      (8): GATv2Conv(32, 32, heads=4)\n",
       "      (9): ReLU()\n",
       "      (10): GCN2Conv(128, alpha=0.5, beta=0.04879016416943204)\n",
       "      (11): ReLU()\n",
       "      (12): GCNConv(128, 256)\n",
       "      (13): ReLU()\n",
       "      (14): GCN2Conv(256, alpha=0.5, beta=0.04879016416943204)\n",
       "      (15): ReLU()\n",
       "    )\n",
       "    (pooling_layer1): GCNConv(256, 5)\n",
       "    (pooling_layer2): DenseGCNConv(256, 1)\n",
       "    (output_layer): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_func): BCEWithLogitsLoss()\n",
       "  (train_acc): BinaryAccuracy()\n",
       "  (val_acc): BinaryAccuracy()\n",
       "  (test_acc): BinaryAccuracy()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_checkpoint('example.ckpt')\n",
    "BinaryLightningModel.load_from_checkpoint(checkpoint_path='example.ckpt', model=torch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m lightning_model\u001b[39m.\u001b[39meval()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     logits \u001b[39m=\u001b[39m lightning_model(\u001b[39m'\u001b[39;49m\u001b[39mnew_data\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\Models\\LightningModels\\LightningModels.py:21\u001b[0m, in \u001b[0;36mBaseLightningModel.forward\u001b[1;34m(self, data_batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data_batch, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 21\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data_batch)\n",
      "File \u001b[1;32mc:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\fardin\\Projects\\ColorIntelligence\\Scripts\\CompositionRoot(Tasks)\\TextSentimentAnalysis\\AmazonReview\\AmazonReviewSentimentAnalysisMethod1.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x\u001b[39m.\u001b[39;49mx, x\u001b[39m.\u001b[39medge_index, x\u001b[39m.\u001b[39medge_attr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     all_s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooling_layer1(x1, x\u001b[39m.\u001b[39medge_index, x\u001b[39m.\u001b[39medge_attr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/fardin/Projects/ColorIntelligence/Scripts/CompositionRoot%28Tasks%29/TextSentimentAnalysis/AmazonReview/AmazonReviewSentimentAnalysisMethod1.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     ci \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([x[i]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x))], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint, device\u001b[39m=\u001b[39mx1\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mcumsum(\u001b[39m0\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mint)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "lightning_model.eval()\n",
    "with torch.inference_mode():\n",
    "    logits = lightning_model('new_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=lightning_model, datamodule=data_manager, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import summary\n",
    "from tqdm import tqdm\n",
    "from Scripts.Models.LightningModels.LightningModels import BaseLightningModel\n",
    "from Scripts.Models.ModelsManager.ModelManager import ModelManager\n",
    "from Scripts.Models.BaseModels.GATGCNClassifierSimple import GNNClassifier\n",
    "from Scripts.DataManager.GraphLoader.NLabeledGraphLoader import NLabeledGraphLoader\n",
    "from Scripts.Utils.enums import Optimizer, LossType\n",
    "\n",
    "import lightning as L\n",
    "\n",
    "\n",
    "class SimpleNodeClassifierModelManager(ModelManager):\n",
    "\n",
    "    def __init__(self, graph_handler: NLabeledGraphLoader, device=torch.device('cpu'),\n",
    "                 lr=0.01, weight_decay=0.001, optimizer_type: Optimizer = Optimizer.ADAM,\n",
    "                 loss_type: LossType = LossType.CROSS_ENTROPY):\n",
    "        super(SimpleNodeClassifierModelManager, self).__init__(lr, weight_decay, device)\n",
    "        self.graph_handler = graph_handler\n",
    "        self.num_output_classes = self.graph_handler.num_classes\n",
    "        self.num_input_features = self.graph_handler.num_features\n",
    "        self.loss_type = loss_type\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.model, self.optimizer, self.loss_func = self._create_model(lr, weight_decay, optimizer_type, loss_type)\n",
    "        self.lightning_model = BaseLightningModel(self.model, self.optimizer, self.loss_func)\n",
    "\n",
    "    def train(self, epoch_num: int = 100, lr: float = None, l2_norm: float = None, optimizer: Optimizer = None):\n",
    "\n",
    "        trainer = L.Trainer(max_epochs=100, accelerator='gpu', devices=1)\n",
    "        trainer.fit(self.lightning_model,\n",
    "                    train_dataloaders=self.graph_handler.get_train_data(),\n",
    "                    val_dataloaders=self.graph_handler.get_val_data()\n",
    "                    )\n",
    "\n",
    "\n",
    "\n",
    "        if lr or l2_norm or optimizer:\n",
    "            self.set_optimizer(lr, l2_norm, optimizer)\n",
    "\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        test_losses = []\n",
    "        test_accuracies = []\n",
    "\n",
    "        train_node_x, train_node_y, train_edges = self.graph_handler.get_train_data()\n",
    "        test_node_x, test_node_y, test_edges = self.graph_handler.get_test_data()\n",
    "        for i in tqdm(range(epoch_num)):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            my_node, my_label, my_edges = self.graph_handler.extract_random_sub_edges_graph()\n",
    "            y_hat: torch.Tensor = self.model(my_node, my_edges)\n",
    "            loss = self.loss_func(F.one_hot(my_label, self.num_output_classes).float(), y_hat)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            train_losses.append(loss_value)\n",
    "            accuracy = (torch.sum((y_hat.argmax(dim=1) == my_label).float()) / len(my_label)).cpu().numpy()\n",
    "            train_accuracies.append(accuracy)\n",
    "\n",
    "            my_node, my_label, my_edges = self.graph_handler.extract_random_sub_edges_graph()\n",
    "            y_hat: torch.Tensor = self.model(my_node, my_edges)\n",
    "            loss = self.loss_func(F.one_hot(my_label, self.graph_handler.num_classes).float(), y_hat)\n",
    "            loss_value = loss.item()\n",
    "            test_losses.append(loss_value)\n",
    "            accuracy = (torch.sum((y_hat.argmax(dim=1) == my_label).float()) / len(my_label)).cpu().numpy()\n",
    "            test_accuracies.append(accuracy)\n",
    "        self.history['train_losses'] = train_losses\n",
    "        self.history['train_accuracies'] = train_accuracies\n",
    "        self.history['test_losses'] = test_losses\n",
    "        self.history['test_accuracies'] = test_accuracies\n",
    "        return self.history\n",
    "\n",
    "    def evaluate(self):\n",
    "        node_x, node_y, edges = self.graph_handler.get_val_data()\n",
    "        self.model.eval()\n",
    "        y_hat: torch.Tensor = self.model(node_x, edges)\n",
    "        loss = self.loss_func(F.one_hot(node_y, self.num_output_classes).float(), y_hat)\n",
    "        return y_hat, loss\n",
    "\n",
    "    def predict(self, node_x, edge_index):\n",
    "        self.model.eval()\n",
    "        y_hat: torch.Tensor = self.model(node_x, edge_index)\n",
    "        return y_hat\n",
    "\n",
    "    def draw_summary(self):\n",
    "        nodes_x, nodes_y, edge_indices_test = self.graph_handler.get_test_data()\n",
    "        nodes_x, nodes_y, edge_indices_test = self.graph_handler.extract_random_sub_edges_graph(2)\n",
    "        print(summary(self.model, nodes_x, edge_indices_test))\n",
    "\n",
    "    def _create_model(self, lr, l2_norm, optimizer_type, loss_type):\n",
    "        model = GNNClassifier(input_feature=self.num_input_features, class_counts=self.num_output_classes)\n",
    "        optimizer = ModelManager._create_optimizer(model, lr, l2_norm, optimizer_type)\n",
    "        loss_func = ModelManager._create_loss_func(loss_type)\n",
    "        model.to(self.device)\n",
    "        return model, optimizer, loss_func\n",
    "\n",
    "    def set_optimizer(self, lr, l2_norm, optimizer=Optimizer.ADAM):\n",
    "        if lr:\n",
    "            self.lr = lr\n",
    "        if l2_norm:\n",
    "            self.l2_norm = l2_norm\n",
    "        if optimizer:\n",
    "            self.optimizer_type = optimizer\n",
    "        self.optimizer = ModelManager._create_optimizer(self.model, self.lr, self.l2_norm, self.optimizer_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from Scripts.Models.ModelsManager.SimpleGraphClassifierModelManager import SimpleGraphClassifierModelManager\n",
    "\n",
    "if 'Train Data':\n",
    "    model_manager = SimpleGraphClassifierModelManager(\n",
    "        data_manager.graph_constructors[TextGraphType.CO_OCCURRENCE])\n",
    "    model_manager.draw_summary()\n",
    "    model_manager.train()\n",
    "    model_manager.draw_training_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Draw Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Scripts.Models.BaseModels.GcnGatModel1 import GcnGatModel1\n",
    "from transformers.models.longformer.convert_longformer_original_pytorch_lightning_to_pytorch import LightningModel\n",
    "mymodel = GcnGatModel1(100, 10, config)\n",
    "print(type(mymodel))\n",
    "model = LightningModel(mymodel)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def sample_func() -> Tuple[int, float]:\n",
    "    return 1, 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "targets = sample_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "type(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from Scripts.Utils.enums import Optimizer\n",
    "\n",
    "type(Optimizer.ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_func():\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "aa: Callable = None\n",
    "type(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
